{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36dcd63",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03171600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a057aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = '20071116_100days_15lb_0.7Distance_Falseloops'\n",
    "\n",
    "dataset = torch.load(f'/finance_data/{data_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aba1f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_info(dataset):\n",
    "    print(f'Number of samples = {dataset.snapshot_count}, per sample:')\n",
    "    print(f'Number of vertices: {dataset[0].x.shape[0]}')\n",
    "    print(f'Number of time steps: {dataset[0].x.shape[1]}')\n",
    "    print(f'Number of edge types: {dataset[0].edge_type.max().item() + 1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f387dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples = 100, per sample:\n",
      "Number of vertices: 200\n",
      "Number of time steps: 16\n",
      "Number of edge types: 1\n"
     ]
    }
   ],
   "source": [
    "data_info(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff85f7",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf7f69a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_steps: int,\n",
    "        min_beta: float,\n",
    "        max_beta: float,\n",
    "        num_nodes: int,\n",
    "        feature_size: int,\n",
    "    ):\n",
    "        super(GDN, self).__init__()\n",
    "        self.num_steps = num_steps\n",
    "        self.betas = torch.linspace(min_beta, max_beta, num_steps)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alpha_bars = torch.tensor([torch.prod(self.alphas[:i + 1]) for i in range(len(self.alphas))])\n",
    "        \n",
    "        self._GCN = GCNConv(feature_size, feature_size)  # By default this adds self loops and normalises\n",
    "        \n",
    "        self._PIUNet = PIUNet(num_nodes, num_steps)\n",
    "        \n",
    "        self._DDPM = DDPM(self._PIUNet, num_steps, self.betas, self.alphas, self.alpha_bars)\n",
    "        \n",
    "        self._DDRM = DDRM(self._DDPM, num_nodes, feature_size)\n",
    "        \n",
    "        self._linear = nn.Linear(feature_size, 1)\n",
    "        \n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,  # [200, 16]\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_type: torch.Tensor,\n",
    "        edge_attr: torch.Tensor = None,\n",
    "        return_sample: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        h = self._GCN(x,  edge_index)  # [200, 16]\n",
    "        \n",
    "        if return_sample==False:        \n",
    "            h = self._linear(h)  # [200, 1]\n",
    "            h = torch.squeeze(h)  # [200]\n",
    "        \n",
    "        return h\n",
    "\n",
    "\n",
    "# The PIUNet code is adapted from Pulfer's DDPM code: https://github.com/BrianPulfer/PapersReimplementations/tree/main/ddpm\n",
    "class PIUNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        h: int,  # Height of input matrix, for us h=200\n",
    "        num_steps: int,\n",
    "        time_emb_dim: int = 100\n",
    "    ):\n",
    "        super(PIUNet, self).__init__()\n",
    "\n",
    "        # Sinusoidal embedding\n",
    "        self.time_embed = nn.Embedding(num_steps, time_emb_dim)\n",
    "        self.time_embed.weight.data = sinusoidal_embedding(num_steps, time_emb_dim)\n",
    "        self.time_embed.requires_grad_(False)\n",
    "\n",
    "        # Downsampling regime\n",
    "        self.te1 = self._make_te(time_emb_dim, 1)\n",
    "        self.b1 = nn.Sequential(\n",
    "            PICNN((1, h, 16), 1, 10),\n",
    "            PICNN((10, h, 16), 10, 10),\n",
    "            PICNN((10, h, 16), 10, 10)\n",
    "        )\n",
    "        self.down1 = nn.Conv2d(10, 10, (1,4), (1,2), (0,1))\n",
    "\n",
    "        self.te2 = self._make_te(time_emb_dim, 10)\n",
    "        self.b2 = nn.Sequential(\n",
    "            PICNN((10, h, 8), 10, 20),\n",
    "            PICNN((20, h, 8), 20, 20),\n",
    "            PICNN((20, h, 8), 20, 20)\n",
    "        )\n",
    "        self.down2 = nn.Conv2d(20, 20, (1,4), (1,2), (0,1))\n",
    "\n",
    "        self.te3 = self._make_te(time_emb_dim, 20)\n",
    "        self.b3 = nn.Sequential(\n",
    "            PICNN((20, h, 4), 20, 40),\n",
    "            PICNN((40, h, 4), 40, 40),\n",
    "            PICNN((40, h, 4), 40, 40)\n",
    "        )\n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.Conv2d(40, 40, (1,2), 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(40, 40, (1,3), 1, (0,1))\n",
    "        )\n",
    "\n",
    "        # Bottleneck point\n",
    "        self.te_mid = self._make_te(time_emb_dim, 40)\n",
    "        self.b_mid = nn.Sequential(\n",
    "            PICNN((40, h, 3), 40, 20),\n",
    "            PICNN((20, h, 3), 20, 20),\n",
    "            PICNN((20, h, 3), 20, 40)\n",
    "        )\n",
    "\n",
    "        # Up sampling regime\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(40, 40, (1,3), 1, (0,1)),\n",
    "            nn.SiLU(),\n",
    "            nn.ConvTranspose2d(40, 40, (1,2), 1)\n",
    "        )\n",
    "\n",
    "        self.te4 = self._make_te(time_emb_dim, 80)\n",
    "        self.b4 = nn.Sequential(\n",
    "            PICNN((80, h, 4), 80, 40),\n",
    "            PICNN((40, h, 4), 40, 20),\n",
    "            PICNN((20, h, 4), 20, 20)\n",
    "        )\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(20, 20, (1,4), (1,2), (0,1))\n",
    "        self.te5 = self._make_te(time_emb_dim, 40)\n",
    "        self.b5 = nn.Sequential(\n",
    "            PICNN((40, h, 8), 40, 20),\n",
    "            PICNN((20, h, 8), 20, 10),\n",
    "            PICNN((10, h, 8), 10, 10)\n",
    "        )\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(10, 10, (1,4), (1,2), (0,1))\n",
    "        self.te_out = self._make_te(time_emb_dim, 20)\n",
    "        self.b_out = nn.Sequential(\n",
    "            PICNN((20, h, 16), 20, 10),\n",
    "            PICNN((10, h, 16), 10, 10),\n",
    "            PICNN((10, h, 16), 10, 10, normalize=False)\n",
    "        )\n",
    "\n",
    "        self.conv_out = nn.Conv2d(10, 1, (1,3), 1, (0,1))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "    \n",
    "        # x is dim [N, 1, 200, 16], where N = batch size\n",
    "        t = self.time_embed(t)\n",
    "        n = len(x)\n",
    "        out1 = self.b1(x + self.te1(t).reshape(n, -1, 1, 1))  # [N, 10, 200, 16]\n",
    "        out2 = self.b2(self.down1(out1) + self.te2(t).reshape(n, -1, 1, 1))  # [N, 20, 200, 8]\n",
    "        out3 = self.b3(self.down2(out2) + self.te3(t).reshape(n, -1, 1, 1))  # [N, 40, 200, 4]\n",
    "        out_mid = self.b_mid(self.down3(out3) + self.te_mid(t).reshape(n, -1, 1, 1))  # [N, 40, 200, 3]\n",
    "        out4 = torch.cat((out3, self.up1(out_mid)), dim=1)  # [N, 80, 200, 4]        \n",
    "        out4 = self.b4(out4 + self.te4(t).reshape(n, -1, 1, 1))  # [N, 20, 200, 4]\n",
    "        out5 = torch.cat((out2, self.up2(out4)), dim=1)  # [N, 40, 200, 8]\n",
    "        out5 = self.b5(out5 + self.te5(t).reshape(n, -1, 1, 1))  # [N, 10, 200, 8]\n",
    "        out = torch.cat((out1, self.up3(out5)), dim=1)  # [N, 20, 200, 16]        \n",
    "        out = self.b_out(out + self.te_out(t).reshape(n, -1, 1, 1))  # [N, 10, 200, 16]\n",
    "        out = self.conv_out(out)  # [N, 1, 200, 16]\n",
    "\n",
    "        return out\n",
    "    \n",
    "    # The single hidden layer MLP which is used to map positional embeddings\n",
    "    def _make_te(self, dim_in, dim_out):\n",
    "        return nn.Sequential(\n",
    "        nn.Linear(dim_in, dim_out),\n",
    "        nn.SiLU(),\n",
    "        nn.Linear(dim_out, dim_out)\n",
    "      )\n",
    "\n",
    "# The \"PICNN\" called on is the below 1-D CNN class:\n",
    "class PICNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        shape: tuple,\n",
    "        in_c: int,\n",
    "        out_c: int,\n",
    "        kernel_size: tuple = (1,3),\n",
    "        stride: int = 1,\n",
    "        padding: tuple = (0,1),\n",
    "        normalize: bool = True,\n",
    "    ):\n",
    "        super(PICNN, self).__init__()\n",
    "        self.ln = nn.LayerNorm(shape)\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size, stride, padding)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size, stride, padding)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "    \n",
    "        out = self.ln(x) if self.normalize else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# The \"sinusoidal_embedding\" called on is the below function, this\n",
    "# returns the standard positional embedding\n",
    "def sinusoidal_embedding(n, d):\n",
    "    embedding = torch.zeros(n, d)\n",
    "    wk = torch.tensor([1 / 10_000 ** (2 * j / d) for j in range(d)])\n",
    "    wk = wk.reshape((1, d))\n",
    "    t = torch.arange(n).reshape((n, 1))\n",
    "    embedding[:,::2] = torch.sin(t * wk[:,::2])\n",
    "    embedding[:,1::2] = torch.cos(t * wk[:,::2])\n",
    "\n",
    "    return embedding\n",
    "\n",
    "\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        network,\n",
    "        num_steps,\n",
    "        betas,\n",
    "        alphas,\n",
    "        alpha_bars,\n",
    "    ):\n",
    "        super(DDPM, self).__init__()\n",
    "        self.num_steps = num_steps\n",
    "        self.network = network  # Will pass in PIU-Net as network\n",
    "        self.betas = betas\n",
    "        self.alphas = alphas\n",
    "        self.alpha_bars = alpha_bars\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x0: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        epsilon: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "    \n",
    "        n, c, h, w = x0.shape  # n = batch size, c = num channels, h = height, w = width\n",
    "        a_bar = self.alpha_bars[t]\n",
    "\n",
    "        if epsilon is None:\n",
    "            epsilon = torch.randn(n, c, h, w)\n",
    "        \n",
    "        noisy = a_bar.sqrt().reshape(n, 1, 1, 1)*x0 + (1 - a_bar).sqrt().reshape(n, 1, 1, 1)*epsilon\n",
    "        return noisy\n",
    "\n",
    "    def backward(self, x, t):\n",
    "        # Runs each image in the batch through the network, each for their\n",
    "        # random time step t, the network returns its estimation of\n",
    "        # the noise that was added in the last step\n",
    "        return self.network(x, t)\n",
    "\n",
    "\n",
    "\n",
    "class DDRM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ddpm,\n",
    "        h: int,\n",
    "        w: int,\n",
    "    ):\n",
    "        super(DDRM, self).__init__()\n",
    "        self.ddpm = ddpm\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        y: torch.Tensor,\n",
    "        sigma_y: float,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        ddpm = self.ddpm\n",
    "        h = self.h\n",
    "        w = self.w\n",
    "        \n",
    "        for idx, t in enumerate(list(range(ddpm.num_steps))[::-1]):\n",
    "            # Using \\sigma_t^2 = \\beta_t, Ho et al. found the model to be robust to the \\sigma_t choice\n",
    "            beta_t = ddpm.betas[t]\n",
    "            sigma_t = beta_t.sqrt()\n",
    "\n",
    "            # To satisfy Theorem 1\n",
    "            eta = 1\n",
    "            eta_b = (2*sigma_t**2)/(sigma_t**2 + sigma_y**2)\n",
    "            \n",
    "            z_t = torch.randn(h, w) \n",
    "\n",
    "            if idx==0:\n",
    "                assert sigma_y<=sigma_t, f'Error: sigma_y = {sigma_y} > {sigma_t} = sigma_T, decrease sigma_y value.'\n",
    "                x_t = y + (sigma_t**2 - sigma_y**2).sqrt() * z_t\n",
    "\n",
    "\n",
    "            else:           \n",
    "                time_tensor = (torch.ones(1, 1) * (t+1)).long()\n",
    "                tilde_epsilon_t = ddpm.backward(x_t[None, None, :], time_tensor)[0][0]  # here x_t corresponds to t+1\n",
    "\n",
    "                alpha_t = ddpm.alphas[t]\n",
    "                alpha_t_bar = ddpm.alpha_bars[t]\n",
    "                \n",
    "                tilde_x_t = (1/alpha_t.sqrt()) * (x_t - (beta_t/((1 - alpha_t_bar).sqrt())) * tilde_epsilon_t)\n",
    "                \n",
    "                if sigma_t<sigma_y:\n",
    "                    x_t = tilde_x_t + sigma_t * z_t\n",
    "                else:\n",
    "                    x_t = (1-eta_b)*tilde_x_t + eta_b*y + (sigma_t**2 - (sigma_y**2)*(eta_b**2)).sqrt() * z_t\n",
    "\n",
    "        return x_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af5df5",
   "metadata": {},
   "source": [
    "# Rolling train/test algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4743de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_hat, y, gamma, delta):\n",
    "    loss = delta*torch.mean(-torch.tanh(gamma*y_hat)*y) + (1-delta)*torch.mean((y_hat-y)**2)\n",
    "    return loss\n",
    "\n",
    "def pnl_function(y_hat, y):\n",
    "    pnl = torch.mean(torch.sign(y_hat)*y)\n",
    "    return pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05dea244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_train_test(model, data, GDN_epochs, DDPM_epochs, gamma, delta, GDN_lr, DDPM_lr, DDPM_lb, training_lb, GCN):    \n",
    "    train_losses, train_pnls, test_losses, test_pnls, test_ys, all_batches = [], [], [], [], [], []\n",
    "    T = data.snapshot_count\n",
    "    \n",
    "    for t in range(training_lb, T):\n",
    "        print(f' ----- On train/test for day {t} ----- ')\n",
    "        \n",
    "        if GCN==True:\n",
    "            b = T\n",
    "        else:\n",
    "            b = DDPM_lb+training_lb\n",
    "        \n",
    "        if t<b:\n",
    "            model_passing_outputs = model_passing(model, data, GDN_epochs, gamma, delta, GDN_lr, training_lb,\n",
    "                                                  t, False, GCN)\n",
    "            all_batches.append(model_passing_outputs[0])\n",
    "            train_losses.append(model_passing_outputs[1])\n",
    "            train_pnls.append(model_passing_outputs[2])\n",
    "            test_losses.append(model_passing_outputs[3])\n",
    "            test_pnls.append(model_passing_outputs[4])\n",
    "            test_ys.append(model_passing_outputs[5])\n",
    "            print(f'Current test pnl = {model_passing_outputs[4]}')\n",
    "            \n",
    "        else:\n",
    "            model.train()\n",
    "            previous_batches = np.array(all_batches[-DDPM_lb:])\n",
    "            previous_batches = torch.tensor(previous_batches)\n",
    "            \n",
    "            DDPMoptim = Adam(model._DDPM.parameters(), lr=DDPM_lr)\n",
    "            mse = nn.MSELoss()\n",
    "            num_steps = model.num_steps\n",
    "            \n",
    "            for epoch in range(DDPM_epochs):\n",
    "                epoch_loss = 0.0\n",
    "                \n",
    "                for batch in previous_batches:\n",
    "                    x0 = batch                    \n",
    "                    n = len(x0)\n",
    "                    epsilon = torch.randn_like(x0)\n",
    "                    time = torch.randint(0, num_steps, (n,))\n",
    "                    noisy_imgs = model._DDPM(x0, time, epsilon)\n",
    "                    epsilon_theta = model._DDPM.backward(noisy_imgs, time.reshape(n, -1))\n",
    "                    DDPM_loss = mse(epsilon_theta, epsilon)\n",
    "                    \n",
    "                    DDPMoptim.zero_grad()\n",
    "                    DDPM_loss.backward()\n",
    "                    DDPMoptim.step()\n",
    "                    \n",
    "                    epoch_loss += DDPM_loss.item()\n",
    "\n",
    "                print(f\"Loss at DDPM epoch {epoch + 1}: {epoch_loss/DDPM_lb:.6f}\")\n",
    "            \n",
    "            model_passing_outputs = model_passing(model, data, GDN_epochs, gamma, delta, GDN_lr, training_lb,\n",
    "                                                  t, True, GCN)\n",
    "            all_batches.append(model_passing_outputs[0])\n",
    "            train_losses.append(model_passing_outputs[1])\n",
    "            train_pnls.append(model_passing_outputs[2])\n",
    "            test_losses.append(model_passing_outputs[3])\n",
    "            test_pnls.append(model_passing_outputs[4])\n",
    "            test_ys.append(model_passing_outputs[5])\n",
    "            print(f'Current test pnl = {model_passing_outputs[4]:.6f}')\n",
    "            \n",
    "    return train_losses, train_pnls, test_losses, test_pnls, np.array(test_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16e1aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_passing(model, data, GDN_epochs, gamma, delta, GDN_lr, training_lb, t, use_DDRM, GCN):    \n",
    "    current_batch, train_t_losses, train_t_pnls = [], [], []\n",
    "    \n",
    "    params = list(model._GCN.parameters()) + list(model._linear.parameters())\n",
    "    optim = Adam(params, lr=GDN_lr)\n",
    "    \n",
    "    model.eval()\n",
    "    x_data = []\n",
    "    for i in range(t-training_lb, t+1):\n",
    "        m = data[i].x\n",
    "        \n",
    "        if use_DDRM==True:\n",
    "            print(f'Sampling for day {i-t+training_lb+1}/{training_lb+1} which is day {i}')\n",
    "            std_dev = m.std()\n",
    "            \n",
    "            # Below we standardise before denoising the sample via the DDRM and then scale back\n",
    "            # up post-DDRM\n",
    "            m = m/std_dev            \n",
    "            x_data.append((model._DDRM(m, 0.05).detach().numpy())*std_dev.item())\n",
    "        else:\n",
    "            x_data.append(m.detach().numpy())\n",
    "    \n",
    "    x_data = np.array(x_data)\n",
    "    x_data = torch.tensor(x_data)\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(GDN_epochs):\n",
    "        train_epoch_loss = 0\n",
    "        train_pnl_loss = 0\n",
    "        \n",
    "        for i, t_ in enumerate(range(t-training_lb, t)):\n",
    "            snapshot = data[t_]\n",
    "            y_hat = model(x_data[i], snapshot.edge_index, snapshot.edge_type, snapshot.edge_attr, False)\n",
    "            train_loss = custom_loss(y_hat, snapshot.y, gamma, delta)\n",
    "            train_pnl_loss += pnl_function(y_hat, snapshot.y).item()\n",
    "                \n",
    "            optim.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            train_epoch_loss += train_loss.item()\n",
    "        \n",
    "        print(f'On epoch {epoch+1} of day {t} training, loss = {train_epoch_loss/training_lb:.8f}')\n",
    "\n",
    "        # We divide by training_lb so can compare fairly for varying training_lb size\n",
    "        train_t_losses.append(train_epoch_loss/training_lb)\n",
    "        train_t_pnls.append(train_pnl_loss/training_lb)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    if GCN==False:\n",
    "        for i, t_ in enumerate(range(t-training_lb, t+1)):\n",
    "            snapshot = data[t_]\n",
    "            x_0_hat = model(x_data[i], snapshot.edge_index, snapshot.edge_type, snapshot.edge_attr, True)\n",
    "            x_0_hat = x_0_hat/x_0_hat.std()  # So all of DDPM training samples are standardised\n",
    "            x_0_hat = x_0_hat[None, :]\n",
    "            current_batch.append(x_0_hat.detach().numpy())\n",
    "    \n",
    "    snapshot = data[t]\n",
    "    y_hat_t = model(x_data[-1], snapshot.edge_index, snapshot.edge_type, snapshot.edge_attr, False)\n",
    "    test_t_loss = custom_loss(y_hat_t, snapshot.y, gamma, delta).item()\n",
    "    test_t_pnl = pnl_function(y_hat_t, snapshot.y).item()\n",
    "    \n",
    "    return current_batch, train_t_losses, train_t_pnls, test_t_loss, test_t_pnl, (y_hat_t.detach().numpy(), snapshot.y.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dc7d151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples = 100, per sample:\n",
      "Number of vertices: 200\n",
      "Number of time steps: 16\n",
      "Number of edge types: 1\n"
     ]
    }
   ],
   "source": [
    "data = dataset\n",
    "data_info(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39427f5",
   "metadata": {},
   "source": [
    "# Training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ff42e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 500\n",
    "min_beta = 10**-4\n",
    "max_beta = 0.02\n",
    "num_nodes = 200\n",
    "feature_size = 16\n",
    "\n",
    "gdn = GDN(num_steps, min_beta, max_beta, num_nodes, feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eb507c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GDN(\n",
       "  (_GCN): GCNConv(16, 16)\n",
       "  (_PIUNet): PIUNet(\n",
       "    (time_embed): Embedding(500, 100)\n",
       "    (te1): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=1, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=1, out_features=1, bias=True)\n",
       "    )\n",
       "    (b1): Sequential(\n",
       "      (0): PICNN(\n",
       "        (ln): LayerNorm((1, 200, 16), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(1, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (1): PICNN(\n",
       "        (ln): LayerNorm((10, 200, 16), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (2): PICNN(\n",
       "        (ln): LayerNorm((10, 200, 16), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (down1): Conv2d(10, 10, kernel_size=(1, 4), stride=(1, 2), padding=(0, 1))\n",
       "    (te2): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=10, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): PICNN(\n",
       "        (ln): LayerNorm((10, 200, 8), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(10, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (1): PICNN(\n",
       "        (ln): LayerNorm((20, 200, 8), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (2): PICNN(\n",
       "        (ln): LayerNorm((20, 200, 8), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (down2): Conv2d(20, 20, kernel_size=(1, 4), stride=(1, 2), padding=(0, 1))\n",
       "    (te3): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=20, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=20, out_features=20, bias=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): PICNN(\n",
       "        (ln): LayerNorm((20, 200, 4), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(20, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (1): PICNN(\n",
       "        (ln): LayerNorm((40, 200, 4), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (2): PICNN(\n",
       "        (ln): LayerNorm((40, 200, 4), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (down3): Sequential(\n",
       "      (0): Conv2d(40, 40, kernel_size=(1, 2), stride=(1, 1))\n",
       "      (1): SiLU()\n",
       "      (2): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    )\n",
       "    (te_mid): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=40, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=40, out_features=40, bias=True)\n",
       "    )\n",
       "    (b_mid): Sequential(\n",
       "      (0): PICNN(\n",
       "        (ln): LayerNorm((40, 200, 3), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(40, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (1): PICNN(\n",
       "        (ln): LayerNorm((20, 200, 3), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (2): PICNN(\n",
       "        (ln): LayerNorm((20, 200, 3), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(20, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (up1): Sequential(\n",
       "      (0): ConvTranspose2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "      (1): SiLU()\n",
       "      (2): ConvTranspose2d(40, 40, kernel_size=(1, 2), stride=(1, 1))\n",
       "    )\n",
       "    (te4): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=80, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): PICNN(\n",
       "        (ln): LayerNorm((80, 200, 4), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(80, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (1): PICNN(\n",
       "        (ln): LayerNorm((40, 200, 4), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(40, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (2): PICNN(\n",
       "        (ln): LayerNorm((20, 200, 4), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (up2): ConvTranspose2d(20, 20, kernel_size=(1, 4), stride=(1, 2), padding=(0, 1))\n",
       "    (te5): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=40, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=40, out_features=40, bias=True)\n",
       "    )\n",
       "    (b5): Sequential(\n",
       "      (0): PICNN(\n",
       "        (ln): LayerNorm((40, 200, 8), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(40, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (1): PICNN(\n",
       "        (ln): LayerNorm((20, 200, 8), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(20, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (2): PICNN(\n",
       "        (ln): LayerNorm((10, 200, 8), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (up3): ConvTranspose2d(10, 10, kernel_size=(1, 4), stride=(1, 2), padding=(0, 1))\n",
       "    (te_out): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=20, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=20, out_features=20, bias=True)\n",
       "    )\n",
       "    (b_out): Sequential(\n",
       "      (0): PICNN(\n",
       "        (ln): LayerNorm((20, 200, 16), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(20, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (1): PICNN(\n",
       "        (ln): LayerNorm((10, 200, 16), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (2): PICNN(\n",
       "        (ln): LayerNorm((10, 200, 16), eps=1e-05, elementwise_affine=True)\n",
       "        (conv1): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (conv_out): Conv2d(10, 1, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "  )\n",
       "  (_DDPM): DDPM(\n",
       "    (network): PIUNet(\n",
       "      (time_embed): Embedding(500, 100)\n",
       "      (te1): Sequential(\n",
       "        (0): Linear(in_features=100, out_features=1, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=1, out_features=1, bias=True)\n",
       "      )\n",
       "      (b1): Sequential(\n",
       "        (0): PICNN(\n",
       "          (ln): LayerNorm((1, 200, 16), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(1, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (1): PICNN(\n",
       "          (ln): LayerNorm((10, 200, 16), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (2): PICNN(\n",
       "          (ln): LayerNorm((10, 200, 16), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (down1): Conv2d(10, 10, kernel_size=(1, 4), stride=(1, 2), padding=(0, 1))\n",
       "      (te2): Sequential(\n",
       "        (0): Linear(in_features=100, out_features=10, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "      )\n",
       "      (b2): Sequential(\n",
       "        (0): PICNN(\n",
       "          (ln): LayerNorm((10, 200, 8), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(10, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (1): PICNN(\n",
       "          (ln): LayerNorm((20, 200, 8), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (2): PICNN(\n",
       "          (ln): LayerNorm((20, 200, 8), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (down2): Conv2d(20, 20, kernel_size=(1, 4), stride=(1, 2), padding=(0, 1))\n",
       "      (te3): Sequential(\n",
       "        (0): Linear(in_features=100, out_features=20, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=20, out_features=20, bias=True)\n",
       "      )\n",
       "      (b3): Sequential(\n",
       "        (0): PICNN(\n",
       "          (ln): LayerNorm((20, 200, 4), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(20, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (1): PICNN(\n",
       "          (ln): LayerNorm((40, 200, 4), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (2): PICNN(\n",
       "          (ln): LayerNorm((40, 200, 4), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (down3): Sequential(\n",
       "        (0): Conv2d(40, 40, kernel_size=(1, 2), stride=(1, 1))\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "      )\n",
       "      (te_mid): Sequential(\n",
       "        (0): Linear(in_features=100, out_features=40, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=40, out_features=40, bias=True)\n",
       "      )\n",
       "      (b_mid): Sequential(\n",
       "        (0): PICNN(\n",
       "          (ln): LayerNorm((40, 200, 3), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(40, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (1): PICNN(\n",
       "          (ln): LayerNorm((20, 200, 3), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (2): PICNN(\n",
       "          (ln): LayerNorm((20, 200, 3), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(20, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (up1): Sequential(\n",
       "        (0): ConvTranspose2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (1): SiLU()\n",
       "        (2): ConvTranspose2d(40, 40, kernel_size=(1, 2), stride=(1, 1))\n",
       "      )\n",
       "      (te4): Sequential(\n",
       "        (0): Linear(in_features=100, out_features=80, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      )\n",
       "      (b4): Sequential(\n",
       "        (0): PICNN(\n",
       "          (ln): LayerNorm((80, 200, 4), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(80, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (1): PICNN(\n",
       "          (ln): LayerNorm((40, 200, 4), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(40, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (2): PICNN(\n",
       "          (ln): LayerNorm((20, 200, 4), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (up2): ConvTranspose2d(20, 20, kernel_size=(1, 4), stride=(1, 2), padding=(0, 1))\n",
       "      (te5): Sequential(\n",
       "        (0): Linear(in_features=100, out_features=40, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=40, out_features=40, bias=True)\n",
       "      )\n",
       "      (b5): Sequential(\n",
       "        (0): PICNN(\n",
       "          (ln): LayerNorm((40, 200, 8), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(40, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (1): PICNN(\n",
       "          (ln): LayerNorm((20, 200, 8), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(20, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (2): PICNN(\n",
       "          (ln): LayerNorm((10, 200, 8), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (up3): ConvTranspose2d(10, 10, kernel_size=(1, 4), stride=(1, 2), padding=(0, 1))\n",
       "      (te_out): Sequential(\n",
       "        (0): Linear(in_features=100, out_features=20, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=20, out_features=20, bias=True)\n",
       "      )\n",
       "      (b_out): Sequential(\n",
       "        (0): PICNN(\n",
       "          (ln): LayerNorm((20, 200, 16), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(20, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (1): PICNN(\n",
       "          (ln): LayerNorm((10, 200, 16), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (2): PICNN(\n",
       "          (ln): LayerNorm((10, 200, 16), eps=1e-05, elementwise_affine=True)\n",
       "          (conv1): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (conv_out): Conv2d(10, 1, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    )\n",
       "  )\n",
       "  (_DDRM): DDRM(\n",
       "    (ddpm): DDPM(\n",
       "      (network): PIUNet(\n",
       "        (time_embed): Embedding(500, 100)\n",
       "        (te1): Sequential(\n",
       "          (0): Linear(in_features=100, out_features=1, bias=True)\n",
       "          (1): SiLU()\n",
       "          (2): Linear(in_features=1, out_features=1, bias=True)\n",
       "        )\n",
       "        (b1): Sequential(\n",
       "          (0): PICNN(\n",
       "            (ln): LayerNorm((1, 200, 16), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(1, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (1): PICNN(\n",
       "            (ln): LayerNorm((10, 200, 16), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (2): PICNN(\n",
       "            (ln): LayerNorm((10, 200, 16), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (down1): Conv2d(10, 10, kernel_size=(1, 4), stride=(1, 2), padding=(0, 1))\n",
       "        (te2): Sequential(\n",
       "          (0): Linear(in_features=100, out_features=10, bias=True)\n",
       "          (1): SiLU()\n",
       "          (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "        )\n",
       "        (b2): Sequential(\n",
       "          (0): PICNN(\n",
       "            (ln): LayerNorm((10, 200, 8), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(10, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (1): PICNN(\n",
       "            (ln): LayerNorm((20, 200, 8), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (2): PICNN(\n",
       "            (ln): LayerNorm((20, 200, 8), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (down2): Conv2d(20, 20, kernel_size=(1, 4), stride=(1, 2), padding=(0, 1))\n",
       "        (te3): Sequential(\n",
       "          (0): Linear(in_features=100, out_features=20, bias=True)\n",
       "          (1): SiLU()\n",
       "          (2): Linear(in_features=20, out_features=20, bias=True)\n",
       "        )\n",
       "        (b3): Sequential(\n",
       "          (0): PICNN(\n",
       "            (ln): LayerNorm((20, 200, 4), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(20, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (1): PICNN(\n",
       "            (ln): LayerNorm((40, 200, 4), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (2): PICNN(\n",
       "            (ln): LayerNorm((40, 200, 4), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (down3): Sequential(\n",
       "          (0): Conv2d(40, 40, kernel_size=(1, 2), stride=(1, 1))\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        )\n",
       "        (te_mid): Sequential(\n",
       "          (0): Linear(in_features=100, out_features=40, bias=True)\n",
       "          (1): SiLU()\n",
       "          (2): Linear(in_features=40, out_features=40, bias=True)\n",
       "        )\n",
       "        (b_mid): Sequential(\n",
       "          (0): PICNN(\n",
       "            (ln): LayerNorm((40, 200, 3), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(40, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (1): PICNN(\n",
       "            (ln): LayerNorm((20, 200, 3), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (2): PICNN(\n",
       "            (ln): LayerNorm((20, 200, 3), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(20, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up1): Sequential(\n",
       "          (0): ConvTranspose2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (1): SiLU()\n",
       "          (2): ConvTranspose2d(40, 40, kernel_size=(1, 2), stride=(1, 1))\n",
       "        )\n",
       "        (te4): Sequential(\n",
       "          (0): Linear(in_features=100, out_features=80, bias=True)\n",
       "          (1): SiLU()\n",
       "          (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "        )\n",
       "        (b4): Sequential(\n",
       "          (0): PICNN(\n",
       "            (ln): LayerNorm((80, 200, 4), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(80, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(40, 40, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (1): PICNN(\n",
       "            (ln): LayerNorm((40, 200, 4), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(40, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (2): PICNN(\n",
       "            (ln): LayerNorm((20, 200, 4), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up2): ConvTranspose2d(20, 20, kernel_size=(1, 4), stride=(1, 2), padding=(0, 1))\n",
       "        (te5): Sequential(\n",
       "          (0): Linear(in_features=100, out_features=40, bias=True)\n",
       "          (1): SiLU()\n",
       "          (2): Linear(in_features=40, out_features=40, bias=True)\n",
       "        )\n",
       "        (b5): Sequential(\n",
       "          (0): PICNN(\n",
       "            (ln): LayerNorm((40, 200, 8), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(40, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(20, 20, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (1): PICNN(\n",
       "            (ln): LayerNorm((20, 200, 8), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(20, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (2): PICNN(\n",
       "            (ln): LayerNorm((10, 200, 8), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up3): ConvTranspose2d(10, 10, kernel_size=(1, 4), stride=(1, 2), padding=(0, 1))\n",
       "        (te_out): Sequential(\n",
       "          (0): Linear(in_features=100, out_features=20, bias=True)\n",
       "          (1): SiLU()\n",
       "          (2): Linear(in_features=20, out_features=20, bias=True)\n",
       "        )\n",
       "        (b_out): Sequential(\n",
       "          (0): PICNN(\n",
       "            (ln): LayerNorm((20, 200, 16), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(20, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (1): PICNN(\n",
       "            (ln): LayerNorm((10, 200, 16), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (2): PICNN(\n",
       "            (ln): LayerNorm((10, 200, 16), eps=1e-05, elementwise_affine=True)\n",
       "            (conv1): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (conv2): Conv2d(10, 10, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (conv_out): Conv2d(10, 1, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_linear): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5d2d229",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will be training/testing on 95 days\n",
      "In GCN mode\n",
      "\n",
      " ----- On train/test for day 5 ----- \n",
      "On epoch 1 of day 5 training, loss = 0.00551004\n",
      "On epoch 2 of day 5 training, loss = 0.00453584\n",
      "On epoch 3 of day 5 training, loss = 0.00363193\n",
      "On epoch 4 of day 5 training, loss = 0.00280288\n",
      "On epoch 5 of day 5 training, loss = 0.00206414\n",
      "On epoch 6 of day 5 training, loss = 0.00143306\n",
      "On epoch 7 of day 5 training, loss = 0.00092467\n",
      "On epoch 8 of day 5 training, loss = 0.00054730\n",
      "On epoch 9 of day 5 training, loss = 0.00029820\n",
      "On epoch 10 of day 5 training, loss = 0.00016061\n",
      "On epoch 11 of day 5 training, loss = 0.00010480\n",
      "On epoch 12 of day 5 training, loss = 0.00009504\n",
      "On epoch 13 of day 5 training, loss = 0.00010057\n",
      "On epoch 14 of day 5 training, loss = 0.00010399\n",
      "On epoch 15 of day 5 training, loss = 0.00010131\n",
      "On epoch 16 of day 5 training, loss = 0.00009564\n",
      "On epoch 17 of day 5 training, loss = 0.00009070\n",
      "On epoch 18 of day 5 training, loss = 0.00008800\n",
      "On epoch 19 of day 5 training, loss = 0.00008711\n",
      "On epoch 20 of day 5 training, loss = 0.00008701\n",
      "On epoch 21 of day 5 training, loss = 0.00008693\n",
      "On epoch 22 of day 5 training, loss = 0.00008661\n",
      "On epoch 23 of day 5 training, loss = 0.00008609\n",
      "On epoch 24 of day 5 training, loss = 0.00008550\n",
      "On epoch 25 of day 5 training, loss = 0.00008492\n",
      "Current test pnl = 0.001197966281324625\n",
      " ----- On train/test for day 6 ----- \n",
      "On epoch 1 of day 6 training, loss = 0.00007813\n",
      "On epoch 2 of day 6 training, loss = 0.00007085\n",
      "On epoch 3 of day 6 training, loss = 0.00006940\n",
      "On epoch 4 of day 6 training, loss = 0.00006803\n",
      "On epoch 5 of day 6 training, loss = 0.00006742\n",
      "On epoch 6 of day 6 training, loss = 0.00006642\n",
      "On epoch 7 of day 6 training, loss = 0.00006541\n",
      "On epoch 8 of day 6 training, loss = 0.00006450\n",
      "On epoch 9 of day 6 training, loss = 0.00006363\n",
      "On epoch 10 of day 6 training, loss = 0.00006287\n",
      "On epoch 11 of day 6 training, loss = 0.00006212\n",
      "On epoch 12 of day 6 training, loss = 0.00006133\n",
      "On epoch 13 of day 6 training, loss = 0.00006054\n",
      "On epoch 14 of day 6 training, loss = 0.00005979\n",
      "On epoch 15 of day 6 training, loss = 0.00005907\n",
      "On epoch 16 of day 6 training, loss = 0.00005838\n",
      "On epoch 17 of day 6 training, loss = 0.00005769\n",
      "On epoch 18 of day 6 training, loss = 0.00005701\n",
      "On epoch 19 of day 6 training, loss = 0.00005634\n",
      "On epoch 20 of day 6 training, loss = 0.00005569\n",
      "On epoch 21 of day 6 training, loss = 0.00005506\n",
      "On epoch 22 of day 6 training, loss = 0.00005445\n",
      "On epoch 23 of day 6 training, loss = 0.00005384\n",
      "On epoch 24 of day 6 training, loss = 0.00005325\n",
      "On epoch 25 of day 6 training, loss = 0.00005267\n",
      "Current test pnl = -0.0028822170570492744\n",
      " ----- On train/test for day 7 ----- \n",
      "On epoch 1 of day 7 training, loss = 0.00005607\n",
      "On epoch 2 of day 7 training, loss = 0.00003859\n",
      "On epoch 3 of day 7 training, loss = 0.00004548\n",
      "On epoch 4 of day 7 training, loss = 0.00004533\n",
      "On epoch 5 of day 7 training, loss = 0.00004262\n",
      "On epoch 6 of day 7 training, loss = 0.00003962\n",
      "On epoch 7 of day 7 training, loss = 0.00003800\n",
      "On epoch 8 of day 7 training, loss = 0.00003827\n",
      "On epoch 9 of day 7 training, loss = 0.00003815\n",
      "On epoch 10 of day 7 training, loss = 0.00003715\n",
      "On epoch 11 of day 7 training, loss = 0.00003585\n",
      "On epoch 12 of day 7 training, loss = 0.00003487\n",
      "On epoch 13 of day 7 training, loss = 0.00003435\n",
      "On epoch 14 of day 7 training, loss = 0.00003388\n",
      "On epoch 15 of day 7 training, loss = 0.00003321\n",
      "On epoch 16 of day 7 training, loss = 0.00003242\n",
      "On epoch 17 of day 7 training, loss = 0.00003172\n",
      "On epoch 18 of day 7 training, loss = 0.00003116\n",
      "On epoch 19 of day 7 training, loss = 0.00003064\n",
      "On epoch 20 of day 7 training, loss = 0.00003008\n",
      "On epoch 21 of day 7 training, loss = 0.00002950\n",
      "On epoch 22 of day 7 training, loss = 0.00002895\n",
      "On epoch 23 of day 7 training, loss = 0.00002845\n",
      "On epoch 24 of day 7 training, loss = 0.00002798\n",
      "On epoch 25 of day 7 training, loss = 0.00002750\n",
      "Current test pnl = -0.0024961517192423344\n",
      " ----- On train/test for day 8 ----- \n",
      "On epoch 1 of day 8 training, loss = 0.00003967\n",
      "On epoch 2 of day 8 training, loss = 0.00003423\n",
      "On epoch 3 of day 8 training, loss = 0.00003780\n",
      "On epoch 4 of day 8 training, loss = 0.00003478\n",
      "On epoch 5 of day 8 training, loss = 0.00003134\n",
      "On epoch 6 of day 8 training, loss = 0.00003101\n",
      "On epoch 7 of day 8 training, loss = 0.00003223\n",
      "On epoch 8 of day 8 training, loss = 0.00003163\n",
      "On epoch 9 of day 8 training, loss = 0.00003043\n",
      "On epoch 10 of day 8 training, loss = 0.00002979\n",
      "On epoch 11 of day 8 training, loss = 0.00002996\n",
      "On epoch 12 of day 8 training, loss = 0.00002978\n",
      "On epoch 13 of day 8 training, loss = 0.00002922\n",
      "On epoch 14 of day 8 training, loss = 0.00002879\n",
      "On epoch 15 of day 8 training, loss = 0.00002865\n",
      "On epoch 16 of day 8 training, loss = 0.00002846\n",
      "On epoch 17 of day 8 training, loss = 0.00002814\n",
      "On epoch 18 of day 8 training, loss = 0.00002784\n",
      "On epoch 19 of day 8 training, loss = 0.00002765\n",
      "On epoch 20 of day 8 training, loss = 0.00002747\n",
      "On epoch 21 of day 8 training, loss = 0.00002723\n",
      "On epoch 22 of day 8 training, loss = 0.00002702\n",
      "On epoch 23 of day 8 training, loss = 0.00002685\n",
      "On epoch 24 of day 8 training, loss = 0.00002668\n",
      "On epoch 25 of day 8 training, loss = 0.00002650\n",
      "Current test pnl = -0.002540126210078597\n",
      " ----- On train/test for day 9 ----- \n",
      "On epoch 1 of day 9 training, loss = 0.00004847\n",
      "On epoch 2 of day 9 training, loss = 0.00004796\n",
      "On epoch 3 of day 9 training, loss = 0.00004609\n",
      "On epoch 4 of day 9 training, loss = 0.00004353\n",
      "On epoch 5 of day 9 training, loss = 0.00004329\n",
      "On epoch 6 of day 9 training, loss = 0.00004289\n",
      "On epoch 7 of day 9 training, loss = 0.00004174\n",
      "On epoch 8 of day 9 training, loss = 0.00004084\n",
      "On epoch 9 of day 9 training, loss = 0.00004054\n",
      "On epoch 10 of day 9 training, loss = 0.00003983\n",
      "On epoch 11 of day 9 training, loss = 0.00003907\n",
      "On epoch 12 of day 9 training, loss = 0.00003861\n",
      "On epoch 13 of day 9 training, loss = 0.00003808\n",
      "On epoch 14 of day 9 training, loss = 0.00003747\n",
      "On epoch 15 of day 9 training, loss = 0.00003699\n",
      "On epoch 16 of day 9 training, loss = 0.00003652\n",
      "On epoch 17 of day 9 training, loss = 0.00003602\n",
      "On epoch 18 of day 9 training, loss = 0.00003557\n",
      "On epoch 19 of day 9 training, loss = 0.00003515\n",
      "On epoch 20 of day 9 training, loss = 0.00003472\n",
      "On epoch 21 of day 9 training, loss = 0.00003431\n",
      "On epoch 22 of day 9 training, loss = 0.00003393\n",
      "On epoch 23 of day 9 training, loss = 0.00003356\n",
      "On epoch 24 of day 9 training, loss = 0.00003319\n",
      "On epoch 25 of day 9 training, loss = 0.00003285\n",
      "Current test pnl = 0.00028715416556224227\n",
      " ----- On train/test for day 10 ----- \n",
      "On epoch 1 of day 10 training, loss = 0.00004872\n",
      "On epoch 2 of day 10 training, loss = 0.00004867\n",
      "On epoch 3 of day 10 training, loss = 0.00004737\n",
      "On epoch 4 of day 10 training, loss = 0.00004518\n",
      "On epoch 5 of day 10 training, loss = 0.00004554\n",
      "On epoch 6 of day 10 training, loss = 0.00004538\n",
      "On epoch 7 of day 10 training, loss = 0.00004435\n",
      "On epoch 8 of day 10 training, loss = 0.00004392\n",
      "On epoch 9 of day 10 training, loss = 0.00004396\n",
      "On epoch 10 of day 10 training, loss = 0.00004342\n",
      "On epoch 11 of day 10 training, loss = 0.00004292\n",
      "On epoch 12 of day 10 training, loss = 0.00004280\n",
      "On epoch 13 of day 10 training, loss = 0.00004250\n",
      "On epoch 14 of day 10 training, loss = 0.00004208\n",
      "On epoch 15 of day 10 training, loss = 0.00004186\n",
      "On epoch 16 of day 10 training, loss = 0.00004163\n",
      "On epoch 17 of day 10 training, loss = 0.00004131\n",
      "On epoch 18 of day 10 training, loss = 0.00004106\n",
      "On epoch 19 of day 10 training, loss = 0.00004084\n",
      "On epoch 20 of day 10 training, loss = 0.00004058\n",
      "On epoch 21 of day 10 training, loss = 0.00004034\n",
      "On epoch 22 of day 10 training, loss = 0.00004013\n",
      "On epoch 23 of day 10 training, loss = 0.00003991\n",
      "On epoch 24 of day 10 training, loss = 0.00003969\n",
      "On epoch 25 of day 10 training, loss = 0.00003949\n",
      "Current test pnl = -0.004294730257242918\n",
      " ----- On train/test for day 11 ----- \n",
      "On epoch 1 of day 11 training, loss = 0.00007071\n",
      "On epoch 2 of day 11 training, loss = 0.00006267\n",
      "On epoch 3 of day 11 training, loss = 0.00006507\n",
      "On epoch 4 of day 11 training, loss = 0.00006391\n",
      "On epoch 5 of day 11 training, loss = 0.00006167\n",
      "On epoch 6 of day 11 training, loss = 0.00006008\n",
      "On epoch 7 of day 11 training, loss = 0.00005992\n",
      "On epoch 8 of day 11 training, loss = 0.00005937\n",
      "On epoch 9 of day 11 training, loss = 0.00005823\n",
      "On epoch 10 of day 11 training, loss = 0.00005716\n",
      "On epoch 11 of day 11 training, loss = 0.00005654\n",
      "On epoch 12 of day 11 training, loss = 0.00005600\n",
      "On epoch 13 of day 11 training, loss = 0.00005524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On epoch 14 of day 11 training, loss = 0.00005446\n",
      "On epoch 15 of day 11 training, loss = 0.00005384\n",
      "On epoch 16 of day 11 training, loss = 0.00005329\n",
      "On epoch 17 of day 11 training, loss = 0.00005269\n",
      "On epoch 18 of day 11 training, loss = 0.00005209\n",
      "On epoch 19 of day 11 training, loss = 0.00005154\n",
      "On epoch 20 of day 11 training, loss = 0.00005104\n",
      "On epoch 21 of day 11 training, loss = 0.00005054\n",
      "On epoch 22 of day 11 training, loss = 0.00005005\n",
      "On epoch 23 of day 11 training, loss = 0.00004959\n",
      "On epoch 24 of day 11 training, loss = 0.00004915\n",
      "On epoch 25 of day 11 training, loss = 0.00004873\n",
      "Current test pnl = 0.0011091406922787428\n",
      " ----- On train/test for day 12 ----- \n",
      "On epoch 1 of day 12 training, loss = 0.00006937\n",
      "On epoch 2 of day 12 training, loss = 0.00006564\n",
      "On epoch 3 of day 12 training, loss = 0.00006598\n",
      "On epoch 4 of day 12 training, loss = 0.00006390\n",
      "On epoch 5 of day 12 training, loss = 0.00006201\n",
      "On epoch 6 of day 12 training, loss = 0.00006256\n",
      "On epoch 7 of day 12 training, loss = 0.00006202\n",
      "On epoch 8 of day 12 training, loss = 0.00006079\n",
      "On epoch 9 of day 12 training, loss = 0.00006021\n",
      "On epoch 10 of day 12 training, loss = 0.00006010\n",
      "On epoch 11 of day 12 training, loss = 0.00005953\n",
      "On epoch 12 of day 12 training, loss = 0.00005881\n",
      "On epoch 13 of day 12 training, loss = 0.00005846\n",
      "On epoch 14 of day 12 training, loss = 0.00005815\n",
      "On epoch 15 of day 12 training, loss = 0.00005765\n",
      "On epoch 16 of day 12 training, loss = 0.00005720\n",
      "On epoch 17 of day 12 training, loss = 0.00005689\n",
      "On epoch 18 of day 12 training, loss = 0.00005654\n",
      "On epoch 19 of day 12 training, loss = 0.00005614\n",
      "On epoch 20 of day 12 training, loss = 0.00005581\n",
      "On epoch 21 of day 12 training, loss = 0.00005550\n",
      "On epoch 22 of day 12 training, loss = 0.00005519\n",
      "On epoch 23 of day 12 training, loss = 0.00005487\n",
      "On epoch 24 of day 12 training, loss = 0.00005458\n",
      "On epoch 25 of day 12 training, loss = 0.00005431\n",
      "Current test pnl = 7.258116966113448e-05\n",
      " ----- On train/test for day 13 ----- \n",
      "On epoch 1 of day 13 training, loss = 0.00006895\n",
      "On epoch 2 of day 13 training, loss = 0.00007060\n",
      "On epoch 3 of day 13 training, loss = 0.00006924\n",
      "On epoch 4 of day 13 training, loss = 0.00006620\n",
      "On epoch 5 of day 13 training, loss = 0.00006340\n",
      "On epoch 6 of day 13 training, loss = 0.00006387\n",
      "On epoch 7 of day 13 training, loss = 0.00006389\n",
      "On epoch 8 of day 13 training, loss = 0.00006276\n",
      "On epoch 9 of day 13 training, loss = 0.00006147\n",
      "On epoch 10 of day 13 training, loss = 0.00006080\n",
      "On epoch 11 of day 13 training, loss = 0.00006057\n",
      "On epoch 12 of day 13 training, loss = 0.00006003\n",
      "On epoch 13 of day 13 training, loss = 0.00005925\n",
      "On epoch 14 of day 13 training, loss = 0.00005858\n",
      "On epoch 15 of day 13 training, loss = 0.00005813\n",
      "On epoch 16 of day 13 training, loss = 0.00005768\n",
      "On epoch 17 of day 13 training, loss = 0.00005715\n",
      "On epoch 18 of day 13 training, loss = 0.00005660\n",
      "On epoch 19 of day 13 training, loss = 0.00005613\n",
      "On epoch 20 of day 13 training, loss = 0.00005571\n",
      "On epoch 21 of day 13 training, loss = 0.00005528\n",
      "On epoch 22 of day 13 training, loss = 0.00005483\n",
      "On epoch 23 of day 13 training, loss = 0.00005441\n",
      "On epoch 24 of day 13 training, loss = 0.00005403\n",
      "On epoch 25 of day 13 training, loss = 0.00005365\n",
      "Current test pnl = 0.0012905012117698789\n",
      " ----- On train/test for day 14 ----- \n",
      "On epoch 1 of day 14 training, loss = 0.00006155\n",
      "On epoch 2 of day 14 training, loss = 0.00005899\n",
      "On epoch 3 of day 14 training, loss = 0.00005832\n",
      "On epoch 4 of day 14 training, loss = 0.00005735\n",
      "On epoch 5 of day 14 training, loss = 0.00005642\n",
      "On epoch 6 of day 14 training, loss = 0.00005611\n",
      "On epoch 7 of day 14 training, loss = 0.00005585\n",
      "On epoch 8 of day 14 training, loss = 0.00005534\n",
      "On epoch 9 of day 14 training, loss = 0.00005481\n",
      "On epoch 10 of day 14 training, loss = 0.00005437\n",
      "On epoch 11 of day 14 training, loss = 0.00005405\n",
      "On epoch 12 of day 14 training, loss = 0.00005371\n",
      "On epoch 13 of day 14 training, loss = 0.00005332\n",
      "On epoch 14 of day 14 training, loss = 0.00005294\n",
      "On epoch 15 of day 14 training, loss = 0.00005260\n",
      "On epoch 16 of day 14 training, loss = 0.00005229\n",
      "On epoch 17 of day 14 training, loss = 0.00005197\n",
      "On epoch 18 of day 14 training, loss = 0.00005165\n",
      "On epoch 19 of day 14 training, loss = 0.00005134\n",
      "On epoch 20 of day 14 training, loss = 0.00005105\n",
      "On epoch 21 of day 14 training, loss = 0.00005076\n",
      "On epoch 22 of day 14 training, loss = 0.00005048\n",
      "On epoch 23 of day 14 training, loss = 0.00005021\n",
      "On epoch 24 of day 14 training, loss = 0.00004994\n",
      "On epoch 25 of day 14 training, loss = 0.00004969\n",
      "Current test pnl = 0.0007685598684474826\n",
      " ----- On train/test for day 15 ----- \n",
      "On epoch 1 of day 15 training, loss = 0.00006515\n",
      "On epoch 2 of day 15 training, loss = 0.00005920\n",
      "On epoch 3 of day 15 training, loss = 0.00006262\n",
      "On epoch 4 of day 15 training, loss = 0.00006062\n",
      "On epoch 5 of day 15 training, loss = 0.00005869\n",
      "On epoch 6 of day 15 training, loss = 0.00005699\n",
      "On epoch 7 of day 15 training, loss = 0.00005666\n",
      "On epoch 8 of day 15 training, loss = 0.00005680\n",
      "On epoch 9 of day 15 training, loss = 0.00005635\n",
      "On epoch 10 of day 15 training, loss = 0.00005556\n",
      "On epoch 11 of day 15 training, loss = 0.00005479\n",
      "On epoch 12 of day 15 training, loss = 0.00005429\n",
      "On epoch 13 of day 15 training, loss = 0.00005399\n",
      "On epoch 14 of day 15 training, loss = 0.00005364\n",
      "On epoch 15 of day 15 training, loss = 0.00005317\n",
      "On epoch 16 of day 15 training, loss = 0.00005269\n",
      "On epoch 17 of day 15 training, loss = 0.00005227\n",
      "On epoch 18 of day 15 training, loss = 0.00005192\n",
      "On epoch 19 of day 15 training, loss = 0.00005158\n",
      "On epoch 20 of day 15 training, loss = 0.00005123\n",
      "On epoch 21 of day 15 training, loss = 0.00005087\n",
      "On epoch 22 of day 15 training, loss = 0.00005053\n",
      "On epoch 23 of day 15 training, loss = 0.00005021\n",
      "On epoch 24 of day 15 training, loss = 0.00004991\n",
      "On epoch 25 of day 15 training, loss = 0.00004962\n",
      "Current test pnl = -0.0035527069121599197\n",
      " ----- On train/test for day 16 ----- \n",
      "On epoch 1 of day 16 training, loss = 0.00007560\n",
      "On epoch 2 of day 16 training, loss = 0.00006901\n",
      "On epoch 3 of day 16 training, loss = 0.00006751\n",
      "On epoch 4 of day 16 training, loss = 0.00006702\n",
      "On epoch 5 of day 16 training, loss = 0.00006573\n",
      "On epoch 6 of day 16 training, loss = 0.00006417\n",
      "On epoch 7 of day 16 training, loss = 0.00006282\n",
      "On epoch 8 of day 16 training, loss = 0.00006190\n",
      "On epoch 9 of day 16 training, loss = 0.00006107\n",
      "On epoch 10 of day 16 training, loss = 0.00006009\n",
      "On epoch 11 of day 16 training, loss = 0.00005905\n",
      "On epoch 12 of day 16 training, loss = 0.00005811\n",
      "On epoch 13 of day 16 training, loss = 0.00005730\n",
      "On epoch 14 of day 16 training, loss = 0.00005653\n",
      "On epoch 15 of day 16 training, loss = 0.00005573\n",
      "On epoch 16 of day 16 training, loss = 0.00005493\n",
      "On epoch 17 of day 16 training, loss = 0.00005419\n",
      "On epoch 18 of day 16 training, loss = 0.00005349\n",
      "On epoch 19 of day 16 training, loss = 0.00005282\n",
      "On epoch 20 of day 16 training, loss = 0.00005215\n",
      "On epoch 21 of day 16 training, loss = 0.00005150\n",
      "On epoch 22 of day 16 training, loss = 0.00005088\n",
      "On epoch 23 of day 16 training, loss = 0.00005028\n",
      "On epoch 24 of day 16 training, loss = 0.00004971\n",
      "On epoch 25 of day 16 training, loss = 0.00004914\n",
      "Current test pnl = -0.0005891692126169801\n",
      " ----- On train/test for day 17 ----- \n",
      "On epoch 1 of day 17 training, loss = 0.00006891\n",
      "On epoch 2 of day 17 training, loss = 0.00005182\n",
      "On epoch 3 of day 17 training, loss = 0.00005948\n",
      "On epoch 4 of day 17 training, loss = 0.00005909\n",
      "On epoch 5 of day 17 training, loss = 0.00005684\n",
      "On epoch 6 of day 17 training, loss = 0.00005421\n",
      "On epoch 7 of day 17 training, loss = 0.00005295\n",
      "On epoch 8 of day 17 training, loss = 0.00005358\n",
      "On epoch 9 of day 17 training, loss = 0.00005388\n",
      "On epoch 10 of day 17 training, loss = 0.00005332\n",
      "On epoch 11 of day 17 training, loss = 0.00005242\n",
      "On epoch 12 of day 17 training, loss = 0.00005172\n",
      "On epoch 13 of day 17 training, loss = 0.00005148\n",
      "On epoch 14 of day 17 training, loss = 0.00005138\n",
      "On epoch 15 of day 17 training, loss = 0.00005111\n",
      "On epoch 16 of day 17 training, loss = 0.00005068\n",
      "On epoch 17 of day 17 training, loss = 0.00005027\n",
      "On epoch 18 of day 17 training, loss = 0.00004997\n",
      "On epoch 19 of day 17 training, loss = 0.00004975\n",
      "On epoch 20 of day 17 training, loss = 0.00004952\n",
      "On epoch 21 of day 17 training, loss = 0.00004925\n",
      "On epoch 22 of day 17 training, loss = 0.00004897\n",
      "On epoch 23 of day 17 training, loss = 0.00004872\n",
      "On epoch 24 of day 17 training, loss = 0.00004851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On epoch 25 of day 17 training, loss = 0.00004830\n",
      "Current test pnl = -0.004036452621221542\n",
      " ----- On train/test for day 18 ----- \n",
      "On epoch 1 of day 18 training, loss = 0.00007780\n",
      "On epoch 2 of day 18 training, loss = 0.00007426\n",
      "On epoch 3 of day 18 training, loss = 0.00007348\n",
      "On epoch 4 of day 18 training, loss = 0.00007153\n",
      "On epoch 5 of day 18 training, loss = 0.00006976\n",
      "On epoch 6 of day 18 training, loss = 0.00006852\n",
      "On epoch 7 of day 18 training, loss = 0.00006749\n",
      "On epoch 8 of day 18 training, loss = 0.00006625\n",
      "On epoch 9 of day 18 training, loss = 0.00006496\n",
      "On epoch 10 of day 18 training, loss = 0.00006380\n",
      "On epoch 11 of day 18 training, loss = 0.00006277\n",
      "On epoch 12 of day 18 training, loss = 0.00006176\n",
      "On epoch 13 of day 18 training, loss = 0.00006072\n",
      "On epoch 14 of day 18 training, loss = 0.00005972\n",
      "On epoch 15 of day 18 training, loss = 0.00005878\n",
      "On epoch 16 of day 18 training, loss = 0.00005788\n",
      "On epoch 17 of day 18 training, loss = 0.00005700\n",
      "On epoch 18 of day 18 training, loss = 0.00005614\n",
      "On epoch 19 of day 18 training, loss = 0.00005532\n",
      "On epoch 20 of day 18 training, loss = 0.00005452\n",
      "On epoch 21 of day 18 training, loss = 0.00005375\n",
      "On epoch 22 of day 18 training, loss = 0.00005301\n",
      "On epoch 23 of day 18 training, loss = 0.00005228\n",
      "On epoch 24 of day 18 training, loss = 0.00005159\n",
      "On epoch 25 of day 18 training, loss = 0.00005091\n",
      "Current test pnl = -0.0007360634044744074\n",
      " ----- On train/test for day 19 ----- \n",
      "On epoch 1 of day 19 training, loss = 0.00006456\n",
      "On epoch 2 of day 19 training, loss = 0.00006199\n",
      "On epoch 3 of day 19 training, loss = 0.00006037\n",
      "On epoch 4 of day 19 training, loss = 0.00005943\n",
      "On epoch 5 of day 19 training, loss = 0.00005837\n",
      "On epoch 6 of day 19 training, loss = 0.00005788\n",
      "On epoch 7 of day 19 training, loss = 0.00005714\n",
      "On epoch 8 of day 19 training, loss = 0.00005642\n",
      "On epoch 9 of day 19 training, loss = 0.00005570\n",
      "On epoch 10 of day 19 training, loss = 0.00005514\n",
      "On epoch 11 of day 19 training, loss = 0.00005457\n",
      "On epoch 12 of day 19 training, loss = 0.00005398\n",
      "On epoch 13 of day 19 training, loss = 0.00005340\n",
      "On epoch 14 of day 19 training, loss = 0.00005288\n",
      "On epoch 15 of day 19 training, loss = 0.00005238\n",
      "On epoch 16 of day 19 training, loss = 0.00005188\n",
      "On epoch 17 of day 19 training, loss = 0.00005139\n",
      "On epoch 18 of day 19 training, loss = 0.00005093\n",
      "On epoch 19 of day 19 training, loss = 0.00005049\n",
      "On epoch 20 of day 19 training, loss = 0.00005006\n",
      "On epoch 21 of day 19 training, loss = 0.00004964\n",
      "On epoch 22 of day 19 training, loss = 0.00004923\n",
      "On epoch 23 of day 19 training, loss = 0.00004884\n",
      "On epoch 24 of day 19 training, loss = 0.00004847\n",
      "On epoch 25 of day 19 training, loss = 0.00004810\n",
      "Current test pnl = 0.0013997522182762623\n",
      " ----- On train/test for day 20 ----- \n",
      "On epoch 1 of day 20 training, loss = 0.00005443\n",
      "On epoch 2 of day 20 training, loss = 0.00005235\n",
      "On epoch 3 of day 20 training, loss = 0.00005124\n",
      "On epoch 4 of day 20 training, loss = 0.00005076\n",
      "On epoch 5 of day 20 training, loss = 0.00005019\n",
      "On epoch 6 of day 20 training, loss = 0.00004960\n",
      "On epoch 7 of day 20 training, loss = 0.00004913\n",
      "On epoch 8 of day 20 training, loss = 0.00004868\n",
      "On epoch 9 of day 20 training, loss = 0.00004824\n",
      "On epoch 10 of day 20 training, loss = 0.00004779\n",
      "On epoch 11 of day 20 training, loss = 0.00004739\n",
      "On epoch 12 of day 20 training, loss = 0.00004699\n",
      "On epoch 13 of day 20 training, loss = 0.00004661\n",
      "On epoch 14 of day 20 training, loss = 0.00004623\n",
      "On epoch 15 of day 20 training, loss = 0.00004588\n",
      "On epoch 16 of day 20 training, loss = 0.00004553\n",
      "On epoch 17 of day 20 training, loss = 0.00004519\n",
      "On epoch 18 of day 20 training, loss = 0.00004486\n",
      "On epoch 19 of day 20 training, loss = 0.00004455\n",
      "On epoch 20 of day 20 training, loss = 0.00004424\n",
      "On epoch 21 of day 20 training, loss = 0.00004394\n",
      "On epoch 22 of day 20 training, loss = 0.00004365\n",
      "On epoch 23 of day 20 training, loss = 0.00004337\n",
      "On epoch 24 of day 20 training, loss = 0.00004310\n",
      "On epoch 25 of day 20 training, loss = 0.00004283\n",
      "Current test pnl = -0.0005576604744419456\n",
      " ----- On train/test for day 21 ----- \n",
      "On epoch 1 of day 21 training, loss = 0.00006507\n",
      "On epoch 2 of day 21 training, loss = 0.00006444\n",
      "On epoch 3 of day 21 training, loss = 0.00006322\n",
      "On epoch 4 of day 21 training, loss = 0.00006137\n",
      "On epoch 5 of day 21 training, loss = 0.00006035\n",
      "On epoch 6 of day 21 training, loss = 0.00006074\n",
      "On epoch 7 of day 21 training, loss = 0.00006003\n",
      "On epoch 8 of day 21 training, loss = 0.00005907\n",
      "On epoch 9 of day 21 training, loss = 0.00005869\n",
      "On epoch 10 of day 21 training, loss = 0.00005853\n",
      "On epoch 11 of day 21 training, loss = 0.00005800\n",
      "On epoch 12 of day 21 training, loss = 0.00005742\n",
      "On epoch 13 of day 21 training, loss = 0.00005710\n",
      "On epoch 14 of day 21 training, loss = 0.00005680\n",
      "On epoch 15 of day 21 training, loss = 0.00005637\n",
      "On epoch 16 of day 21 training, loss = 0.00005596\n",
      "On epoch 17 of day 21 training, loss = 0.00005565\n",
      "On epoch 18 of day 21 training, loss = 0.00005533\n",
      "On epoch 19 of day 21 training, loss = 0.00005497\n",
      "On epoch 20 of day 21 training, loss = 0.00005464\n",
      "On epoch 21 of day 21 training, loss = 0.00005435\n",
      "On epoch 22 of day 21 training, loss = 0.00005405\n",
      "On epoch 23 of day 21 training, loss = 0.00005374\n",
      "On epoch 24 of day 21 training, loss = 0.00005345\n",
      "On epoch 25 of day 21 training, loss = 0.00005318\n",
      "Current test pnl = 0.0006185311358422041\n",
      " ----- On train/test for day 22 ----- \n",
      "On epoch 1 of day 22 training, loss = 0.00008231\n",
      "On epoch 2 of day 22 training, loss = 0.00007650\n",
      "On epoch 3 of day 22 training, loss = 0.00007790\n",
      "On epoch 4 of day 22 training, loss = 0.00007730\n",
      "On epoch 5 of day 22 training, loss = 0.00007597\n",
      "On epoch 6 of day 22 training, loss = 0.00007547\n",
      "On epoch 7 of day 22 training, loss = 0.00007561\n",
      "On epoch 8 of day 22 training, loss = 0.00007517\n",
      "On epoch 9 of day 22 training, loss = 0.00007453\n",
      "On epoch 10 of day 22 training, loss = 0.00007423\n",
      "On epoch 11 of day 22 training, loss = 0.00007408\n",
      "On epoch 12 of day 22 training, loss = 0.00007373\n",
      "On epoch 13 of day 22 training, loss = 0.00007334\n",
      "On epoch 14 of day 22 training, loss = 0.00007310\n",
      "On epoch 15 of day 22 training, loss = 0.00007288\n",
      "On epoch 16 of day 22 training, loss = 0.00007259\n",
      "On epoch 17 of day 22 training, loss = 0.00007231\n",
      "On epoch 18 of day 22 training, loss = 0.00007208\n",
      "On epoch 19 of day 22 training, loss = 0.00007186\n",
      "On epoch 20 of day 22 training, loss = 0.00007162\n",
      "On epoch 21 of day 22 training, loss = 0.00007139\n",
      "On epoch 22 of day 22 training, loss = 0.00007119\n",
      "On epoch 23 of day 22 training, loss = 0.00007098\n",
      "On epoch 24 of day 22 training, loss = 0.00007078\n",
      "On epoch 25 of day 22 training, loss = 0.00007058\n",
      "Current test pnl = -0.00154681911226362\n",
      " ----- On train/test for day 23 ----- \n",
      "On epoch 1 of day 23 training, loss = 0.00007737\n",
      "On epoch 2 of day 23 training, loss = 0.00007273\n",
      "On epoch 3 of day 23 training, loss = 0.00007251\n",
      "On epoch 4 of day 23 training, loss = 0.00007217\n",
      "On epoch 5 of day 23 training, loss = 0.00007120\n",
      "On epoch 6 of day 23 training, loss = 0.00007095\n",
      "On epoch 7 of day 23 training, loss = 0.00007078\n",
      "On epoch 8 of day 23 training, loss = 0.00007048\n",
      "On epoch 9 of day 23 training, loss = 0.00007005\n",
      "On epoch 10 of day 23 training, loss = 0.00006982\n",
      "On epoch 11 of day 23 training, loss = 0.00006966\n",
      "On epoch 12 of day 23 training, loss = 0.00006941\n",
      "On epoch 13 of day 23 training, loss = 0.00006914\n",
      "On epoch 14 of day 23 training, loss = 0.00006894\n",
      "On epoch 15 of day 23 training, loss = 0.00006877\n",
      "On epoch 16 of day 23 training, loss = 0.00006857\n",
      "On epoch 17 of day 23 training, loss = 0.00006837\n",
      "On epoch 18 of day 23 training, loss = 0.00006820\n",
      "On epoch 19 of day 23 training, loss = 0.00006804\n",
      "On epoch 20 of day 23 training, loss = 0.00006787\n",
      "On epoch 21 of day 23 training, loss = 0.00006770\n",
      "On epoch 22 of day 23 training, loss = 0.00006755\n",
      "On epoch 23 of day 23 training, loss = 0.00006741\n",
      "On epoch 24 of day 23 training, loss = 0.00006726\n",
      "On epoch 25 of day 23 training, loss = 0.00006711\n",
      "Current test pnl = 0.0002291837299708277\n",
      " ----- On train/test for day 24 ----- \n",
      "On epoch 1 of day 24 training, loss = 0.00008400\n",
      "On epoch 2 of day 24 training, loss = 0.00007979\n",
      "On epoch 3 of day 24 training, loss = 0.00007961\n",
      "On epoch 4 of day 24 training, loss = 0.00007939\n",
      "On epoch 5 of day 24 training, loss = 0.00007861\n",
      "On epoch 6 of day 24 training, loss = 0.00007819\n",
      "On epoch 7 of day 24 training, loss = 0.00007829\n",
      "On epoch 8 of day 24 training, loss = 0.00007787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On epoch 9 of day 24 training, loss = 0.00007748\n",
      "On epoch 10 of day 24 training, loss = 0.00007745\n",
      "On epoch 11 of day 24 training, loss = 0.00007722\n",
      "On epoch 12 of day 24 training, loss = 0.00007692\n",
      "On epoch 13 of day 24 training, loss = 0.00007680\n",
      "On epoch 14 of day 24 training, loss = 0.00007663\n",
      "On epoch 15 of day 24 training, loss = 0.00007640\n",
      "On epoch 16 of day 24 training, loss = 0.00007626\n",
      "On epoch 17 of day 24 training, loss = 0.00007612\n",
      "On epoch 18 of day 24 training, loss = 0.00007594\n",
      "On epoch 19 of day 24 training, loss = 0.00007580\n",
      "On epoch 20 of day 24 training, loss = 0.00007566\n",
      "On epoch 21 of day 24 training, loss = 0.00007551\n",
      "On epoch 22 of day 24 training, loss = 0.00007539\n",
      "On epoch 23 of day 24 training, loss = 0.00007526\n",
      "On epoch 24 of day 24 training, loss = 0.00007513\n",
      "On epoch 25 of day 24 training, loss = 0.00007502\n",
      "Current test pnl = -0.0027250745333731174\n",
      " ----- On train/test for day 25 ----- \n",
      "On epoch 1 of day 25 training, loss = 0.00008021\n",
      "On epoch 2 of day 25 training, loss = 0.00007631\n",
      "On epoch 3 of day 25 training, loss = 0.00007645\n",
      "On epoch 4 of day 25 training, loss = 0.00007566\n",
      "On epoch 5 of day 25 training, loss = 0.00007485\n",
      "On epoch 6 of day 25 training, loss = 0.00007435\n",
      "On epoch 7 of day 25 training, loss = 0.00007422\n",
      "On epoch 8 of day 25 training, loss = 0.00007372\n",
      "On epoch 9 of day 25 training, loss = 0.00007323\n",
      "On epoch 10 of day 25 training, loss = 0.00007295\n",
      "On epoch 11 of day 25 training, loss = 0.00007267\n",
      "On epoch 12 of day 25 training, loss = 0.00007229\n",
      "On epoch 13 of day 25 training, loss = 0.00007195\n",
      "On epoch 14 of day 25 training, loss = 0.00007169\n",
      "On epoch 15 of day 25 training, loss = 0.00007141\n",
      "On epoch 16 of day 25 training, loss = 0.00007110\n",
      "On epoch 17 of day 25 training, loss = 0.00007084\n",
      "On epoch 18 of day 25 training, loss = 0.00007059\n",
      "On epoch 19 of day 25 training, loss = 0.00007033\n",
      "On epoch 20 of day 25 training, loss = 0.00007008\n",
      "On epoch 21 of day 25 training, loss = 0.00006985\n",
      "On epoch 22 of day 25 training, loss = 0.00006962\n",
      "On epoch 23 of day 25 training, loss = 0.00006939\n",
      "On epoch 24 of day 25 training, loss = 0.00006918\n",
      "On epoch 25 of day 25 training, loss = 0.00006897\n",
      "Current test pnl = 0.00018118933076038957\n",
      " ----- On train/test for day 26 ----- \n",
      "On epoch 1 of day 26 training, loss = 0.00006907\n",
      "On epoch 2 of day 26 training, loss = 0.00006854\n",
      "On epoch 3 of day 26 training, loss = 0.00006799\n",
      "On epoch 4 of day 26 training, loss = 0.00006647\n",
      "On epoch 5 of day 26 training, loss = 0.00006516\n",
      "On epoch 6 of day 26 training, loss = 0.00006507\n",
      "On epoch 7 of day 26 training, loss = 0.00006514\n",
      "On epoch 8 of day 26 training, loss = 0.00006465\n",
      "On epoch 9 of day 26 training, loss = 0.00006402\n",
      "On epoch 10 of day 26 training, loss = 0.00006354\n",
      "On epoch 11 of day 26 training, loss = 0.00006332\n",
      "On epoch 12 of day 26 training, loss = 0.00006308\n",
      "On epoch 13 of day 26 training, loss = 0.00006271\n",
      "On epoch 14 of day 26 training, loss = 0.00006232\n",
      "On epoch 15 of day 26 training, loss = 0.00006199\n",
      "On epoch 16 of day 26 training, loss = 0.00006173\n",
      "On epoch 17 of day 26 training, loss = 0.00006145\n",
      "On epoch 18 of day 26 training, loss = 0.00006115\n",
      "On epoch 19 of day 26 training, loss = 0.00006085\n",
      "On epoch 20 of day 26 training, loss = 0.00006058\n",
      "On epoch 21 of day 26 training, loss = 0.00006033\n",
      "On epoch 22 of day 26 training, loss = 0.00006008\n",
      "On epoch 23 of day 26 training, loss = 0.00005982\n",
      "On epoch 24 of day 26 training, loss = 0.00005957\n",
      "On epoch 25 of day 26 training, loss = 0.00005934\n",
      "Current test pnl = 0.0005428426084108651\n",
      " ----- On train/test for day 27 ----- \n",
      "On epoch 1 of day 27 training, loss = 0.00004590\n",
      "On epoch 2 of day 27 training, loss = 0.00004446\n",
      "On epoch 3 of day 27 training, loss = 0.00004381\n",
      "On epoch 4 of day 27 training, loss = 0.00004346\n",
      "On epoch 5 of day 27 training, loss = 0.00004298\n",
      "On epoch 6 of day 27 training, loss = 0.00004279\n",
      "On epoch 7 of day 27 training, loss = 0.00004256\n",
      "On epoch 8 of day 27 training, loss = 0.00004231\n",
      "On epoch 9 of day 27 training, loss = 0.00004206\n",
      "On epoch 10 of day 27 training, loss = 0.00004185\n",
      "On epoch 11 of day 27 training, loss = 0.00004166\n",
      "On epoch 12 of day 27 training, loss = 0.00004147\n",
      "On epoch 13 of day 27 training, loss = 0.00004127\n",
      "On epoch 14 of day 27 training, loss = 0.00004108\n",
      "On epoch 15 of day 27 training, loss = 0.00004091\n",
      "On epoch 16 of day 27 training, loss = 0.00004074\n",
      "On epoch 17 of day 27 training, loss = 0.00004057\n",
      "On epoch 18 of day 27 training, loss = 0.00004041\n",
      "On epoch 19 of day 27 training, loss = 0.00004026\n",
      "On epoch 20 of day 27 training, loss = 0.00004011\n",
      "On epoch 21 of day 27 training, loss = 0.00003996\n",
      "On epoch 22 of day 27 training, loss = 0.00003982\n",
      "On epoch 23 of day 27 training, loss = 0.00003968\n",
      "On epoch 24 of day 27 training, loss = 0.00003955\n",
      "On epoch 25 of day 27 training, loss = 0.00003942\n",
      "Current test pnl = -0.0013023917563259602\n",
      " ----- On train/test for day 28 ----- \n",
      "On epoch 1 of day 28 training, loss = 0.00004762\n",
      "On epoch 2 of day 28 training, loss = 0.00004654\n",
      "On epoch 3 of day 28 training, loss = 0.00004513\n",
      "On epoch 4 of day 28 training, loss = 0.00004448\n",
      "On epoch 5 of day 28 training, loss = 0.00004373\n",
      "On epoch 6 of day 28 training, loss = 0.00004354\n",
      "On epoch 7 of day 28 training, loss = 0.00004302\n",
      "On epoch 8 of day 28 training, loss = 0.00004254\n",
      "On epoch 9 of day 28 training, loss = 0.00004209\n",
      "On epoch 10 of day 28 training, loss = 0.00004178\n",
      "On epoch 11 of day 28 training, loss = 0.00004140\n",
      "On epoch 12 of day 28 training, loss = 0.00004100\n",
      "On epoch 13 of day 28 training, loss = 0.00004063\n",
      "On epoch 14 of day 28 training, loss = 0.00004030\n",
      "On epoch 15 of day 28 training, loss = 0.00003997\n",
      "On epoch 16 of day 28 training, loss = 0.00003962\n",
      "On epoch 17 of day 28 training, loss = 0.00003930\n",
      "On epoch 18 of day 28 training, loss = 0.00003899\n",
      "On epoch 19 of day 28 training, loss = 0.00003868\n",
      "On epoch 20 of day 28 training, loss = 0.00003837\n",
      "On epoch 21 of day 28 training, loss = 0.00003808\n",
      "On epoch 22 of day 28 training, loss = 0.00003779\n",
      "On epoch 23 of day 28 training, loss = 0.00003751\n",
      "On epoch 24 of day 28 training, loss = 0.00003723\n",
      "On epoch 25 of day 28 training, loss = 0.00003696\n",
      "Current test pnl = 0.00586942071095109\n",
      " ----- On train/test for day 29 ----- \n",
      "On epoch 1 of day 29 training, loss = 0.00003048\n",
      "On epoch 2 of day 29 training, loss = 0.00002789\n",
      "On epoch 3 of day 29 training, loss = 0.00002775\n",
      "On epoch 4 of day 29 training, loss = 0.00002652\n",
      "On epoch 5 of day 29 training, loss = 0.00002521\n",
      "On epoch 6 of day 29 training, loss = 0.00002516\n",
      "On epoch 7 of day 29 training, loss = 0.00002509\n",
      "On epoch 8 of day 29 training, loss = 0.00002455\n",
      "On epoch 9 of day 29 training, loss = 0.00002396\n",
      "On epoch 10 of day 29 training, loss = 0.00002363\n",
      "On epoch 11 of day 29 training, loss = 0.00002344\n",
      "On epoch 12 of day 29 training, loss = 0.00002311\n",
      "On epoch 13 of day 29 training, loss = 0.00002272\n",
      "On epoch 14 of day 29 training, loss = 0.00002239\n",
      "On epoch 15 of day 29 training, loss = 0.00002215\n",
      "On epoch 16 of day 29 training, loss = 0.00002187\n",
      "On epoch 17 of day 29 training, loss = 0.00002157\n",
      "On epoch 18 of day 29 training, loss = 0.00002129\n",
      "On epoch 19 of day 29 training, loss = 0.00002104\n",
      "On epoch 20 of day 29 training, loss = 0.00002079\n",
      "On epoch 21 of day 29 training, loss = 0.00002054\n",
      "On epoch 22 of day 29 training, loss = 0.00002030\n",
      "On epoch 23 of day 29 training, loss = 0.00002007\n",
      "On epoch 24 of day 29 training, loss = 0.00001984\n",
      "On epoch 25 of day 29 training, loss = 0.00001962\n",
      "Current test pnl = 0.003581546014174819\n",
      " ----- On train/test for day 30 ----- \n",
      "On epoch 1 of day 30 training, loss = 0.00002730\n",
      "On epoch 2 of day 30 training, loss = 0.00002004\n",
      "On epoch 3 of day 30 training, loss = 0.00002108\n",
      "On epoch 4 of day 30 training, loss = 0.00002030\n",
      "On epoch 5 of day 30 training, loss = 0.00001860\n",
      "On epoch 6 of day 30 training, loss = 0.00001716\n",
      "On epoch 7 of day 30 training, loss = 0.00001668\n",
      "On epoch 8 of day 30 training, loss = 0.00001630\n",
      "On epoch 9 of day 30 training, loss = 0.00001549\n",
      "On epoch 10 of day 30 training, loss = 0.00001459\n",
      "On epoch 11 of day 30 training, loss = 0.00001393\n",
      "On epoch 12 of day 30 training, loss = 0.00001348\n",
      "On epoch 13 of day 30 training, loss = 0.00001297\n",
      "On epoch 14 of day 30 training, loss = 0.00001238\n",
      "On epoch 15 of day 30 training, loss = 0.00001183\n",
      "On epoch 16 of day 30 training, loss = 0.00001139\n",
      "On epoch 17 of day 30 training, loss = 0.00001099\n",
      "On epoch 18 of day 30 training, loss = 0.00001057\n",
      "On epoch 19 of day 30 training, loss = 0.00001016\n",
      "On epoch 20 of day 30 training, loss = 0.00000979\n",
      "On epoch 21 of day 30 training, loss = 0.00000945\n",
      "On epoch 22 of day 30 training, loss = 0.00000913\n",
      "On epoch 23 of day 30 training, loss = 0.00000882\n",
      "On epoch 24 of day 30 training, loss = 0.00000853\n",
      "On epoch 25 of day 30 training, loss = 0.00000826\n",
      "Current test pnl = 0.001367076300084591\n",
      " ----- On train/test for day 31 ----- \n",
      "On epoch 1 of day 31 training, loss = 0.00004912\n",
      "On epoch 2 of day 31 training, loss = 0.00003137\n",
      "On epoch 3 of day 31 training, loss = 0.00003721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On epoch 4 of day 31 training, loss = 0.00003377\n",
      "On epoch 5 of day 31 training, loss = 0.00003087\n",
      "On epoch 6 of day 31 training, loss = 0.00002709\n",
      "On epoch 7 of day 31 training, loss = 0.00002630\n",
      "On epoch 8 of day 31 training, loss = 0.00002596\n",
      "On epoch 9 of day 31 training, loss = 0.00002454\n",
      "On epoch 10 of day 31 training, loss = 0.00002274\n",
      "On epoch 11 of day 31 training, loss = 0.00002126\n",
      "On epoch 12 of day 31 training, loss = 0.00002046\n",
      "On epoch 13 of day 31 training, loss = 0.00001965\n",
      "On epoch 14 of day 31 training, loss = 0.00001857\n",
      "On epoch 15 of day 31 training, loss = 0.00001745\n",
      "On epoch 16 of day 31 training, loss = 0.00001655\n",
      "On epoch 17 of day 31 training, loss = 0.00001581\n",
      "On epoch 18 of day 31 training, loss = 0.00001504\n",
      "On epoch 19 of day 31 training, loss = 0.00001423\n",
      "On epoch 20 of day 31 training, loss = 0.00001347\n",
      "On epoch 21 of day 31 training, loss = 0.00001280\n",
      "On epoch 22 of day 31 training, loss = 0.00001218\n",
      "On epoch 23 of day 31 training, loss = 0.00001156\n",
      "On epoch 24 of day 31 training, loss = 0.00001096\n",
      "On epoch 25 of day 31 training, loss = 0.00001040\n",
      "Current test pnl = 0.0004832021950278431\n",
      " ----- On train/test for day 32 ----- \n",
      "On epoch 1 of day 32 training, loss = 0.00004116\n",
      "On epoch 2 of day 32 training, loss = 0.00003326\n",
      "On epoch 3 of day 32 training, loss = 0.00003124\n",
      "On epoch 4 of day 32 training, loss = 0.00003044\n",
      "On epoch 5 of day 32 training, loss = 0.00002891\n",
      "On epoch 6 of day 32 training, loss = 0.00002730\n",
      "On epoch 7 of day 32 training, loss = 0.00002586\n",
      "On epoch 8 of day 32 training, loss = 0.00002477\n",
      "On epoch 9 of day 32 training, loss = 0.00002389\n",
      "On epoch 10 of day 32 training, loss = 0.00002297\n",
      "On epoch 11 of day 32 training, loss = 0.00002200\n",
      "On epoch 12 of day 32 training, loss = 0.00002110\n",
      "On epoch 13 of day 32 training, loss = 0.00002030\n",
      "On epoch 14 of day 32 training, loss = 0.00001960\n",
      "On epoch 15 of day 32 training, loss = 0.00001893\n",
      "On epoch 16 of day 32 training, loss = 0.00001826\n",
      "On epoch 17 of day 32 training, loss = 0.00001763\n",
      "On epoch 18 of day 32 training, loss = 0.00001705\n",
      "On epoch 19 of day 32 training, loss = 0.00001651\n",
      "On epoch 20 of day 32 training, loss = 0.00001600\n",
      "On epoch 21 of day 32 training, loss = 0.00001551\n",
      "On epoch 22 of day 32 training, loss = 0.00001504\n",
      "On epoch 23 of day 32 training, loss = 0.00001460\n",
      "On epoch 24 of day 32 training, loss = 0.00001418\n",
      "On epoch 25 of day 32 training, loss = 0.00001379\n",
      "Current test pnl = -8.13614169601351e-05\n",
      " ----- On train/test for day 33 ----- \n",
      "On epoch 1 of day 33 training, loss = 0.00006373\n",
      "On epoch 2 of day 33 training, loss = 0.00003904\n",
      "On epoch 3 of day 33 training, loss = 0.00004782\n",
      "On epoch 4 of day 33 training, loss = 0.00004907\n",
      "On epoch 5 of day 33 training, loss = 0.00004652\n",
      "On epoch 6 of day 33 training, loss = 0.00004358\n",
      "On epoch 7 of day 33 training, loss = 0.00004135\n",
      "On epoch 8 of day 33 training, loss = 0.00004189\n",
      "On epoch 9 of day 33 training, loss = 0.00004271\n",
      "On epoch 10 of day 33 training, loss = 0.00004241\n",
      "On epoch 11 of day 33 training, loss = 0.00004149\n",
      "On epoch 12 of day 33 training, loss = 0.00004062\n",
      "On epoch 13 of day 33 training, loss = 0.00004033\n",
      "On epoch 14 of day 33 training, loss = 0.00004036\n",
      "On epoch 15 of day 33 training, loss = 0.00004024\n",
      "On epoch 16 of day 33 training, loss = 0.00003989\n",
      "On epoch 17 of day 33 training, loss = 0.00003948\n",
      "On epoch 18 of day 33 training, loss = 0.00003920\n",
      "On epoch 19 of day 33 training, loss = 0.00003904\n",
      "On epoch 20 of day 33 training, loss = 0.00003890\n",
      "On epoch 21 of day 33 training, loss = 0.00003869\n",
      "On epoch 22 of day 33 training, loss = 0.00003846\n",
      "On epoch 23 of day 33 training, loss = 0.00003826\n",
      "On epoch 24 of day 33 training, loss = 0.00003809\n",
      "On epoch 25 of day 33 training, loss = 0.00003795\n",
      "Current test pnl = -2.2593139874516055e-05\n",
      " ----- On train/test for day 34 ----- \n",
      "On epoch 1 of day 34 training, loss = 0.00005348\n",
      "On epoch 2 of day 34 training, loss = 0.00004544\n",
      "On epoch 3 of day 34 training, loss = 0.00004778\n",
      "On epoch 4 of day 34 training, loss = 0.00004660\n",
      "On epoch 5 of day 34 training, loss = 0.00004495\n",
      "On epoch 6 of day 34 training, loss = 0.00004391\n",
      "On epoch 7 of day 34 training, loss = 0.00004453\n",
      "On epoch 8 of day 34 training, loss = 0.00004452\n",
      "On epoch 9 of day 34 training, loss = 0.00004406\n",
      "On epoch 10 of day 34 training, loss = 0.00004349\n",
      "On epoch 11 of day 34 training, loss = 0.00004341\n",
      "On epoch 12 of day 34 training, loss = 0.00004344\n",
      "On epoch 13 of day 34 training, loss = 0.00004327\n",
      "On epoch 14 of day 34 training, loss = 0.00004299\n",
      "On epoch 15 of day 34 training, loss = 0.00004281\n",
      "On epoch 16 of day 34 training, loss = 0.00004274\n",
      "On epoch 17 of day 34 training, loss = 0.00004264\n",
      "On epoch 18 of day 34 training, loss = 0.00004248\n",
      "On epoch 19 of day 34 training, loss = 0.00004233\n",
      "On epoch 20 of day 34 training, loss = 0.00004223\n",
      "On epoch 21 of day 34 training, loss = 0.00004214\n",
      "On epoch 22 of day 34 training, loss = 0.00004203\n",
      "On epoch 23 of day 34 training, loss = 0.00004192\n",
      "On epoch 24 of day 34 training, loss = 0.00004182\n",
      "On epoch 25 of day 34 training, loss = 0.00004174\n",
      "Current test pnl = -0.0023166434839367867\n",
      " ----- On train/test for day 35 ----- \n",
      "On epoch 1 of day 35 training, loss = 0.00009342\n",
      "On epoch 2 of day 35 training, loss = 0.00008961\n",
      "On epoch 3 of day 35 training, loss = 0.00009029\n",
      "On epoch 4 of day 35 training, loss = 0.00008774\n",
      "On epoch 5 of day 35 training, loss = 0.00008509\n",
      "On epoch 6 of day 35 training, loss = 0.00008552\n",
      "On epoch 7 of day 35 training, loss = 0.00008531\n",
      "On epoch 8 of day 35 training, loss = 0.00008420\n",
      "On epoch 9 of day 35 training, loss = 0.00008317\n",
      "On epoch 10 of day 35 training, loss = 0.00008302\n",
      "On epoch 11 of day 35 training, loss = 0.00008273\n",
      "On epoch 12 of day 35 training, loss = 0.00008207\n",
      "On epoch 13 of day 35 training, loss = 0.00008151\n",
      "On epoch 14 of day 35 training, loss = 0.00008124\n",
      "On epoch 15 of day 35 training, loss = 0.00008091\n",
      "On epoch 16 of day 35 training, loss = 0.00008046\n",
      "On epoch 17 of day 35 training, loss = 0.00008008\n",
      "On epoch 18 of day 35 training, loss = 0.00007980\n",
      "On epoch 19 of day 35 training, loss = 0.00007950\n",
      "On epoch 20 of day 35 training, loss = 0.00007917\n",
      "On epoch 21 of day 35 training, loss = 0.00007888\n",
      "On epoch 22 of day 35 training, loss = 0.00007862\n",
      "On epoch 23 of day 35 training, loss = 0.00007836\n",
      "On epoch 24 of day 35 training, loss = 0.00007810\n",
      "On epoch 25 of day 35 training, loss = 0.00007786\n",
      "Current test pnl = -0.006324301939457655\n",
      " ----- On train/test for day 36 ----- \n",
      "On epoch 1 of day 36 training, loss = 0.00012135\n",
      "On epoch 2 of day 36 training, loss = 0.00010489\n",
      "On epoch 3 of day 36 training, loss = 0.00011417\n",
      "On epoch 4 of day 36 training, loss = 0.00011380\n",
      "On epoch 5 of day 36 training, loss = 0.00011106\n",
      "On epoch 6 of day 36 training, loss = 0.00010831\n",
      "On epoch 7 of day 36 training, loss = 0.00010651\n",
      "On epoch 8 of day 36 training, loss = 0.00010670\n",
      "On epoch 9 of day 36 training, loss = 0.00010720\n",
      "On epoch 10 of day 36 training, loss = 0.00010688\n",
      "On epoch 11 of day 36 training, loss = 0.00010600\n",
      "On epoch 12 of day 36 training, loss = 0.00010506\n",
      "On epoch 13 of day 36 training, loss = 0.00010443\n",
      "On epoch 14 of day 36 training, loss = 0.00010413\n",
      "On epoch 15 of day 36 training, loss = 0.00010387\n",
      "On epoch 16 of day 36 training, loss = 0.00010346\n",
      "On epoch 17 of day 36 training, loss = 0.00010294\n",
      "On epoch 18 of day 36 training, loss = 0.00010242\n",
      "On epoch 19 of day 36 training, loss = 0.00010199\n",
      "On epoch 20 of day 36 training, loss = 0.00010162\n",
      "On epoch 21 of day 36 training, loss = 0.00010126\n",
      "On epoch 22 of day 36 training, loss = 0.00010088\n",
      "On epoch 23 of day 36 training, loss = 0.00010048\n",
      "On epoch 24 of day 36 training, loss = 0.00010009\n",
      "On epoch 25 of day 36 training, loss = 0.00009973\n",
      "Current test pnl = 0.0003168841067235917\n",
      " ----- On train/test for day 37 ----- \n",
      "On epoch 1 of day 37 training, loss = 0.00011679\n",
      "On epoch 2 of day 37 training, loss = 0.00011160\n",
      "On epoch 3 of day 37 training, loss = 0.00011104\n",
      "On epoch 4 of day 37 training, loss = 0.00010965\n",
      "On epoch 5 of day 37 training, loss = 0.00010854\n",
      "On epoch 6 of day 37 training, loss = 0.00010725\n",
      "On epoch 7 of day 37 training, loss = 0.00010640\n",
      "On epoch 8 of day 37 training, loss = 0.00010567\n",
      "On epoch 9 of day 37 training, loss = 0.00010487\n",
      "On epoch 10 of day 37 training, loss = 0.00010406\n",
      "On epoch 11 of day 37 training, loss = 0.00010325\n",
      "On epoch 12 of day 37 training, loss = 0.00010252\n",
      "On epoch 13 of day 37 training, loss = 0.00010186\n",
      "On epoch 14 of day 37 training, loss = 0.00010121\n",
      "On epoch 15 of day 37 training, loss = 0.00010056\n",
      "On epoch 16 of day 37 training, loss = 0.00009993\n",
      "On epoch 17 of day 37 training, loss = 0.00009932\n",
      "On epoch 18 of day 37 training, loss = 0.00009876\n",
      "On epoch 19 of day 37 training, loss = 0.00009821\n",
      "On epoch 20 of day 37 training, loss = 0.00009768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On epoch 21 of day 37 training, loss = 0.00009716\n",
      "On epoch 22 of day 37 training, loss = 0.00009666\n",
      "On epoch 23 of day 37 training, loss = 0.00009619\n",
      "On epoch 24 of day 37 training, loss = 0.00009573\n",
      "On epoch 25 of day 37 training, loss = 0.00009528\n",
      "Current test pnl = -0.0005478773382492363\n",
      " ----- On train/test for day 38 ----- \n",
      "On epoch 1 of day 38 training, loss = 0.00009248\n",
      "On epoch 2 of day 38 training, loss = 0.00008997\n",
      "On epoch 3 of day 38 training, loss = 0.00008996\n",
      "On epoch 4 of day 38 training, loss = 0.00008888\n",
      "On epoch 5 of day 38 training, loss = 0.00008793\n",
      "On epoch 6 of day 38 training, loss = 0.00008745\n",
      "On epoch 7 of day 38 training, loss = 0.00008724\n",
      "On epoch 8 of day 38 training, loss = 0.00008675\n",
      "On epoch 9 of day 38 training, loss = 0.00008620\n",
      "On epoch 10 of day 38 training, loss = 0.00008574\n",
      "On epoch 11 of day 38 training, loss = 0.00008541\n",
      "On epoch 12 of day 38 training, loss = 0.00008507\n",
      "On epoch 13 of day 38 training, loss = 0.00008467\n",
      "On epoch 14 of day 38 training, loss = 0.00008429\n",
      "On epoch 15 of day 38 training, loss = 0.00008396\n",
      "On epoch 16 of day 38 training, loss = 0.00008365\n",
      "On epoch 17 of day 38 training, loss = 0.00008334\n",
      "On epoch 18 of day 38 training, loss = 0.00008302\n",
      "On epoch 19 of day 38 training, loss = 0.00008273\n",
      "On epoch 20 of day 38 training, loss = 0.00008245\n",
      "On epoch 21 of day 38 training, loss = 0.00008218\n",
      "On epoch 22 of day 38 training, loss = 0.00008191\n",
      "On epoch 23 of day 38 training, loss = 0.00008165\n",
      "On epoch 24 of day 38 training, loss = 0.00008141\n",
      "On epoch 25 of day 38 training, loss = 0.00008117\n",
      "Current test pnl = -0.005797103047370911\n",
      " ----- On train/test for day 39 ----- \n",
      "On epoch 1 of day 39 training, loss = 0.00011796\n",
      "On epoch 2 of day 39 training, loss = 0.00009984\n",
      "On epoch 3 of day 39 training, loss = 0.00010081\n",
      "On epoch 4 of day 39 training, loss = 0.00010401\n",
      "On epoch 5 of day 39 training, loss = 0.00010318\n",
      "On epoch 6 of day 39 training, loss = 0.00010105\n",
      "On epoch 7 of day 39 training, loss = 0.00009892\n",
      "On epoch 8 of day 39 training, loss = 0.00009701\n",
      "On epoch 9 of day 39 training, loss = 0.00009578\n",
      "On epoch 10 of day 39 training, loss = 0.00009520\n",
      "On epoch 11 of day 39 training, loss = 0.00009472\n",
      "On epoch 12 of day 39 training, loss = 0.00009398\n",
      "On epoch 13 of day 39 training, loss = 0.00009302\n",
      "On epoch 14 of day 39 training, loss = 0.00009197\n",
      "On epoch 15 of day 39 training, loss = 0.00009097\n",
      "On epoch 16 of day 39 training, loss = 0.00009009\n",
      "On epoch 17 of day 39 training, loss = 0.00008931\n",
      "On epoch 18 of day 39 training, loss = 0.00008857\n",
      "On epoch 19 of day 39 training, loss = 0.00008781\n",
      "On epoch 20 of day 39 training, loss = 0.00008702\n",
      "On epoch 21 of day 39 training, loss = 0.00008623\n",
      "On epoch 22 of day 39 training, loss = 0.00008545\n",
      "On epoch 23 of day 39 training, loss = 0.00008471\n",
      "On epoch 24 of day 39 training, loss = 0.00008399\n",
      "On epoch 25 of day 39 training, loss = 0.00008329\n",
      "Current test pnl = 5.7747215578274336e-06\n",
      " ----- On train/test for day 40 ----- \n",
      "On epoch 1 of day 40 training, loss = 0.00014895\n",
      "On epoch 2 of day 40 training, loss = 0.00010644\n",
      "On epoch 3 of day 40 training, loss = 0.00010872\n",
      "On epoch 4 of day 40 training, loss = 0.00011764\n",
      "On epoch 5 of day 40 training, loss = 0.00011718\n",
      "On epoch 6 of day 40 training, loss = 0.00011378\n",
      "On epoch 7 of day 40 training, loss = 0.00011059\n",
      "On epoch 8 of day 40 training, loss = 0.00010739\n",
      "On epoch 9 of day 40 training, loss = 0.00010521\n",
      "On epoch 10 of day 40 training, loss = 0.00010458\n",
      "On epoch 11 of day 40 training, loss = 0.00010453\n",
      "On epoch 12 of day 40 training, loss = 0.00010409\n",
      "On epoch 13 of day 40 training, loss = 0.00010311\n",
      "On epoch 14 of day 40 training, loss = 0.00010184\n",
      "On epoch 15 of day 40 training, loss = 0.00010055\n",
      "On epoch 16 of day 40 training, loss = 0.00009943\n",
      "On epoch 17 of day 40 training, loss = 0.00009855\n",
      "On epoch 18 of day 40 training, loss = 0.00009778\n",
      "On epoch 19 of day 40 training, loss = 0.00009700\n",
      "On epoch 20 of day 40 training, loss = 0.00009616\n",
      "On epoch 21 of day 40 training, loss = 0.00009527\n",
      "On epoch 22 of day 40 training, loss = 0.00009439\n",
      "On epoch 23 of day 40 training, loss = 0.00009354\n",
      "On epoch 24 of day 40 training, loss = 0.00009274\n",
      "On epoch 25 of day 40 training, loss = 0.00009198\n",
      "Current test pnl = 0.005952565465122461\n",
      " ----- On train/test for day 41 ----- \n",
      "On epoch 1 of day 41 training, loss = 0.00011381\n",
      "On epoch 2 of day 41 training, loss = 0.00007611\n",
      "On epoch 3 of day 41 training, loss = 0.00008591\n",
      "On epoch 4 of day 41 training, loss = 0.00008770\n",
      "On epoch 5 of day 41 training, loss = 0.00008304\n",
      "On epoch 6 of day 41 training, loss = 0.00008038\n",
      "On epoch 7 of day 41 training, loss = 0.00007680\n",
      "On epoch 8 of day 41 training, loss = 0.00007465\n",
      "On epoch 9 of day 41 training, loss = 0.00007479\n",
      "On epoch 10 of day 41 training, loss = 0.00007465\n",
      "On epoch 11 of day 41 training, loss = 0.00007360\n",
      "On epoch 12 of day 41 training, loss = 0.00007217\n",
      "On epoch 13 of day 41 training, loss = 0.00007074\n",
      "On epoch 14 of day 41 training, loss = 0.00006971\n",
      "On epoch 15 of day 41 training, loss = 0.00006901\n",
      "On epoch 16 of day 41 training, loss = 0.00006830\n",
      "On epoch 17 of day 41 training, loss = 0.00006743\n",
      "On epoch 18 of day 41 training, loss = 0.00006647\n",
      "On epoch 19 of day 41 training, loss = 0.00006555\n",
      "On epoch 20 of day 41 training, loss = 0.00006474\n",
      "On epoch 21 of day 41 training, loss = 0.00006400\n",
      "On epoch 22 of day 41 training, loss = 0.00006327\n",
      "On epoch 23 of day 41 training, loss = 0.00006251\n",
      "On epoch 24 of day 41 training, loss = 0.00006175\n",
      "On epoch 25 of day 41 training, loss = 0.00006103\n",
      "Current test pnl = 0.00855833850800991\n",
      " ----- On train/test for day 42 ----- \n",
      "On epoch 1 of day 42 training, loss = 0.00010539\n",
      "On epoch 2 of day 42 training, loss = 0.00009654\n",
      "On epoch 3 of day 42 training, loss = 0.00009686\n",
      "On epoch 4 of day 42 training, loss = 0.00009489\n",
      "On epoch 5 of day 42 training, loss = 0.00009287\n",
      "On epoch 6 of day 42 training, loss = 0.00009060\n",
      "On epoch 7 of day 42 training, loss = 0.00008926\n",
      "On epoch 8 of day 42 training, loss = 0.00008809\n",
      "On epoch 9 of day 42 training, loss = 0.00008668\n",
      "On epoch 10 of day 42 training, loss = 0.00008512\n",
      "On epoch 11 of day 42 training, loss = 0.00008363\n",
      "On epoch 12 of day 42 training, loss = 0.00008234\n",
      "On epoch 13 of day 42 training, loss = 0.00008111\n",
      "On epoch 14 of day 42 training, loss = 0.00007982\n",
      "On epoch 15 of day 42 training, loss = 0.00007852\n",
      "On epoch 16 of day 42 training, loss = 0.00007727\n",
      "On epoch 17 of day 42 training, loss = 0.00007609\n",
      "On epoch 18 of day 42 training, loss = 0.00007493\n",
      "On epoch 19 of day 42 training, loss = 0.00007378\n",
      "On epoch 20 of day 42 training, loss = 0.00007264\n",
      "On epoch 21 of day 42 training, loss = 0.00007153\n",
      "On epoch 22 of day 42 training, loss = 0.00007046\n",
      "On epoch 23 of day 42 training, loss = 0.00006941\n",
      "On epoch 24 of day 42 training, loss = 0.00006837\n",
      "On epoch 25 of day 42 training, loss = 0.00006735\n",
      "Current test pnl = 0.008827528916299343\n",
      " ----- On train/test for day 43 ----- \n",
      "On epoch 1 of day 43 training, loss = 0.00010299\n",
      "On epoch 2 of day 43 training, loss = 0.00009821\n",
      "On epoch 3 of day 43 training, loss = 0.00009510\n",
      "On epoch 4 of day 43 training, loss = 0.00009214\n",
      "On epoch 5 of day 43 training, loss = 0.00008903\n",
      "On epoch 6 of day 43 training, loss = 0.00008610\n",
      "On epoch 7 of day 43 training, loss = 0.00008327\n",
      "On epoch 8 of day 43 training, loss = 0.00008041\n",
      "On epoch 9 of day 43 training, loss = 0.00007764\n",
      "On epoch 10 of day 43 training, loss = 0.00007494\n",
      "On epoch 11 of day 43 training, loss = 0.00007229\n",
      "On epoch 12 of day 43 training, loss = 0.00006968\n",
      "On epoch 13 of day 43 training, loss = 0.00006715\n",
      "On epoch 14 of day 43 training, loss = 0.00006467\n",
      "On epoch 15 of day 43 training, loss = 0.00006224\n",
      "On epoch 16 of day 43 training, loss = 0.00005987\n",
      "On epoch 17 of day 43 training, loss = 0.00005757\n",
      "On epoch 18 of day 43 training, loss = 0.00005531\n",
      "On epoch 19 of day 43 training, loss = 0.00005312\n",
      "On epoch 20 of day 43 training, loss = 0.00005098\n",
      "On epoch 21 of day 43 training, loss = 0.00004891\n",
      "On epoch 22 of day 43 training, loss = 0.00004688\n",
      "On epoch 23 of day 43 training, loss = 0.00004492\n",
      "On epoch 24 of day 43 training, loss = 0.00004302\n",
      "On epoch 25 of day 43 training, loss = 0.00004117\n",
      "Current test pnl = -0.0029030481819063425\n",
      " ----- On train/test for day 44 ----- \n",
      "On epoch 1 of day 44 training, loss = 0.00013081\n",
      "On epoch 2 of day 44 training, loss = 0.00011884\n",
      "On epoch 3 of day 44 training, loss = 0.00012399\n",
      "On epoch 4 of day 44 training, loss = 0.00012062\n",
      "On epoch 5 of day 44 training, loss = 0.00011670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On epoch 6 of day 44 training, loss = 0.00011291\n",
      "On epoch 7 of day 44 training, loss = 0.00011017\n",
      "On epoch 8 of day 44 training, loss = 0.00010888\n",
      "On epoch 9 of day 44 training, loss = 0.00010749\n",
      "On epoch 10 of day 44 training, loss = 0.00010549\n",
      "On epoch 11 of day 44 training, loss = 0.00010322\n",
      "On epoch 12 of day 44 training, loss = 0.00010102\n",
      "On epoch 13 of day 44 training, loss = 0.00009913\n",
      "On epoch 14 of day 44 training, loss = 0.00009747\n",
      "On epoch 15 of day 44 training, loss = 0.00009581\n",
      "On epoch 16 of day 44 training, loss = 0.00009406\n",
      "On epoch 17 of day 44 training, loss = 0.00009228\n",
      "On epoch 18 of day 44 training, loss = 0.00009055\n",
      "On epoch 19 of day 44 training, loss = 0.00008892\n",
      "On epoch 20 of day 44 training, loss = 0.00008736\n",
      "On epoch 21 of day 44 training, loss = 0.00008581\n",
      "On epoch 22 of day 44 training, loss = 0.00008426\n",
      "On epoch 23 of day 44 training, loss = 0.00008274\n",
      "On epoch 24 of day 44 training, loss = 0.00008126\n",
      "On epoch 25 of day 44 training, loss = 0.00007982\n",
      "Current test pnl = 0.0029421469662338495\n",
      " ----- On train/test for day 45 ----- \n",
      "On epoch 1 of day 45 training, loss = 0.00009510\n",
      "On epoch 2 of day 45 training, loss = 0.00008887\n",
      "On epoch 3 of day 45 training, loss = 0.00008831\n",
      "On epoch 4 of day 45 training, loss = 0.00008625\n",
      "On epoch 5 of day 45 training, loss = 0.00008451\n",
      "On epoch 6 of day 45 training, loss = 0.00008269\n",
      "On epoch 7 of day 45 training, loss = 0.00008148\n",
      "On epoch 8 of day 45 training, loss = 0.00008025\n",
      "On epoch 9 of day 45 training, loss = 0.00007892\n",
      "On epoch 10 of day 45 training, loss = 0.00007759\n",
      "On epoch 11 of day 45 training, loss = 0.00007630\n",
      "On epoch 12 of day 45 training, loss = 0.00007513\n",
      "On epoch 13 of day 45 training, loss = 0.00007398\n",
      "On epoch 14 of day 45 training, loss = 0.00007284\n",
      "On epoch 15 of day 45 training, loss = 0.00007171\n",
      "On epoch 16 of day 45 training, loss = 0.00007062\n",
      "On epoch 17 of day 45 training, loss = 0.00006957\n",
      "On epoch 18 of day 45 training, loss = 0.00006854\n",
      "On epoch 19 of day 45 training, loss = 0.00006754\n",
      "On epoch 20 of day 45 training, loss = 0.00006656\n",
      "On epoch 21 of day 45 training, loss = 0.00006560\n",
      "On epoch 22 of day 45 training, loss = 0.00006467\n",
      "On epoch 23 of day 45 training, loss = 0.00006376\n",
      "On epoch 24 of day 45 training, loss = 0.00006287\n",
      "On epoch 25 of day 45 training, loss = 0.00006200\n",
      "Current test pnl = 0.004500440787523985\n",
      " ----- On train/test for day 46 ----- \n",
      "On epoch 1 of day 46 training, loss = 0.00007192\n",
      "On epoch 2 of day 46 training, loss = 0.00006748\n",
      "On epoch 3 of day 46 training, loss = 0.00006622\n",
      "On epoch 4 of day 46 training, loss = 0.00006540\n",
      "On epoch 5 of day 46 training, loss = 0.00006448\n",
      "On epoch 6 of day 46 training, loss = 0.00006354\n",
      "On epoch 7 of day 46 training, loss = 0.00006268\n",
      "On epoch 8 of day 46 training, loss = 0.00006190\n",
      "On epoch 9 of day 46 training, loss = 0.00006115\n",
      "On epoch 10 of day 46 training, loss = 0.00006038\n",
      "On epoch 11 of day 46 training, loss = 0.00005961\n",
      "On epoch 12 of day 46 training, loss = 0.00005889\n",
      "On epoch 13 of day 46 training, loss = 0.00005819\n",
      "On epoch 14 of day 46 training, loss = 0.00005749\n",
      "On epoch 15 of day 46 training, loss = 0.00005681\n",
      "On epoch 16 of day 46 training, loss = 0.00005614\n",
      "On epoch 17 of day 46 training, loss = 0.00005549\n",
      "On epoch 18 of day 46 training, loss = 0.00005485\n",
      "On epoch 19 of day 46 training, loss = 0.00005423\n",
      "On epoch 20 of day 46 training, loss = 0.00005361\n",
      "On epoch 21 of day 46 training, loss = 0.00005301\n",
      "On epoch 22 of day 46 training, loss = 0.00005242\n",
      "On epoch 23 of day 46 training, loss = 0.00005185\n",
      "On epoch 24 of day 46 training, loss = 0.00005128\n",
      "On epoch 25 of day 46 training, loss = 0.00005072\n",
      "Current test pnl = -0.0013685041340067983\n",
      " ----- On train/test for day 47 ----- \n",
      "On epoch 1 of day 47 training, loss = 0.00006060\n",
      "On epoch 2 of day 47 training, loss = 0.00005616\n",
      "On epoch 3 of day 47 training, loss = 0.00005544\n",
      "On epoch 4 of day 47 training, loss = 0.00005470\n",
      "On epoch 5 of day 47 training, loss = 0.00005389\n",
      "On epoch 6 of day 47 training, loss = 0.00005320\n",
      "On epoch 7 of day 47 training, loss = 0.00005265\n",
      "On epoch 8 of day 47 training, loss = 0.00005208\n",
      "On epoch 9 of day 47 training, loss = 0.00005149\n",
      "On epoch 10 of day 47 training, loss = 0.00005091\n",
      "On epoch 11 of day 47 training, loss = 0.00005039\n",
      "On epoch 12 of day 47 training, loss = 0.00004987\n",
      "On epoch 13 of day 47 training, loss = 0.00004936\n",
      "On epoch 14 of day 47 training, loss = 0.00004885\n",
      "On epoch 15 of day 47 training, loss = 0.00004836\n",
      "On epoch 16 of day 47 training, loss = 0.00004789\n",
      "On epoch 17 of day 47 training, loss = 0.00004742\n",
      "On epoch 18 of day 47 training, loss = 0.00004696\n",
      "On epoch 19 of day 47 training, loss = 0.00004651\n",
      "On epoch 20 of day 47 training, loss = 0.00004607\n",
      "On epoch 21 of day 47 training, loss = 0.00004564\n",
      "On epoch 22 of day 47 training, loss = 0.00004522\n",
      "On epoch 23 of day 47 training, loss = 0.00004481\n",
      "On epoch 24 of day 47 training, loss = 0.00004441\n",
      "On epoch 25 of day 47 training, loss = 0.00004401\n",
      "Current test pnl = 0.0022803086321800947\n",
      " ----- On train/test for day 48 ----- \n",
      "On epoch 1 of day 48 training, loss = 0.00011342\n",
      "On epoch 2 of day 48 training, loss = 0.00010894\n",
      "On epoch 3 of day 48 training, loss = 0.00010855\n",
      "On epoch 4 of day 48 training, loss = 0.00010597\n",
      "On epoch 5 of day 48 training, loss = 0.00010361\n",
      "On epoch 6 of day 48 training, loss = 0.00010252\n",
      "On epoch 7 of day 48 training, loss = 0.00010180\n",
      "On epoch 8 of day 48 training, loss = 0.00010057\n",
      "On epoch 9 of day 48 training, loss = 0.00009933\n",
      "On epoch 10 of day 48 training, loss = 0.00009836\n",
      "On epoch 11 of day 48 training, loss = 0.00009766\n",
      "On epoch 12 of day 48 training, loss = 0.00009689\n",
      "On epoch 13 of day 48 training, loss = 0.00009608\n",
      "On epoch 14 of day 48 training, loss = 0.00009536\n",
      "On epoch 15 of day 48 training, loss = 0.00009476\n",
      "On epoch 16 of day 48 training, loss = 0.00009418\n",
      "On epoch 17 of day 48 training, loss = 0.00009360\n",
      "On epoch 18 of day 48 training, loss = 0.00009306\n",
      "On epoch 19 of day 48 training, loss = 0.00009257\n",
      "On epoch 20 of day 48 training, loss = 0.00009211\n",
      "On epoch 21 of day 48 training, loss = 0.00009167\n",
      "On epoch 22 of day 48 training, loss = 0.00009125\n",
      "On epoch 23 of day 48 training, loss = 0.00009085\n",
      "On epoch 24 of day 48 training, loss = 0.00009048\n",
      "On epoch 25 of day 48 training, loss = 0.00009013\n",
      "Current test pnl = 0.001459705177694559\n",
      " ----- On train/test for day 49 ----- \n",
      "On epoch 1 of day 49 training, loss = 0.00011991\n",
      "On epoch 2 of day 49 training, loss = 0.00011552\n",
      "On epoch 3 of day 49 training, loss = 0.00011669\n",
      "On epoch 4 of day 49 training, loss = 0.00011420\n",
      "On epoch 5 of day 49 training, loss = 0.00011182\n",
      "On epoch 6 of day 49 training, loss = 0.00011081\n",
      "On epoch 7 of day 49 training, loss = 0.00011078\n",
      "On epoch 8 of day 49 training, loss = 0.00011001\n",
      "On epoch 9 of day 49 training, loss = 0.00010891\n",
      "On epoch 10 of day 49 training, loss = 0.00010798\n",
      "On epoch 11 of day 49 training, loss = 0.00010749\n",
      "On epoch 12 of day 49 training, loss = 0.00010705\n",
      "On epoch 13 of day 49 training, loss = 0.00010644\n",
      "On epoch 14 of day 49 training, loss = 0.00010580\n",
      "On epoch 15 of day 49 training, loss = 0.00010529\n",
      "On epoch 16 of day 49 training, loss = 0.00010488\n",
      "On epoch 17 of day 49 training, loss = 0.00010446\n",
      "On epoch 18 of day 49 training, loss = 0.00010401\n",
      "On epoch 19 of day 49 training, loss = 0.00010359\n",
      "On epoch 20 of day 49 training, loss = 0.00010323\n",
      "On epoch 21 of day 49 training, loss = 0.00010289\n",
      "On epoch 22 of day 49 training, loss = 0.00010255\n",
      "On epoch 23 of day 49 training, loss = 0.00010222\n",
      "On epoch 24 of day 49 training, loss = 0.00010192\n",
      "On epoch 25 of day 49 training, loss = 0.00010164\n",
      "Current test pnl = -0.0015510247321799397\n",
      " ----- On train/test for day 50 ----- \n",
      "On epoch 1 of day 50 training, loss = 0.00010418\n",
      "On epoch 2 of day 50 training, loss = 0.00010214\n",
      "On epoch 3 of day 50 training, loss = 0.00010228\n",
      "On epoch 4 of day 50 training, loss = 0.00010054\n",
      "On epoch 5 of day 50 training, loss = 0.00009905\n",
      "On epoch 6 of day 50 training, loss = 0.00009812\n",
      "On epoch 7 of day 50 training, loss = 0.00009788\n",
      "On epoch 8 of day 50 training, loss = 0.00009743\n",
      "On epoch 9 of day 50 training, loss = 0.00009671\n",
      "On epoch 10 of day 50 training, loss = 0.00009594\n",
      "On epoch 11 of day 50 training, loss = 0.00009530\n",
      "On epoch 12 of day 50 training, loss = 0.00009483\n",
      "On epoch 13 of day 50 training, loss = 0.00009436\n",
      "On epoch 14 of day 50 training, loss = 0.00009384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On epoch 15 of day 50 training, loss = 0.00009330\n",
      "On epoch 16 of day 50 training, loss = 0.00009280\n",
      "On epoch 17 of day 50 training, loss = 0.00009235\n",
      "On epoch 18 of day 50 training, loss = 0.00009192\n",
      "On epoch 19 of day 50 training, loss = 0.00009149\n",
      "On epoch 20 of day 50 training, loss = 0.00009106\n",
      "On epoch 21 of day 50 training, loss = 0.00009065\n",
      "On epoch 22 of day 50 training, loss = 0.00009026\n",
      "On epoch 23 of day 50 training, loss = 0.00008989\n",
      "On epoch 24 of day 50 training, loss = 0.00008952\n",
      "On epoch 25 of day 50 training, loss = 0.00008917\n",
      "Current test pnl = -0.0027659041807055473\n",
      " ----- On train/test for day 51 ----- \n",
      "On epoch 1 of day 51 training, loss = 0.00013654\n",
      "On epoch 2 of day 51 training, loss = 0.00011841\n",
      "On epoch 3 of day 51 training, loss = 0.00012218\n",
      "On epoch 4 of day 51 training, loss = 0.00012471\n",
      "On epoch 5 of day 51 training, loss = 0.00012257\n",
      "On epoch 6 of day 51 training, loss = 0.00012043\n",
      "On epoch 7 of day 51 training, loss = 0.00011836\n",
      "On epoch 8 of day 51 training, loss = 0.00011739\n",
      "On epoch 9 of day 51 training, loss = 0.00011750\n",
      "On epoch 10 of day 51 training, loss = 0.00011742\n",
      "On epoch 11 of day 51 training, loss = 0.00011680\n",
      "On epoch 12 of day 51 training, loss = 0.00011594\n",
      "On epoch 13 of day 51 training, loss = 0.00011515\n",
      "On epoch 14 of day 51 training, loss = 0.00011460\n",
      "On epoch 15 of day 51 training, loss = 0.00011423\n",
      "On epoch 16 of day 51 training, loss = 0.00011385\n",
      "On epoch 17 of day 51 training, loss = 0.00011338\n",
      "On epoch 18 of day 51 training, loss = 0.00011287\n",
      "On epoch 19 of day 51 training, loss = 0.00011238\n",
      "On epoch 20 of day 51 training, loss = 0.00011196\n",
      "On epoch 21 of day 51 training, loss = 0.00011159\n",
      "On epoch 22 of day 51 training, loss = 0.00011121\n",
      "On epoch 23 of day 51 training, loss = 0.00011083\n",
      "On epoch 24 of day 51 training, loss = 0.00011045\n",
      "On epoch 25 of day 51 training, loss = 0.00011009\n",
      "Current test pnl = 0.002120574936270714\n",
      " ----- On train/test for day 52 ----- \n",
      "On epoch 1 of day 52 training, loss = 0.00013830\n",
      "On epoch 2 of day 52 training, loss = 0.00012488\n",
      "On epoch 3 of day 52 training, loss = 0.00012685\n",
      "On epoch 4 of day 52 training, loss = 0.00012479\n",
      "On epoch 5 of day 52 training, loss = 0.00012329\n",
      "On epoch 6 of day 52 training, loss = 0.00012186\n",
      "On epoch 7 of day 52 training, loss = 0.00012030\n",
      "On epoch 8 of day 52 training, loss = 0.00011965\n",
      "On epoch 9 of day 52 training, loss = 0.00011901\n",
      "On epoch 10 of day 52 training, loss = 0.00011822\n",
      "On epoch 11 of day 52 training, loss = 0.00011742\n",
      "On epoch 12 of day 52 training, loss = 0.00011662\n",
      "On epoch 13 of day 52 training, loss = 0.00011595\n",
      "On epoch 14 of day 52 training, loss = 0.00011539\n",
      "On epoch 15 of day 52 training, loss = 0.00011482\n",
      "On epoch 16 of day 52 training, loss = 0.00011425\n",
      "On epoch 17 of day 52 training, loss = 0.00011369\n",
      "On epoch 18 of day 52 training, loss = 0.00011317\n",
      "On epoch 19 of day 52 training, loss = 0.00011268\n",
      "On epoch 20 of day 52 training, loss = 0.00011222\n",
      "On epoch 21 of day 52 training, loss = 0.00011177\n",
      "On epoch 22 of day 52 training, loss = 0.00011132\n",
      "On epoch 23 of day 52 training, loss = 0.00011090\n",
      "On epoch 24 of day 52 training, loss = 0.00011049\n",
      "On epoch 25 of day 52 training, loss = 0.00011010\n",
      "Current test pnl = 0.002351862844079733\n",
      " ----- On train/test for day 53 ----- \n",
      "On epoch 1 of day 53 training, loss = 0.00011630\n",
      "On epoch 2 of day 53 training, loss = 0.00011096\n",
      "On epoch 3 of day 53 training, loss = 0.00011010\n",
      "On epoch 4 of day 53 training, loss = 0.00010910\n",
      "On epoch 5 of day 53 training, loss = 0.00010811\n",
      "On epoch 6 of day 53 training, loss = 0.00010716\n",
      "On epoch 7 of day 53 training, loss = 0.00010664\n",
      "On epoch 8 of day 53 training, loss = 0.00010606\n",
      "On epoch 9 of day 53 training, loss = 0.00010551\n",
      "On epoch 10 of day 53 training, loss = 0.00010494\n",
      "On epoch 11 of day 53 training, loss = 0.00010447\n",
      "On epoch 12 of day 53 training, loss = 0.00010408\n",
      "On epoch 13 of day 53 training, loss = 0.00010369\n",
      "On epoch 14 of day 53 training, loss = 0.00010331\n",
      "On epoch 15 of day 53 training, loss = 0.00010296\n",
      "On epoch 16 of day 53 training, loss = 0.00010266\n",
      "On epoch 17 of day 53 training, loss = 0.00010237\n",
      "On epoch 18 of day 53 training, loss = 0.00010210\n",
      "On epoch 19 of day 53 training, loss = 0.00010184\n",
      "On epoch 20 of day 53 training, loss = 0.00010159\n",
      "On epoch 21 of day 53 training, loss = 0.00010137\n",
      "On epoch 22 of day 53 training, loss = 0.00010115\n",
      "On epoch 23 of day 53 training, loss = 0.00010095\n",
      "On epoch 24 of day 53 training, loss = 0.00010075\n",
      "On epoch 25 of day 53 training, loss = 0.00010057\n",
      "Current test pnl = -0.0038686427287757397\n",
      " ----- On train/test for day 54 ----- \n",
      "On epoch 1 of day 54 training, loss = 0.00011088\n",
      "On epoch 2 of day 54 training, loss = 0.00010560\n",
      "On epoch 3 of day 54 training, loss = 0.00010321\n",
      "On epoch 4 of day 54 training, loss = 0.00010210\n",
      "On epoch 5 of day 54 training, loss = 0.00010083\n",
      "On epoch 6 of day 54 training, loss = 0.00009926\n",
      "On epoch 7 of day 54 training, loss = 0.00009785\n",
      "On epoch 8 of day 54 training, loss = 0.00009675\n",
      "On epoch 9 of day 54 training, loss = 0.00009576\n",
      "On epoch 10 of day 54 training, loss = 0.00009467\n",
      "On epoch 11 of day 54 training, loss = 0.00009363\n",
      "On epoch 12 of day 54 training, loss = 0.00009273\n",
      "On epoch 13 of day 54 training, loss = 0.00009190\n",
      "On epoch 14 of day 54 training, loss = 0.00009108\n",
      "On epoch 15 of day 54 training, loss = 0.00009029\n",
      "On epoch 16 of day 54 training, loss = 0.00008956\n",
      "On epoch 17 of day 54 training, loss = 0.00008889\n",
      "On epoch 18 of day 54 training, loss = 0.00008824\n",
      "On epoch 19 of day 54 training, loss = 0.00008763\n",
      "On epoch 20 of day 54 training, loss = 0.00008705\n",
      "On epoch 21 of day 54 training, loss = 0.00008651\n",
      "On epoch 22 of day 54 training, loss = 0.00008599\n",
      "On epoch 23 of day 54 training, loss = 0.00008550\n",
      "On epoch 24 of day 54 training, loss = 0.00008503\n",
      "On epoch 25 of day 54 training, loss = 0.00008459\n",
      "Current test pnl = 0.00541146332398057\n",
      " ----- On train/test for day 55 ----- \n",
      "On epoch 1 of day 55 training, loss = 0.00007951\n",
      "On epoch 2 of day 55 training, loss = 0.00007600\n",
      "On epoch 3 of day 55 training, loss = 0.00007583\n",
      "On epoch 4 of day 55 training, loss = 0.00007566\n",
      "On epoch 5 of day 55 training, loss = 0.00007506\n",
      "On epoch 6 of day 55 training, loss = 0.00007480\n",
      "On epoch 7 of day 55 training, loss = 0.00007483\n",
      "On epoch 8 of day 55 training, loss = 0.00007453\n",
      "On epoch 9 of day 55 training, loss = 0.00007428\n",
      "On epoch 10 of day 55 training, loss = 0.00007426\n",
      "On epoch 11 of day 55 training, loss = 0.00007410\n",
      "On epoch 12 of day 55 training, loss = 0.00007392\n",
      "On epoch 13 of day 55 training, loss = 0.00007385\n",
      "On epoch 14 of day 55 training, loss = 0.00007376\n",
      "On epoch 15 of day 55 training, loss = 0.00007362\n",
      "On epoch 16 of day 55 training, loss = 0.00007354\n",
      "On epoch 17 of day 55 training, loss = 0.00007347\n",
      "On epoch 18 of day 55 training, loss = 0.00007337\n",
      "On epoch 19 of day 55 training, loss = 0.00007329\n",
      "On epoch 20 of day 55 training, loss = 0.00007323\n",
      "On epoch 21 of day 55 training, loss = 0.00007316\n",
      "On epoch 22 of day 55 training, loss = 0.00007309\n",
      "On epoch 23 of day 55 training, loss = 0.00007303\n",
      "On epoch 24 of day 55 training, loss = 0.00007298\n",
      "On epoch 25 of day 55 training, loss = 0.00007292\n",
      "Current test pnl = 0.0019809959921985865\n",
      " ----- On train/test for day 56 ----- \n",
      "On epoch 1 of day 56 training, loss = 0.00008868\n",
      "On epoch 2 of day 56 training, loss = 0.00008573\n",
      "On epoch 3 of day 56 training, loss = 0.00008529\n",
      "On epoch 4 of day 56 training, loss = 0.00008316\n",
      "On epoch 5 of day 56 training, loss = 0.00008208\n",
      "On epoch 6 of day 56 training, loss = 0.00008226\n",
      "On epoch 7 of day 56 training, loss = 0.00008135\n",
      "On epoch 8 of day 56 training, loss = 0.00008026\n",
      "On epoch 9 of day 56 training, loss = 0.00008010\n",
      "On epoch 10 of day 56 training, loss = 0.00007965\n",
      "On epoch 11 of day 56 training, loss = 0.00007889\n",
      "On epoch 12 of day 56 training, loss = 0.00007848\n",
      "On epoch 13 of day 56 training, loss = 0.00007816\n",
      "On epoch 14 of day 56 training, loss = 0.00007763\n",
      "On epoch 15 of day 56 training, loss = 0.00007720\n",
      "On epoch 16 of day 56 training, loss = 0.00007689\n",
      "On epoch 17 of day 56 training, loss = 0.00007649\n",
      "On epoch 18 of day 56 training, loss = 0.00007610\n",
      "On epoch 19 of day 56 training, loss = 0.00007580\n",
      "On epoch 20 of day 56 training, loss = 0.00007547\n",
      "On epoch 21 of day 56 training, loss = 0.00007514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On epoch 22 of day 56 training, loss = 0.00007485\n",
      "On epoch 23 of day 56 training, loss = 0.00007457\n",
      "On epoch 24 of day 56 training, loss = 0.00007429\n",
      "On epoch 25 of day 56 training, loss = 0.00007403\n",
      "Current test pnl = 0.00037866723141632974\n",
      " ----- On train/test for day 57 ----- \n",
      "On epoch 1 of day 57 training, loss = 0.00007987\n",
      "On epoch 2 of day 57 training, loss = 0.00007285\n",
      "On epoch 3 of day 57 training, loss = 0.00007466\n",
      "On epoch 4 of day 57 training, loss = 0.00007378\n",
      "On epoch 5 of day 57 training, loss = 0.00007237\n",
      "On epoch 6 of day 57 training, loss = 0.00007115\n",
      "On epoch 7 of day 57 training, loss = 0.00007136\n",
      "On epoch 8 of day 57 training, loss = 0.00007115\n",
      "On epoch 9 of day 57 training, loss = 0.00007045\n",
      "On epoch 10 of day 57 training, loss = 0.00006981\n",
      "On epoch 11 of day 57 training, loss = 0.00006959\n",
      "On epoch 12 of day 57 training, loss = 0.00006938\n",
      "On epoch 13 of day 57 training, loss = 0.00006897\n",
      "On epoch 14 of day 57 training, loss = 0.00006853\n",
      "On epoch 15 of day 57 training, loss = 0.00006825\n",
      "On epoch 16 of day 57 training, loss = 0.00006801\n",
      "On epoch 17 of day 57 training, loss = 0.00006771\n",
      "On epoch 18 of day 57 training, loss = 0.00006739\n",
      "On epoch 19 of day 57 training, loss = 0.00006712\n",
      "On epoch 20 of day 57 training, loss = 0.00006688\n",
      "On epoch 21 of day 57 training, loss = 0.00006664\n",
      "On epoch 22 of day 57 training, loss = 0.00006638\n",
      "On epoch 23 of day 57 training, loss = 0.00006615\n",
      "On epoch 24 of day 57 training, loss = 0.00006594\n",
      "On epoch 25 of day 57 training, loss = 0.00006573\n",
      "Current test pnl = -0.0004621376865543425\n",
      " ----- On train/test for day 58 ----- \n",
      "On epoch 1 of day 58 training, loss = 0.00007014\n",
      "On epoch 2 of day 58 training, loss = 0.00006918\n",
      "On epoch 3 of day 58 training, loss = 0.00006957\n",
      "On epoch 4 of day 58 training, loss = 0.00006767\n",
      "On epoch 5 of day 58 training, loss = 0.00006569\n",
      "On epoch 6 of day 58 training, loss = 0.00006597\n",
      "On epoch 7 of day 58 training, loss = 0.00006623\n",
      "On epoch 8 of day 58 training, loss = 0.00006566\n",
      "On epoch 9 of day 58 training, loss = 0.00006493\n",
      "On epoch 10 of day 58 training, loss = 0.00006466\n",
      "On epoch 11 of day 58 training, loss = 0.00006469\n",
      "On epoch 12 of day 58 training, loss = 0.00006449\n",
      "On epoch 13 of day 58 training, loss = 0.00006413\n",
      "On epoch 14 of day 58 training, loss = 0.00006386\n",
      "On epoch 15 of day 58 training, loss = 0.00006375\n",
      "On epoch 16 of day 58 training, loss = 0.00006362\n",
      "On epoch 17 of day 58 training, loss = 0.00006341\n",
      "On epoch 18 of day 58 training, loss = 0.00006322\n",
      "On epoch 19 of day 58 training, loss = 0.00006308\n",
      "On epoch 20 of day 58 training, loss = 0.00006296\n",
      "On epoch 21 of day 58 training, loss = 0.00006282\n",
      "On epoch 22 of day 58 training, loss = 0.00006268\n",
      "On epoch 23 of day 58 training, loss = 0.00006255\n",
      "On epoch 24 of day 58 training, loss = 0.00006245\n",
      "On epoch 25 of day 58 training, loss = 0.00006234\n",
      "Current test pnl = 0.0026337981689721346\n",
      " ----- On train/test for day 59 ----- \n",
      "On epoch 1 of day 59 training, loss = 0.00006994\n",
      "On epoch 2 of day 59 training, loss = 0.00004925\n",
      "On epoch 3 of day 59 training, loss = 0.00005915\n",
      "On epoch 4 of day 59 training, loss = 0.00005827\n",
      "On epoch 5 of day 59 training, loss = 0.00005594\n",
      "On epoch 6 of day 59 training, loss = 0.00005346\n",
      "On epoch 7 of day 59 training, loss = 0.00005195\n",
      "On epoch 8 of day 59 training, loss = 0.00005285\n",
      "On epoch 9 of day 59 training, loss = 0.00005355\n",
      "On epoch 10 of day 59 training, loss = 0.00005324\n",
      "On epoch 11 of day 59 training, loss = 0.00005253\n",
      "On epoch 12 of day 59 training, loss = 0.00005192\n",
      "On epoch 13 of day 59 training, loss = 0.00005180\n",
      "On epoch 14 of day 59 training, loss = 0.00005191\n",
      "On epoch 15 of day 59 training, loss = 0.00005186\n",
      "On epoch 16 of day 59 training, loss = 0.00005161\n",
      "On epoch 17 of day 59 training, loss = 0.00005133\n",
      "On epoch 18 of day 59 training, loss = 0.00005114\n",
      "On epoch 19 of day 59 training, loss = 0.00005105\n",
      "On epoch 20 of day 59 training, loss = 0.00005097\n",
      "On epoch 21 of day 59 training, loss = 0.00005084\n",
      "On epoch 22 of day 59 training, loss = 0.00005068\n",
      "On epoch 23 of day 59 training, loss = 0.00005053\n",
      "On epoch 24 of day 59 training, loss = 0.00005042\n",
      "On epoch 25 of day 59 training, loss = 0.00005032\n",
      "Current test pnl = 0.0016168205766007304\n",
      " ----- On train/test for day 60 ----- \n",
      "On epoch 1 of day 60 training, loss = 0.00005638\n",
      "On epoch 2 of day 60 training, loss = 0.00005004\n",
      "On epoch 3 of day 60 training, loss = 0.00005185\n",
      "On epoch 4 of day 60 training, loss = 0.00005070\n",
      "On epoch 5 of day 60 training, loss = 0.00004950\n",
      "On epoch 6 of day 60 training, loss = 0.00004837\n",
      "On epoch 7 of day 60 training, loss = 0.00004844\n",
      "On epoch 8 of day 60 training, loss = 0.00004842\n",
      "On epoch 9 of day 60 training, loss = 0.00004802\n",
      "On epoch 10 of day 60 training, loss = 0.00004747\n",
      "On epoch 11 of day 60 training, loss = 0.00004707\n",
      "On epoch 12 of day 60 training, loss = 0.00004690\n",
      "On epoch 13 of day 60 training, loss = 0.00004673\n",
      "On epoch 14 of day 60 training, loss = 0.00004646\n",
      "On epoch 15 of day 60 training, loss = 0.00004615\n",
      "On epoch 16 of day 60 training, loss = 0.00004589\n",
      "On epoch 17 of day 60 training, loss = 0.00004570\n",
      "On epoch 18 of day 60 training, loss = 0.00004552\n",
      "On epoch 19 of day 60 training, loss = 0.00004531\n",
      "On epoch 20 of day 60 training, loss = 0.00004509\n",
      "On epoch 21 of day 60 training, loss = 0.00004490\n",
      "On epoch 22 of day 60 training, loss = 0.00004472\n",
      "On epoch 23 of day 60 training, loss = 0.00004456\n",
      "On epoch 24 of day 60 training, loss = 0.00004439\n",
      "On epoch 25 of day 60 training, loss = 0.00004422\n",
      "Current test pnl = 0.0011312004644423723\n",
      " ----- On train/test for day 61 ----- \n",
      "On epoch 1 of day 61 training, loss = 0.00005236\n",
      "On epoch 2 of day 61 training, loss = 0.00004996\n",
      "On epoch 3 of day 61 training, loss = 0.00004945\n",
      "On epoch 4 of day 61 training, loss = 0.00004898\n",
      "On epoch 5 of day 61 training, loss = 0.00004855\n",
      "On epoch 6 of day 61 training, loss = 0.00004816\n",
      "On epoch 7 of day 61 training, loss = 0.00004781\n",
      "On epoch 8 of day 61 training, loss = 0.00004747\n",
      "On epoch 9 of day 61 training, loss = 0.00004715\n",
      "On epoch 10 of day 61 training, loss = 0.00004684\n",
      "On epoch 11 of day 61 training, loss = 0.00004656\n",
      "On epoch 12 of day 61 training, loss = 0.00004629\n",
      "On epoch 13 of day 61 training, loss = 0.00004603\n",
      "On epoch 14 of day 61 training, loss = 0.00004579\n",
      "On epoch 15 of day 61 training, loss = 0.00004556\n",
      "On epoch 16 of day 61 training, loss = 0.00004534\n",
      "On epoch 17 of day 61 training, loss = 0.00004513\n",
      "On epoch 18 of day 61 training, loss = 0.00004493\n",
      "On epoch 19 of day 61 training, loss = 0.00004474\n",
      "On epoch 20 of day 61 training, loss = 0.00004456\n",
      "On epoch 21 of day 61 training, loss = 0.00004439\n",
      "On epoch 22 of day 61 training, loss = 0.00004422\n",
      "On epoch 23 of day 61 training, loss = 0.00004407\n",
      "On epoch 24 of day 61 training, loss = 0.00004392\n",
      "On epoch 25 of day 61 training, loss = 0.00004378\n",
      "Current test pnl = -0.003783387830480933\n",
      " ----- On train/test for day 62 ----- \n",
      "On epoch 1 of day 62 training, loss = 0.00006805\n",
      "On epoch 2 of day 62 training, loss = 0.00006116\n",
      "On epoch 3 of day 62 training, loss = 0.00006143\n",
      "On epoch 4 of day 62 training, loss = 0.00006125\n",
      "On epoch 5 of day 62 training, loss = 0.00006009\n",
      "On epoch 6 of day 62 training, loss = 0.00005889\n",
      "On epoch 7 of day 62 training, loss = 0.00005797\n",
      "On epoch 8 of day 62 training, loss = 0.00005753\n",
      "On epoch 9 of day 62 training, loss = 0.00005718\n",
      "On epoch 10 of day 62 training, loss = 0.00005664\n",
      "On epoch 11 of day 62 training, loss = 0.00005599\n",
      "On epoch 12 of day 62 training, loss = 0.00005538\n",
      "On epoch 13 of day 62 training, loss = 0.00005490\n",
      "On epoch 14 of day 62 training, loss = 0.00005450\n",
      "On epoch 15 of day 62 training, loss = 0.00005409\n",
      "On epoch 16 of day 62 training, loss = 0.00005365\n",
      "On epoch 17 of day 62 training, loss = 0.00005321\n",
      "On epoch 18 of day 62 training, loss = 0.00005281\n",
      "On epoch 19 of day 62 training, loss = 0.00005245\n",
      "On epoch 20 of day 62 training, loss = 0.00005210\n",
      "On epoch 21 of day 62 training, loss = 0.00005175\n",
      "On epoch 22 of day 62 training, loss = 0.00005141\n",
      "On epoch 23 of day 62 training, loss = 0.00005109\n",
      "On epoch 24 of day 62 training, loss = 0.00005079\n",
      "On epoch 25 of day 62 training, loss = 0.00005050\n",
      "Current test pnl = 0.0016040353802964091\n",
      " ----- On train/test for day 63 ----- \n",
      "On epoch 1 of day 63 training, loss = 0.00005441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On epoch 2 of day 63 training, loss = 0.00004518\n",
      "On epoch 3 of day 63 training, loss = 0.00004681\n",
      "On epoch 4 of day 63 training, loss = 0.00004640\n",
      "On epoch 5 of day 63 training, loss = 0.00004574\n",
      "On epoch 6 of day 63 training, loss = 0.00004481\n",
      "On epoch 7 of day 63 training, loss = 0.00004420\n",
      "On epoch 8 of day 63 training, loss = 0.00004419\n",
      "On epoch 9 of day 63 training, loss = 0.00004411\n",
      "On epoch 10 of day 63 training, loss = 0.00004382\n",
      "On epoch 11 of day 63 training, loss = 0.00004343\n",
      "On epoch 12 of day 63 training, loss = 0.00004310\n",
      "On epoch 13 of day 63 training, loss = 0.00004291\n",
      "On epoch 14 of day 63 training, loss = 0.00004275\n",
      "On epoch 15 of day 63 training, loss = 0.00004255\n",
      "On epoch 16 of day 63 training, loss = 0.00004231\n",
      "On epoch 17 of day 63 training, loss = 0.00004207\n",
      "On epoch 18 of day 63 training, loss = 0.00004188\n",
      "On epoch 19 of day 63 training, loss = 0.00004171\n",
      "On epoch 20 of day 63 training, loss = 0.00004153\n",
      "On epoch 21 of day 63 training, loss = 0.00004134\n",
      "On epoch 22 of day 63 training, loss = 0.00004116\n",
      "On epoch 23 of day 63 training, loss = 0.00004099\n",
      "On epoch 24 of day 63 training, loss = 0.00004083\n",
      "On epoch 25 of day 63 training, loss = 0.00004067\n",
      "Current test pnl = 0.0011449136072769761\n",
      " ----- On train/test for day 64 ----- \n",
      "On epoch 1 of day 64 training, loss = 0.00005716\n",
      "On epoch 2 of day 64 training, loss = 0.00005344\n",
      "On epoch 3 of day 64 training, loss = 0.00005225\n",
      "On epoch 4 of day 64 training, loss = 0.00005219\n",
      "On epoch 5 of day 64 training, loss = 0.00005166\n",
      "On epoch 6 of day 64 training, loss = 0.00005117\n",
      "On epoch 7 of day 64 training, loss = 0.00005087\n",
      "On epoch 8 of day 64 training, loss = 0.00005060\n",
      "On epoch 9 of day 64 training, loss = 0.00005043\n",
      "On epoch 10 of day 64 training, loss = 0.00005016\n",
      "On epoch 11 of day 64 training, loss = 0.00004991\n",
      "On epoch 12 of day 64 training, loss = 0.00004969\n",
      "On epoch 13 of day 64 training, loss = 0.00004951\n",
      "On epoch 14 of day 64 training, loss = 0.00004933\n",
      "On epoch 15 of day 64 training, loss = 0.00004914\n",
      "On epoch 16 of day 64 training, loss = 0.00004896\n",
      "On epoch 17 of day 64 training, loss = 0.00004879\n",
      "On epoch 18 of day 64 training, loss = 0.00004864\n",
      "On epoch 19 of day 64 training, loss = 0.00004849\n",
      "On epoch 20 of day 64 training, loss = 0.00004835\n",
      "On epoch 21 of day 64 training, loss = 0.00004820\n",
      "On epoch 22 of day 64 training, loss = 0.00004807\n",
      "On epoch 23 of day 64 training, loss = 0.00004795\n",
      "On epoch 24 of day 64 training, loss = 0.00004783\n",
      "On epoch 25 of day 64 training, loss = 0.00004771\n",
      "Current test pnl = -0.003178033046424389\n",
      " ----- On train/test for day 65 ----- \n",
      "On epoch 1 of day 65 training, loss = 0.00006339\n",
      "On epoch 2 of day 65 training, loss = 0.00005782\n",
      "On epoch 3 of day 65 training, loss = 0.00005991\n",
      "On epoch 4 of day 65 training, loss = 0.00005801\n",
      "On epoch 5 of day 65 training, loss = 0.00005710\n",
      "On epoch 6 of day 65 training, loss = 0.00005604\n",
      "On epoch 7 of day 65 training, loss = 0.00005575\n",
      "On epoch 8 of day 65 training, loss = 0.00005597\n",
      "On epoch 9 of day 65 training, loss = 0.00005584\n",
      "On epoch 10 of day 65 training, loss = 0.00005547\n",
      "On epoch 11 of day 65 training, loss = 0.00005509\n",
      "On epoch 12 of day 65 training, loss = 0.00005480\n",
      "On epoch 13 of day 65 training, loss = 0.00005467\n",
      "On epoch 14 of day 65 training, loss = 0.00005456\n",
      "On epoch 15 of day 65 training, loss = 0.00005439\n",
      "On epoch 16 of day 65 training, loss = 0.00005418\n",
      "On epoch 17 of day 65 training, loss = 0.00005397\n",
      "On epoch 18 of day 65 training, loss = 0.00005381\n",
      "On epoch 19 of day 65 training, loss = 0.00005368\n",
      "On epoch 20 of day 65 training, loss = 0.00005354\n",
      "On epoch 21 of day 65 training, loss = 0.00005340\n",
      "On epoch 22 of day 65 training, loss = 0.00005325\n",
      "On epoch 23 of day 65 training, loss = 0.00005311\n",
      "On epoch 24 of day 65 training, loss = 0.00005299\n",
      "On epoch 25 of day 65 training, loss = 0.00005287\n",
      "Current test pnl = 0.0002475386136211455\n",
      " ----- On train/test for day 66 ----- \n",
      "On epoch 1 of day 66 training, loss = 0.00005865\n",
      "On epoch 2 of day 66 training, loss = 0.00005370\n",
      "On epoch 3 of day 66 training, loss = 0.00005252\n",
      "On epoch 4 of day 66 training, loss = 0.00005176\n",
      "On epoch 5 of day 66 training, loss = 0.00005114\n",
      "On epoch 6 of day 66 training, loss = 0.00005044\n",
      "On epoch 7 of day 66 training, loss = 0.00004978\n",
      "On epoch 8 of day 66 training, loss = 0.00004916\n",
      "On epoch 9 of day 66 training, loss = 0.00004858\n",
      "On epoch 10 of day 66 training, loss = 0.00004804\n",
      "On epoch 11 of day 66 training, loss = 0.00004750\n",
      "On epoch 12 of day 66 training, loss = 0.00004697\n",
      "On epoch 13 of day 66 training, loss = 0.00004645\n",
      "On epoch 14 of day 66 training, loss = 0.00004596\n",
      "On epoch 15 of day 66 training, loss = 0.00004549\n",
      "On epoch 16 of day 66 training, loss = 0.00004503\n",
      "On epoch 17 of day 66 training, loss = 0.00004458\n",
      "On epoch 18 of day 66 training, loss = 0.00004414\n",
      "On epoch 19 of day 66 training, loss = 0.00004371\n",
      "On epoch 20 of day 66 training, loss = 0.00004330\n",
      "On epoch 21 of day 66 training, loss = 0.00004290\n",
      "On epoch 22 of day 66 training, loss = 0.00004252\n",
      "On epoch 23 of day 66 training, loss = 0.00004214\n",
      "On epoch 24 of day 66 training, loss = 0.00004177\n",
      "On epoch 25 of day 66 training, loss = 0.00004142\n",
      "Current test pnl = -0.00015884485037531704\n",
      " ----- On train/test for day 67 ----- \n",
      "On epoch 1 of day 67 training, loss = 0.00006161\n",
      "On epoch 2 of day 67 training, loss = 0.00005283\n",
      "On epoch 3 of day 67 training, loss = 0.00005263\n",
      "On epoch 4 of day 67 training, loss = 0.00005180\n",
      "On epoch 5 of day 67 training, loss = 0.00005094\n",
      "On epoch 6 of day 67 training, loss = 0.00004993\n",
      "On epoch 7 of day 67 training, loss = 0.00004898\n",
      "On epoch 8 of day 67 training, loss = 0.00004837\n",
      "On epoch 9 of day 67 training, loss = 0.00004779\n",
      "On epoch 10 of day 67 training, loss = 0.00004717\n",
      "On epoch 11 of day 67 training, loss = 0.00004651\n",
      "On epoch 12 of day 67 training, loss = 0.00004586\n",
      "On epoch 13 of day 67 training, loss = 0.00004530\n",
      "On epoch 14 of day 67 training, loss = 0.00004477\n",
      "On epoch 15 of day 67 training, loss = 0.00004425\n",
      "On epoch 16 of day 67 training, loss = 0.00004373\n",
      "On epoch 17 of day 67 training, loss = 0.00004323\n",
      "On epoch 18 of day 67 training, loss = 0.00004275\n",
      "On epoch 19 of day 67 training, loss = 0.00004230\n",
      "On epoch 20 of day 67 training, loss = 0.00004186\n",
      "On epoch 21 of day 67 training, loss = 0.00004143\n",
      "On epoch 22 of day 67 training, loss = 0.00004102\n",
      "On epoch 23 of day 67 training, loss = 0.00004062\n",
      "On epoch 24 of day 67 training, loss = 0.00004023\n",
      "On epoch 25 of day 67 training, loss = 0.00003986\n",
      "Current test pnl = 0.0010829902021214366\n",
      " ----- On train/test for day 68 ----- \n",
      "On epoch 1 of day 68 training, loss = 0.00006581\n",
      "On epoch 2 of day 68 training, loss = 0.00005916\n",
      "On epoch 3 of day 68 training, loss = 0.00005700\n",
      "On epoch 4 of day 68 training, loss = 0.00005759\n",
      "On epoch 5 of day 68 training, loss = 0.00005712\n",
      "On epoch 6 of day 68 training, loss = 0.00005602\n",
      "On epoch 7 of day 68 training, loss = 0.00005491\n",
      "On epoch 8 of day 68 training, loss = 0.00005430\n",
      "On epoch 9 of day 68 training, loss = 0.00005400\n",
      "On epoch 10 of day 68 training, loss = 0.00005351\n",
      "On epoch 11 of day 68 training, loss = 0.00005284\n",
      "On epoch 12 of day 68 training, loss = 0.00005219\n",
      "On epoch 13 of day 68 training, loss = 0.00005170\n",
      "On epoch 14 of day 68 training, loss = 0.00005128\n",
      "On epoch 15 of day 68 training, loss = 0.00005081\n",
      "On epoch 16 of day 68 training, loss = 0.00005031\n",
      "On epoch 17 of day 68 training, loss = 0.00004983\n",
      "On epoch 18 of day 68 training, loss = 0.00004941\n",
      "On epoch 19 of day 68 training, loss = 0.00004902\n",
      "On epoch 20 of day 68 training, loss = 0.00004861\n",
      "On epoch 21 of day 68 training, loss = 0.00004821\n",
      "On epoch 22 of day 68 training, loss = 0.00004783\n",
      "On epoch 23 of day 68 training, loss = 0.00004747\n",
      "On epoch 24 of day 68 training, loss = 0.00004713\n",
      "On epoch 25 of day 68 training, loss = 0.00004679\n",
      "Current test pnl = -0.002062141429632902\n",
      " ----- On train/test for day 69 ----- \n",
      "On epoch 1 of day 69 training, loss = 0.00008323\n",
      "On epoch 2 of day 69 training, loss = 0.00006465\n",
      "On epoch 3 of day 69 training, loss = 0.00007313\n",
      "On epoch 4 of day 69 training, loss = 0.00007276\n",
      "On epoch 5 of day 69 training, loss = 0.00007005\n",
      "On epoch 6 of day 69 training, loss = 0.00006716\n",
      "On epoch 7 of day 69 training, loss = 0.00006526\n",
      "On epoch 8 of day 69 training, loss = 0.00006550\n",
      "On epoch 9 of day 69 training, loss = 0.00006577\n",
      "On epoch 10 of day 69 training, loss = 0.00006512\n",
      "On epoch 11 of day 69 training, loss = 0.00006402\n",
      "On epoch 12 of day 69 training, loss = 0.00006298\n",
      "On epoch 13 of day 69 training, loss = 0.00006239\n",
      "On epoch 14 of day 69 training, loss = 0.00006206\n",
      "On epoch 15 of day 69 training, loss = 0.00006163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On epoch 16 of day 69 training, loss = 0.00006103\n",
      "On epoch 17 of day 69 training, loss = 0.00006038\n",
      "On epoch 18 of day 69 training, loss = 0.00005982\n",
      "On epoch 19 of day 69 training, loss = 0.00005936\n",
      "On epoch 20 of day 69 training, loss = 0.00005893\n",
      "On epoch 21 of day 69 training, loss = 0.00005847\n",
      "On epoch 22 of day 69 training, loss = 0.00005799\n",
      "On epoch 23 of day 69 training, loss = 0.00005753\n",
      "On epoch 24 of day 69 training, loss = 0.00005711\n",
      "On epoch 25 of day 69 training, loss = 0.00005671\n",
      "Current test pnl = 0.003531810361891985\n",
      " ----- On train/test for day 70 ----- \n",
      "On epoch 1 of day 70 training, loss = 0.00008622\n",
      "On epoch 2 of day 70 training, loss = 0.00008202\n",
      "On epoch 3 of day 70 training, loss = 0.00008007\n",
      "On epoch 4 of day 70 training, loss = 0.00007921\n",
      "On epoch 5 of day 70 training, loss = 0.00007782\n",
      "On epoch 6 of day 70 training, loss = 0.00007660\n",
      "On epoch 7 of day 70 training, loss = 0.00007544\n",
      "On epoch 8 of day 70 training, loss = 0.00007443\n",
      "On epoch 9 of day 70 training, loss = 0.00007351\n",
      "On epoch 10 of day 70 training, loss = 0.00007253\n",
      "On epoch 11 of day 70 training, loss = 0.00007155\n",
      "On epoch 12 of day 70 training, loss = 0.00007063\n",
      "On epoch 13 of day 70 training, loss = 0.00006977\n",
      "On epoch 14 of day 70 training, loss = 0.00006895\n",
      "On epoch 15 of day 70 training, loss = 0.00006812\n",
      "On epoch 16 of day 70 training, loss = 0.00006731\n",
      "On epoch 17 of day 70 training, loss = 0.00006654\n",
      "On epoch 18 of day 70 training, loss = 0.00006579\n",
      "On epoch 19 of day 70 training, loss = 0.00006507\n",
      "On epoch 20 of day 70 training, loss = 0.00006437\n",
      "On epoch 21 of day 70 training, loss = 0.00006368\n",
      "On epoch 22 of day 70 training, loss = 0.00006301\n",
      "On epoch 23 of day 70 training, loss = 0.00006236\n",
      "On epoch 24 of day 70 training, loss = 0.00006173\n",
      "On epoch 25 of day 70 training, loss = 0.00006112\n",
      "Current test pnl = -0.0004929144051857293\n",
      " ----- On train/test for day 71 ----- \n",
      "On epoch 1 of day 71 training, loss = 0.00008176\n",
      "On epoch 2 of day 71 training, loss = 0.00007783\n",
      "On epoch 3 of day 71 training, loss = 0.00007874\n",
      "On epoch 4 of day 71 training, loss = 0.00007717\n",
      "On epoch 5 of day 71 training, loss = 0.00007545\n",
      "On epoch 6 of day 71 training, loss = 0.00007478\n",
      "On epoch 7 of day 71 training, loss = 0.00007473\n",
      "On epoch 8 of day 71 training, loss = 0.00007410\n",
      "On epoch 9 of day 71 training, loss = 0.00007324\n",
      "On epoch 10 of day 71 training, loss = 0.00007253\n",
      "On epoch 11 of day 71 training, loss = 0.00007216\n",
      "On epoch 12 of day 71 training, loss = 0.00007175\n",
      "On epoch 13 of day 71 training, loss = 0.00007118\n",
      "On epoch 14 of day 71 training, loss = 0.00007062\n",
      "On epoch 15 of day 71 training, loss = 0.00007018\n",
      "On epoch 16 of day 71 training, loss = 0.00006978\n",
      "On epoch 17 of day 71 training, loss = 0.00006935\n",
      "On epoch 18 of day 71 training, loss = 0.00006890\n",
      "On epoch 19 of day 71 training, loss = 0.00006849\n",
      "On epoch 20 of day 71 training, loss = 0.00006811\n",
      "On epoch 21 of day 71 training, loss = 0.00006774\n",
      "On epoch 22 of day 71 training, loss = 0.00006736\n",
      "On epoch 23 of day 71 training, loss = 0.00006700\n",
      "On epoch 24 of day 71 training, loss = 0.00006665\n",
      "On epoch 25 of day 71 training, loss = 0.00006632\n",
      "Current test pnl = -0.002055724151432514\n",
      " ----- On train/test for day 72 ----- \n",
      "On epoch 1 of day 72 training, loss = 0.00007588\n",
      "On epoch 2 of day 72 training, loss = 0.00007373\n",
      "On epoch 3 of day 72 training, loss = 0.00007287\n",
      "On epoch 4 of day 72 training, loss = 0.00007250\n",
      "On epoch 5 of day 72 training, loss = 0.00007199\n",
      "On epoch 6 of day 72 training, loss = 0.00007142\n",
      "On epoch 7 of day 72 training, loss = 0.00007097\n",
      "On epoch 8 of day 72 training, loss = 0.00007058\n",
      "On epoch 9 of day 72 training, loss = 0.00007015\n",
      "On epoch 10 of day 72 training, loss = 0.00006972\n",
      "On epoch 11 of day 72 training, loss = 0.00006932\n",
      "On epoch 12 of day 72 training, loss = 0.00006896\n",
      "On epoch 13 of day 72 training, loss = 0.00006858\n",
      "On epoch 14 of day 72 training, loss = 0.00006821\n",
      "On epoch 15 of day 72 training, loss = 0.00006786\n",
      "On epoch 16 of day 72 training, loss = 0.00006752\n",
      "On epoch 17 of day 72 training, loss = 0.00006719\n",
      "On epoch 18 of day 72 training, loss = 0.00006686\n",
      "On epoch 19 of day 72 training, loss = 0.00006654\n",
      "On epoch 20 of day 72 training, loss = 0.00006624\n",
      "On epoch 21 of day 72 training, loss = 0.00006594\n",
      "On epoch 22 of day 72 training, loss = 0.00006564\n",
      "On epoch 23 of day 72 training, loss = 0.00006535\n",
      "On epoch 24 of day 72 training, loss = 0.00006507\n",
      "On epoch 25 of day 72 training, loss = 0.00006480\n",
      "Current test pnl = 0.0008260838221758604\n",
      " ----- On train/test for day 73 ----- \n",
      "On epoch 1 of day 73 training, loss = 0.00006892\n",
      "On epoch 2 of day 73 training, loss = 0.00006396\n",
      "On epoch 3 of day 73 training, loss = 0.00006311\n",
      "On epoch 4 of day 73 training, loss = 0.00006277\n",
      "On epoch 5 of day 73 training, loss = 0.00006192\n",
      "On epoch 6 of day 73 training, loss = 0.00006105\n",
      "On epoch 7 of day 73 training, loss = 0.00006035\n",
      "On epoch 8 of day 73 training, loss = 0.00005988\n",
      "On epoch 9 of day 73 training, loss = 0.00005937\n",
      "On epoch 10 of day 73 training, loss = 0.00005878\n",
      "On epoch 11 of day 73 training, loss = 0.00005819\n",
      "On epoch 12 of day 73 training, loss = 0.00005769\n",
      "On epoch 13 of day 73 training, loss = 0.00005723\n",
      "On epoch 14 of day 73 training, loss = 0.00005677\n",
      "On epoch 15 of day 73 training, loss = 0.00005629\n",
      "On epoch 16 of day 73 training, loss = 0.00005584\n",
      "On epoch 17 of day 73 training, loss = 0.00005542\n",
      "On epoch 18 of day 73 training, loss = 0.00005501\n",
      "On epoch 19 of day 73 training, loss = 0.00005461\n",
      "On epoch 20 of day 73 training, loss = 0.00005422\n",
      "On epoch 21 of day 73 training, loss = 0.00005384\n",
      "On epoch 22 of day 73 training, loss = 0.00005348\n",
      "On epoch 23 of day 73 training, loss = 0.00005313\n",
      "On epoch 24 of day 73 training, loss = 0.00005279\n",
      "On epoch 25 of day 73 training, loss = 0.00005245\n",
      "Current test pnl = 0.0009529870585538447\n",
      " ----- On train/test for day 74 ----- \n",
      "On epoch 1 of day 74 training, loss = 0.00006133\n",
      "On epoch 2 of day 74 training, loss = 0.00005481\n",
      "On epoch 3 of day 74 training, loss = 0.00005568\n",
      "On epoch 4 of day 74 training, loss = 0.00005488\n",
      "On epoch 5 of day 74 training, loss = 0.00005370\n",
      "On epoch 6 of day 74 training, loss = 0.00005277\n",
      "On epoch 7 of day 74 training, loss = 0.00005262\n",
      "On epoch 8 of day 74 training, loss = 0.00005224\n",
      "On epoch 9 of day 74 training, loss = 0.00005161\n",
      "On epoch 10 of day 74 training, loss = 0.00005098\n",
      "On epoch 11 of day 74 training, loss = 0.00005060\n",
      "On epoch 12 of day 74 training, loss = 0.00005026\n",
      "On epoch 13 of day 74 training, loss = 0.00004982\n",
      "On epoch 14 of day 74 training, loss = 0.00004935\n",
      "On epoch 15 of day 74 training, loss = 0.00004896\n",
      "On epoch 16 of day 74 training, loss = 0.00004862\n",
      "On epoch 17 of day 74 training, loss = 0.00004826\n",
      "On epoch 18 of day 74 training, loss = 0.00004788\n",
      "On epoch 19 of day 74 training, loss = 0.00004754\n",
      "On epoch 20 of day 74 training, loss = 0.00004722\n",
      "On epoch 21 of day 74 training, loss = 0.00004691\n",
      "On epoch 22 of day 74 training, loss = 0.00004659\n",
      "On epoch 23 of day 74 training, loss = 0.00004629\n",
      "On epoch 24 of day 74 training, loss = 0.00004601\n",
      "On epoch 25 of day 74 training, loss = 0.00004574\n",
      "Current test pnl = -0.002986771985888481\n",
      " ----- On train/test for day 75 ----- \n",
      "On epoch 1 of day 75 training, loss = 0.00007442\n",
      "On epoch 2 of day 75 training, loss = 0.00007166\n",
      "On epoch 3 of day 75 training, loss = 0.00007301\n",
      "On epoch 4 of day 75 training, loss = 0.00007022\n",
      "On epoch 5 of day 75 training, loss = 0.00006783\n",
      "On epoch 6 of day 75 training, loss = 0.00006621\n",
      "On epoch 7 of day 75 training, loss = 0.00006625\n",
      "On epoch 8 of day 75 training, loss = 0.00006594\n",
      "On epoch 9 of day 75 training, loss = 0.00006498\n",
      "On epoch 10 of day 75 training, loss = 0.00006389\n",
      "On epoch 11 of day 75 training, loss = 0.00006302\n",
      "On epoch 12 of day 75 training, loss = 0.00006251\n",
      "On epoch 13 of day 75 training, loss = 0.00006204\n",
      "On epoch 14 of day 75 training, loss = 0.00006142\n",
      "On epoch 15 of day 75 training, loss = 0.00006074\n",
      "On epoch 16 of day 75 training, loss = 0.00006012\n",
      "On epoch 17 of day 75 training, loss = 0.00005961\n",
      "On epoch 18 of day 75 training, loss = 0.00005914\n",
      "On epoch 19 of day 75 training, loss = 0.00005865\n",
      "On epoch 20 of day 75 training, loss = 0.00005815\n",
      "On epoch 21 of day 75 training, loss = 0.00005767\n",
      "On epoch 22 of day 75 training, loss = 0.00005724\n",
      "On epoch 23 of day 75 training, loss = 0.00005682\n",
      "On epoch 24 of day 75 training, loss = 0.00005642\n",
      "On epoch 25 of day 75 training, loss = 0.00005602\n",
      "Current test pnl = 0.008283154107630253\n",
      " ----- On train/test for day 76 ----- \n",
      "On epoch 1 of day 76 training, loss = 0.00009984\n",
      "On epoch 2 of day 76 training, loss = 0.00008784\n",
      "On epoch 3 of day 76 training, loss = 0.00009189\n",
      "On epoch 4 of day 76 training, loss = 0.00009187\n",
      "On epoch 5 of day 76 training, loss = 0.00009002\n",
      "On epoch 6 of day 76 training, loss = 0.00008808\n",
      "On epoch 7 of day 76 training, loss = 0.00008723\n",
      "On epoch 8 of day 76 training, loss = 0.00008761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On epoch 9 of day 76 training, loss = 0.00008749\n",
      "On epoch 10 of day 76 training, loss = 0.00008680\n",
      "On epoch 11 of day 76 training, loss = 0.00008601\n",
      "On epoch 12 of day 76 training, loss = 0.00008555\n",
      "On epoch 13 of day 76 training, loss = 0.00008538\n",
      "On epoch 14 of day 76 training, loss = 0.00008513\n",
      "On epoch 15 of day 76 training, loss = 0.00008472\n",
      "On epoch 16 of day 76 training, loss = 0.00008428\n",
      "On epoch 17 of day 76 training, loss = 0.00008395\n",
      "On epoch 18 of day 76 training, loss = 0.00008371\n",
      "On epoch 19 of day 76 training, loss = 0.00008345\n",
      "On epoch 20 of day 76 training, loss = 0.00008316\n",
      "On epoch 21 of day 76 training, loss = 0.00008286\n",
      "On epoch 22 of day 76 training, loss = 0.00008261\n",
      "On epoch 23 of day 76 training, loss = 0.00008239\n",
      "On epoch 24 of day 76 training, loss = 0.00008216\n",
      "On epoch 25 of day 76 training, loss = 0.00008194\n",
      "Current test pnl = -0.0025127825792878866\n",
      " ----- On train/test for day 77 ----- \n",
      "On epoch 1 of day 77 training, loss = 0.00012034\n",
      "On epoch 2 of day 77 training, loss = 0.00011240\n",
      "On epoch 3 of day 77 training, loss = 0.00011206\n",
      "On epoch 4 of day 77 training, loss = 0.00011045\n",
      "On epoch 5 of day 77 training, loss = 0.00010994\n",
      "On epoch 6 of day 77 training, loss = 0.00010889\n",
      "On epoch 7 of day 77 training, loss = 0.00010796\n",
      "On epoch 8 of day 77 training, loss = 0.00010729\n",
      "On epoch 9 of day 77 training, loss = 0.00010662\n",
      "On epoch 10 of day 77 training, loss = 0.00010603\n",
      "On epoch 11 of day 77 training, loss = 0.00010540\n",
      "On epoch 12 of day 77 training, loss = 0.00010477\n",
      "On epoch 13 of day 77 training, loss = 0.00010421\n",
      "On epoch 14 of day 77 training, loss = 0.00010368\n",
      "On epoch 15 of day 77 training, loss = 0.00010317\n",
      "On epoch 16 of day 77 training, loss = 0.00010268\n",
      "On epoch 17 of day 77 training, loss = 0.00010219\n",
      "On epoch 18 of day 77 training, loss = 0.00010173\n",
      "On epoch 19 of day 77 training, loss = 0.00010129\n",
      "On epoch 20 of day 77 training, loss = 0.00010087\n",
      "On epoch 21 of day 77 training, loss = 0.00010046\n",
      "On epoch 22 of day 77 training, loss = 0.00010006\n",
      "On epoch 23 of day 77 training, loss = 0.00009968\n",
      "On epoch 24 of day 77 training, loss = 0.00009932\n",
      "On epoch 25 of day 77 training, loss = 0.00009897\n",
      "Current test pnl = -0.0028562580700963736\n",
      " ----- On train/test for day 78 ----- \n",
      "On epoch 1 of day 78 training, loss = 0.00012944\n",
      "On epoch 2 of day 78 training, loss = 0.00011446\n",
      "On epoch 3 of day 78 training, loss = 0.00011510\n",
      "On epoch 4 of day 78 training, loss = 0.00011779\n",
      "On epoch 5 of day 78 training, loss = 0.00011706\n",
      "On epoch 6 of day 78 training, loss = 0.00011551\n",
      "On epoch 7 of day 78 training, loss = 0.00011399\n",
      "On epoch 8 of day 78 training, loss = 0.00011290\n",
      "On epoch 9 of day 78 training, loss = 0.00011265\n",
      "On epoch 10 of day 78 training, loss = 0.00011271\n",
      "On epoch 11 of day 78 training, loss = 0.00011252\n",
      "On epoch 12 of day 78 training, loss = 0.00011204\n",
      "On epoch 13 of day 78 training, loss = 0.00011146\n",
      "On epoch 14 of day 78 training, loss = 0.00011096\n",
      "On epoch 15 of day 78 training, loss = 0.00011062\n",
      "On epoch 16 of day 78 training, loss = 0.00011038\n",
      "On epoch 17 of day 78 training, loss = 0.00011013\n",
      "On epoch 18 of day 78 training, loss = 0.00010984\n",
      "On epoch 19 of day 78 training, loss = 0.00010952\n",
      "On epoch 20 of day 78 training, loss = 0.00010921\n",
      "On epoch 21 of day 78 training, loss = 0.00010895\n",
      "On epoch 22 of day 78 training, loss = 0.00010871\n",
      "On epoch 23 of day 78 training, loss = 0.00010849\n",
      "On epoch 24 of day 78 training, loss = 0.00010827\n",
      "On epoch 25 of day 78 training, loss = 0.00010804\n",
      "Current test pnl = 0.0007542417733930051\n",
      " ----- On train/test for day 79 ----- \n",
      "On epoch 1 of day 79 training, loss = 0.00012606\n",
      "On epoch 2 of day 79 training, loss = 0.00010552\n",
      "On epoch 3 of day 79 training, loss = 0.00010971\n",
      "On epoch 4 of day 79 training, loss = 0.00011113\n",
      "On epoch 5 of day 79 training, loss = 0.00010940\n",
      "On epoch 6 of day 79 training, loss = 0.00010814\n",
      "On epoch 7 of day 79 training, loss = 0.00010652\n",
      "On epoch 8 of day 79 training, loss = 0.00010582\n",
      "On epoch 9 of day 79 training, loss = 0.00010612\n",
      "On epoch 10 of day 79 training, loss = 0.00010623\n",
      "On epoch 11 of day 79 training, loss = 0.00010593\n",
      "On epoch 12 of day 79 training, loss = 0.00010543\n",
      "On epoch 13 of day 79 training, loss = 0.00010495\n",
      "On epoch 14 of day 79 training, loss = 0.00010467\n",
      "On epoch 15 of day 79 training, loss = 0.00010455\n",
      "On epoch 16 of day 79 training, loss = 0.00010442\n",
      "On epoch 17 of day 79 training, loss = 0.00010421\n",
      "On epoch 18 of day 79 training, loss = 0.00010396\n",
      "On epoch 19 of day 79 training, loss = 0.00010372\n",
      "On epoch 20 of day 79 training, loss = 0.00010353\n",
      "On epoch 21 of day 79 training, loss = 0.00010337\n",
      "On epoch 22 of day 79 training, loss = 0.00010322\n",
      "On epoch 23 of day 79 training, loss = 0.00010305\n",
      "On epoch 24 of day 79 training, loss = 0.00010288\n",
      "On epoch 25 of day 79 training, loss = 0.00010272\n",
      "Current test pnl = -0.0008804203826002777\n",
      " ----- On train/test for day 80 ----- \n",
      "On epoch 1 of day 80 training, loss = 0.00013146\n",
      "On epoch 2 of day 80 training, loss = 0.00011970\n",
      "On epoch 3 of day 80 training, loss = 0.00012290\n",
      "On epoch 4 of day 80 training, loss = 0.00012199\n",
      "On epoch 5 of day 80 training, loss = 0.00012098\n",
      "On epoch 6 of day 80 training, loss = 0.00011966\n",
      "On epoch 7 of day 80 training, loss = 0.00011865\n",
      "On epoch 8 of day 80 training, loss = 0.00011863\n",
      "On epoch 9 of day 80 training, loss = 0.00011858\n",
      "On epoch 10 of day 80 training, loss = 0.00011821\n",
      "On epoch 11 of day 80 training, loss = 0.00011767\n",
      "On epoch 12 of day 80 training, loss = 0.00011714\n",
      "On epoch 13 of day 80 training, loss = 0.00011679\n",
      "On epoch 14 of day 80 training, loss = 0.00011655\n",
      "On epoch 15 of day 80 training, loss = 0.00011628\n",
      "On epoch 16 of day 80 training, loss = 0.00011595\n",
      "On epoch 17 of day 80 training, loss = 0.00011559\n",
      "On epoch 18 of day 80 training, loss = 0.00011527\n",
      "On epoch 19 of day 80 training, loss = 0.00011499\n",
      "On epoch 20 of day 80 training, loss = 0.00011473\n",
      "On epoch 21 of day 80 training, loss = 0.00011445\n",
      "On epoch 22 of day 80 training, loss = 0.00011417\n",
      "On epoch 23 of day 80 training, loss = 0.00011390\n",
      "On epoch 24 of day 80 training, loss = 0.00011364\n",
      "On epoch 25 of day 80 training, loss = 0.00011339\n",
      "Current test pnl = -0.005324726924300194\n",
      " ----- On train/test for day 81 ----- \n",
      "On epoch 1 of day 81 training, loss = 0.00011331\n",
      "On epoch 2 of day 81 training, loss = 0.00010594\n",
      "On epoch 3 of day 81 training, loss = 0.00010416\n",
      "On epoch 4 of day 81 training, loss = 0.00010501\n",
      "On epoch 5 of day 81 training, loss = 0.00010498\n",
      "On epoch 6 of day 81 training, loss = 0.00010422\n",
      "On epoch 7 of day 81 training, loss = 0.00010338\n",
      "On epoch 8 of day 81 training, loss = 0.00010276\n",
      "On epoch 9 of day 81 training, loss = 0.00010253\n",
      "On epoch 10 of day 81 training, loss = 0.00010239\n",
      "On epoch 11 of day 81 training, loss = 0.00010210\n",
      "On epoch 12 of day 81 training, loss = 0.00010169\n",
      "On epoch 13 of day 81 training, loss = 0.00010128\n",
      "On epoch 14 of day 81 training, loss = 0.00010095\n",
      "On epoch 15 of day 81 training, loss = 0.00010070\n",
      "On epoch 16 of day 81 training, loss = 0.00010045\n",
      "On epoch 17 of day 81 training, loss = 0.00010017\n",
      "On epoch 18 of day 81 training, loss = 0.00009987\n",
      "On epoch 19 of day 81 training, loss = 0.00009958\n",
      "On epoch 20 of day 81 training, loss = 0.00009933\n",
      "On epoch 21 of day 81 training, loss = 0.00009908\n",
      "On epoch 22 of day 81 training, loss = 0.00009884\n",
      "On epoch 23 of day 81 training, loss = 0.00009860\n",
      "On epoch 24 of day 81 training, loss = 0.00009835\n",
      "On epoch 25 of day 81 training, loss = 0.00009812\n",
      "Current test pnl = 0.006701698526740074\n",
      " ----- On train/test for day 82 ----- \n",
      "On epoch 1 of day 82 training, loss = 0.00010456\n",
      "On epoch 2 of day 82 training, loss = 0.00009576\n",
      "On epoch 3 of day 82 training, loss = 0.00009695\n",
      "On epoch 4 of day 82 training, loss = 0.00009697\n",
      "On epoch 5 of day 82 training, loss = 0.00009623\n",
      "On epoch 6 of day 82 training, loss = 0.00009534\n",
      "On epoch 7 of day 82 training, loss = 0.00009458\n",
      "On epoch 8 of day 82 training, loss = 0.00009438\n",
      "On epoch 9 of day 82 training, loss = 0.00009428\n",
      "On epoch 10 of day 82 training, loss = 0.00009400\n",
      "On epoch 11 of day 82 training, loss = 0.00009359\n",
      "On epoch 12 of day 82 training, loss = 0.00009318\n",
      "On epoch 13 of day 82 training, loss = 0.00009287\n",
      "On epoch 14 of day 82 training, loss = 0.00009264\n",
      "On epoch 15 of day 82 training, loss = 0.00009241\n",
      "On epoch 16 of day 82 training, loss = 0.00009214\n",
      "On epoch 17 of day 82 training, loss = 0.00009185\n",
      "On epoch 18 of day 82 training, loss = 0.00009159\n",
      "On epoch 19 of day 82 training, loss = 0.00009135\n",
      "On epoch 20 of day 82 training, loss = 0.00009113\n",
      "On epoch 21 of day 82 training, loss = 0.00009091\n",
      "On epoch 22 of day 82 training, loss = 0.00009068\n",
      "On epoch 23 of day 82 training, loss = 0.00009046\n",
      "On epoch 24 of day 82 training, loss = 0.00009025\n",
      "On epoch 25 of day 82 training, loss = 0.00009005\n",
      "Current test pnl = -0.0007377019501291215\n",
      " ----- On train/test for day 83 ----- \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On epoch 1 of day 83 training, loss = 0.00011216\n",
      "On epoch 2 of day 83 training, loss = 0.00011026\n",
      "On epoch 3 of day 83 training, loss = 0.00010986\n",
      "On epoch 4 of day 83 training, loss = 0.00010932\n",
      "On epoch 5 of day 83 training, loss = 0.00010879\n",
      "On epoch 6 of day 83 training, loss = 0.00010834\n",
      "On epoch 7 of day 83 training, loss = 0.00010798\n",
      "On epoch 8 of day 83 training, loss = 0.00010759\n",
      "On epoch 9 of day 83 training, loss = 0.00010719\n",
      "On epoch 10 of day 83 training, loss = 0.00010679\n",
      "On epoch 11 of day 83 training, loss = 0.00010642\n",
      "On epoch 12 of day 83 training, loss = 0.00010607\n",
      "On epoch 13 of day 83 training, loss = 0.00010572\n",
      "On epoch 14 of day 83 training, loss = 0.00010536\n",
      "On epoch 15 of day 83 training, loss = 0.00010501\n",
      "On epoch 16 of day 83 training, loss = 0.00010468\n",
      "On epoch 17 of day 83 training, loss = 0.00010435\n",
      "On epoch 18 of day 83 training, loss = 0.00010403\n",
      "On epoch 19 of day 83 training, loss = 0.00010371\n",
      "On epoch 20 of day 83 training, loss = 0.00010340\n",
      "On epoch 21 of day 83 training, loss = 0.00010309\n",
      "On epoch 22 of day 83 training, loss = 0.00010279\n",
      "On epoch 23 of day 83 training, loss = 0.00010250\n",
      "On epoch 24 of day 83 training, loss = 0.00010221\n",
      "On epoch 25 of day 83 training, loss = 0.00010192\n",
      "Current test pnl = -0.002287807874381542\n",
      " ----- On train/test for day 84 ----- \n",
      "On epoch 1 of day 84 training, loss = 0.00012627\n",
      "On epoch 2 of day 84 training, loss = 0.00012256\n",
      "On epoch 3 of day 84 training, loss = 0.00012222\n",
      "On epoch 4 of day 84 training, loss = 0.00012101\n",
      "On epoch 5 of day 84 training, loss = 0.00012009\n",
      "On epoch 6 of day 84 training, loss = 0.00011921\n",
      "On epoch 7 of day 84 training, loss = 0.00011885\n",
      "On epoch 8 of day 84 training, loss = 0.00011826\n",
      "On epoch 9 of day 84 training, loss = 0.00011761\n",
      "On epoch 10 of day 84 training, loss = 0.00011696\n",
      "On epoch 11 of day 84 training, loss = 0.00011640\n",
      "On epoch 12 of day 84 training, loss = 0.00011590\n",
      "On epoch 13 of day 84 training, loss = 0.00011537\n",
      "On epoch 14 of day 84 training, loss = 0.00011483\n",
      "On epoch 15 of day 84 training, loss = 0.00011429\n",
      "On epoch 16 of day 84 training, loss = 0.00011379\n",
      "On epoch 17 of day 84 training, loss = 0.00011330\n",
      "On epoch 18 of day 84 training, loss = 0.00011281\n",
      "On epoch 19 of day 84 training, loss = 0.00011233\n",
      "On epoch 20 of day 84 training, loss = 0.00011185\n",
      "On epoch 21 of day 84 training, loss = 0.00011139\n",
      "On epoch 22 of day 84 training, loss = 0.00011093\n",
      "On epoch 23 of day 84 training, loss = 0.00011049\n",
      "On epoch 24 of day 84 training, loss = 0.00011004\n",
      "On epoch 25 of day 84 training, loss = 0.00010961\n",
      "Current test pnl = -0.0034943651407957077\n",
      " ----- On train/test for day 85 ----- \n",
      "On epoch 1 of day 85 training, loss = 0.00011643\n",
      "On epoch 2 of day 85 training, loss = 0.00010630\n",
      "On epoch 3 of day 85 training, loss = 0.00010976\n",
      "On epoch 4 of day 85 training, loss = 0.00010888\n",
      "On epoch 5 of day 85 training, loss = 0.00010695\n",
      "On epoch 6 of day 85 training, loss = 0.00010508\n",
      "On epoch 7 of day 85 training, loss = 0.00010357\n",
      "On epoch 8 of day 85 training, loss = 0.00010299\n",
      "On epoch 9 of day 85 training, loss = 0.00010258\n",
      "On epoch 10 of day 85 training, loss = 0.00010181\n",
      "On epoch 11 of day 85 training, loss = 0.00010083\n",
      "On epoch 12 of day 85 training, loss = 0.00009982\n",
      "On epoch 13 of day 85 training, loss = 0.00009896\n",
      "On epoch 14 of day 85 training, loss = 0.00009826\n",
      "On epoch 15 of day 85 training, loss = 0.00009758\n",
      "On epoch 16 of day 85 training, loss = 0.00009685\n",
      "On epoch 17 of day 85 training, loss = 0.00009609\n",
      "On epoch 18 of day 85 training, loss = 0.00009534\n",
      "On epoch 19 of day 85 training, loss = 0.00009464\n",
      "On epoch 20 of day 85 training, loss = 0.00009397\n",
      "On epoch 21 of day 85 training, loss = 0.00009332\n",
      "On epoch 22 of day 85 training, loss = 0.00009266\n",
      "On epoch 23 of day 85 training, loss = 0.00009201\n",
      "On epoch 24 of day 85 training, loss = 0.00009138\n",
      "On epoch 25 of day 85 training, loss = 0.00009077\n",
      "Current test pnl = 0.0005858818185515702\n",
      " ----- On train/test for day 86 ----- \n",
      "On epoch 1 of day 86 training, loss = 0.00009777\n",
      "On epoch 2 of day 86 training, loss = 0.00009399\n",
      "On epoch 3 of day 86 training, loss = 0.00009300\n",
      "On epoch 4 of day 86 training, loss = 0.00009215\n",
      "On epoch 5 of day 86 training, loss = 0.00009132\n",
      "On epoch 6 of day 86 training, loss = 0.00009052\n",
      "On epoch 7 of day 86 training, loss = 0.00008981\n",
      "On epoch 8 of day 86 training, loss = 0.00008912\n",
      "On epoch 9 of day 86 training, loss = 0.00008844\n",
      "On epoch 10 of day 86 training, loss = 0.00008777\n",
      "On epoch 11 of day 86 training, loss = 0.00008713\n",
      "On epoch 12 of day 86 training, loss = 0.00008652\n",
      "On epoch 13 of day 86 training, loss = 0.00008593\n",
      "On epoch 14 of day 86 training, loss = 0.00008534\n",
      "On epoch 15 of day 86 training, loss = 0.00008477\n",
      "On epoch 16 of day 86 training, loss = 0.00008423\n",
      "On epoch 17 of day 86 training, loss = 0.00008370\n",
      "On epoch 18 of day 86 training, loss = 0.00008318\n",
      "On epoch 19 of day 86 training, loss = 0.00008268\n",
      "On epoch 20 of day 86 training, loss = 0.00008218\n",
      "On epoch 21 of day 86 training, loss = 0.00008171\n",
      "On epoch 22 of day 86 training, loss = 0.00008124\n",
      "On epoch 23 of day 86 training, loss = 0.00008079\n",
      "On epoch 24 of day 86 training, loss = 0.00008035\n",
      "On epoch 25 of day 86 training, loss = 0.00007992\n",
      "Current test pnl = -0.0006490269443020225\n",
      " ----- On train/test for day 87 ----- \n",
      "On epoch 1 of day 87 training, loss = 0.00008103\n",
      "On epoch 2 of day 87 training, loss = 0.00007718\n",
      "On epoch 3 of day 87 training, loss = 0.00007606\n",
      "On epoch 4 of day 87 training, loss = 0.00007548\n",
      "On epoch 5 of day 87 training, loss = 0.00007500\n",
      "On epoch 6 of day 87 training, loss = 0.00007441\n",
      "On epoch 7 of day 87 training, loss = 0.00007381\n",
      "On epoch 8 of day 87 training, loss = 0.00007328\n",
      "On epoch 9 of day 87 training, loss = 0.00007282\n",
      "On epoch 10 of day 87 training, loss = 0.00007237\n",
      "On epoch 11 of day 87 training, loss = 0.00007190\n",
      "On epoch 12 of day 87 training, loss = 0.00007144\n",
      "On epoch 13 of day 87 training, loss = 0.00007100\n",
      "On epoch 14 of day 87 training, loss = 0.00007058\n",
      "On epoch 15 of day 87 training, loss = 0.00007018\n",
      "On epoch 16 of day 87 training, loss = 0.00006977\n",
      "On epoch 17 of day 87 training, loss = 0.00006938\n",
      "On epoch 18 of day 87 training, loss = 0.00006899\n",
      "On epoch 19 of day 87 training, loss = 0.00006863\n",
      "On epoch 20 of day 87 training, loss = 0.00006826\n",
      "On epoch 21 of day 87 training, loss = 0.00006791\n",
      "On epoch 22 of day 87 training, loss = 0.00006756\n",
      "On epoch 23 of day 87 training, loss = 0.00006722\n",
      "On epoch 24 of day 87 training, loss = 0.00006689\n",
      "On epoch 25 of day 87 training, loss = 0.00006657\n",
      "Current test pnl = -0.002242996357381344\n",
      " ----- On train/test for day 88 ----- \n",
      "On epoch 1 of day 88 training, loss = 0.00007508\n",
      "On epoch 2 of day 88 training, loss = 0.00007139\n",
      "On epoch 3 of day 88 training, loss = 0.00006991\n",
      "On epoch 4 of day 88 training, loss = 0.00006872\n",
      "On epoch 5 of day 88 training, loss = 0.00006820\n",
      "On epoch 6 of day 88 training, loss = 0.00006739\n",
      "On epoch 7 of day 88 training, loss = 0.00006653\n",
      "On epoch 8 of day 88 training, loss = 0.00006573\n",
      "On epoch 9 of day 88 training, loss = 0.00006502\n",
      "On epoch 10 of day 88 training, loss = 0.00006443\n",
      "On epoch 11 of day 88 training, loss = 0.00006382\n",
      "On epoch 12 of day 88 training, loss = 0.00006321\n",
      "On epoch 13 of day 88 training, loss = 0.00006261\n",
      "On epoch 14 of day 88 training, loss = 0.00006205\n",
      "On epoch 15 of day 88 training, loss = 0.00006154\n",
      "On epoch 16 of day 88 training, loss = 0.00006104\n",
      "On epoch 17 of day 88 training, loss = 0.00006056\n",
      "On epoch 18 of day 88 training, loss = 0.00006009\n",
      "On epoch 19 of day 88 training, loss = 0.00005964\n",
      "On epoch 20 of day 88 training, loss = 0.00005921\n",
      "On epoch 21 of day 88 training, loss = 0.00005880\n",
      "On epoch 22 of day 88 training, loss = 0.00005841\n",
      "On epoch 23 of day 88 training, loss = 0.00005802\n",
      "On epoch 24 of day 88 training, loss = 0.00005766\n",
      "On epoch 25 of day 88 training, loss = 0.00005731\n",
      "Current test pnl = 0.002909268718212843\n",
      " ----- On train/test for day 89 ----- \n",
      "On epoch 1 of day 89 training, loss = 0.00006279\n",
      "On epoch 2 of day 89 training, loss = 0.00005853\n",
      "On epoch 3 of day 89 training, loss = 0.00005779\n",
      "On epoch 4 of day 89 training, loss = 0.00005753\n",
      "On epoch 5 of day 89 training, loss = 0.00005686\n",
      "On epoch 6 of day 89 training, loss = 0.00005615\n",
      "On epoch 7 of day 89 training, loss = 0.00005560\n",
      "On epoch 8 of day 89 training, loss = 0.00005525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On epoch 9 of day 89 training, loss = 0.00005485\n",
      "On epoch 10 of day 89 training, loss = 0.00005439\n",
      "On epoch 11 of day 89 training, loss = 0.00005394\n",
      "On epoch 12 of day 89 training, loss = 0.00005357\n",
      "On epoch 13 of day 89 training, loss = 0.00005323\n",
      "On epoch 14 of day 89 training, loss = 0.00005288\n",
      "On epoch 15 of day 89 training, loss = 0.00005253\n",
      "On epoch 16 of day 89 training, loss = 0.00005220\n",
      "On epoch 17 of day 89 training, loss = 0.00005191\n",
      "On epoch 18 of day 89 training, loss = 0.00005162\n",
      "On epoch 19 of day 89 training, loss = 0.00005133\n",
      "On epoch 20 of day 89 training, loss = 0.00005106\n",
      "On epoch 21 of day 89 training, loss = 0.00005081\n",
      "On epoch 22 of day 89 training, loss = 0.00005056\n",
      "On epoch 23 of day 89 training, loss = 0.00005033\n",
      "On epoch 24 of day 89 training, loss = 0.00005010\n",
      "On epoch 25 of day 89 training, loss = 0.00004988\n",
      "Current test pnl = 0.004433352034538984\n",
      " ----- On train/test for day 90 ----- \n",
      "On epoch 1 of day 90 training, loss = 0.00005764\n",
      "On epoch 2 of day 90 training, loss = 0.00005046\n",
      "On epoch 3 of day 90 training, loss = 0.00005007\n",
      "On epoch 4 of day 90 training, loss = 0.00004841\n",
      "On epoch 5 of day 90 training, loss = 0.00004733\n",
      "On epoch 6 of day 90 training, loss = 0.00004573\n",
      "On epoch 7 of day 90 training, loss = 0.00004494\n",
      "On epoch 8 of day 90 training, loss = 0.00004405\n",
      "On epoch 9 of day 90 training, loss = 0.00004318\n",
      "On epoch 10 of day 90 training, loss = 0.00004223\n",
      "On epoch 11 of day 90 training, loss = 0.00004141\n",
      "On epoch 12 of day 90 training, loss = 0.00004072\n",
      "On epoch 13 of day 90 training, loss = 0.00004002\n",
      "On epoch 14 of day 90 training, loss = 0.00003931\n",
      "On epoch 15 of day 90 training, loss = 0.00003864\n",
      "On epoch 16 of day 90 training, loss = 0.00003805\n",
      "On epoch 17 of day 90 training, loss = 0.00003749\n",
      "On epoch 18 of day 90 training, loss = 0.00003694\n",
      "On epoch 19 of day 90 training, loss = 0.00003641\n",
      "On epoch 20 of day 90 training, loss = 0.00003592\n",
      "On epoch 21 of day 90 training, loss = 0.00003546\n",
      "On epoch 22 of day 90 training, loss = 0.00003503\n",
      "On epoch 23 of day 90 training, loss = 0.00003461\n",
      "On epoch 24 of day 90 training, loss = 0.00003422\n",
      "On epoch 25 of day 90 training, loss = 0.00003386\n",
      "Current test pnl = -0.0006620592903345823\n",
      " ----- On train/test for day 91 ----- \n",
      "On epoch 1 of day 91 training, loss = 0.00006263\n",
      "On epoch 2 of day 91 training, loss = 0.00005891\n",
      "On epoch 3 of day 91 training, loss = 0.00005748\n",
      "On epoch 4 of day 91 training, loss = 0.00005672\n",
      "On epoch 5 of day 91 training, loss = 0.00005546\n",
      "On epoch 6 of day 91 training, loss = 0.00005464\n",
      "On epoch 7 of day 91 training, loss = 0.00005386\n",
      "On epoch 8 of day 91 training, loss = 0.00005316\n",
      "On epoch 9 of day 91 training, loss = 0.00005240\n",
      "On epoch 10 of day 91 training, loss = 0.00005174\n",
      "On epoch 11 of day 91 training, loss = 0.00005115\n",
      "On epoch 12 of day 91 training, loss = 0.00005059\n",
      "On epoch 13 of day 91 training, loss = 0.00005003\n",
      "On epoch 14 of day 91 training, loss = 0.00004951\n",
      "On epoch 15 of day 91 training, loss = 0.00004903\n",
      "On epoch 16 of day 91 training, loss = 0.00004858\n",
      "On epoch 17 of day 91 training, loss = 0.00004814\n",
      "On epoch 18 of day 91 training, loss = 0.00004773\n",
      "On epoch 19 of day 91 training, loss = 0.00004734\n",
      "On epoch 20 of day 91 training, loss = 0.00004697\n",
      "On epoch 21 of day 91 training, loss = 0.00004662\n",
      "On epoch 22 of day 91 training, loss = 0.00004628\n",
      "On epoch 23 of day 91 training, loss = 0.00004597\n",
      "On epoch 24 of day 91 training, loss = 0.00004566\n",
      "On epoch 25 of day 91 training, loss = 0.00004537\n",
      "Current test pnl = 0.0009098489535972476\n",
      " ----- On train/test for day 92 ----- \n",
      "On epoch 1 of day 92 training, loss = 0.00006527\n",
      "On epoch 2 of day 92 training, loss = 0.00005853\n",
      "On epoch 3 of day 92 training, loss = 0.00005872\n",
      "On epoch 4 of day 92 training, loss = 0.00005680\n",
      "On epoch 5 of day 92 training, loss = 0.00005527\n",
      "On epoch 6 of day 92 training, loss = 0.00005392\n",
      "On epoch 7 of day 92 training, loss = 0.00005348\n",
      "On epoch 8 of day 92 training, loss = 0.00005263\n",
      "On epoch 9 of day 92 training, loss = 0.00005176\n",
      "On epoch 10 of day 92 training, loss = 0.00005096\n",
      "On epoch 11 of day 92 training, loss = 0.00005046\n",
      "On epoch 12 of day 92 training, loss = 0.00004993\n",
      "On epoch 13 of day 92 training, loss = 0.00004935\n",
      "On epoch 14 of day 92 training, loss = 0.00004880\n",
      "On epoch 15 of day 92 training, loss = 0.00004837\n",
      "On epoch 16 of day 92 training, loss = 0.00004797\n",
      "On epoch 17 of day 92 training, loss = 0.00004755\n",
      "On epoch 18 of day 92 training, loss = 0.00004714\n",
      "On epoch 19 of day 92 training, loss = 0.00004679\n",
      "On epoch 20 of day 92 training, loss = 0.00004647\n",
      "On epoch 21 of day 92 training, loss = 0.00004615\n",
      "On epoch 22 of day 92 training, loss = 0.00004584\n",
      "On epoch 23 of day 92 training, loss = 0.00004555\n",
      "On epoch 24 of day 92 training, loss = 0.00004529\n",
      "On epoch 25 of day 92 training, loss = 0.00004503\n",
      "Current test pnl = 0.004419616889208555\n",
      " ----- On train/test for day 93 ----- \n",
      "On epoch 1 of day 93 training, loss = 0.00004890\n",
      "On epoch 2 of day 93 training, loss = 0.00004520\n",
      "On epoch 3 of day 93 training, loss = 0.00004471\n",
      "On epoch 4 of day 93 training, loss = 0.00004421\n",
      "On epoch 5 of day 93 training, loss = 0.00004347\n",
      "On epoch 6 of day 93 training, loss = 0.00004296\n",
      "On epoch 7 of day 93 training, loss = 0.00004260\n",
      "On epoch 8 of day 93 training, loss = 0.00004224\n",
      "On epoch 9 of day 93 training, loss = 0.00004183\n",
      "On epoch 10 of day 93 training, loss = 0.00004144\n",
      "On epoch 11 of day 93 training, loss = 0.00004114\n",
      "On epoch 12 of day 93 training, loss = 0.00004086\n",
      "On epoch 13 of day 93 training, loss = 0.00004057\n",
      "On epoch 14 of day 93 training, loss = 0.00004028\n",
      "On epoch 15 of day 93 training, loss = 0.00004003\n",
      "On epoch 16 of day 93 training, loss = 0.00003981\n",
      "On epoch 17 of day 93 training, loss = 0.00003958\n",
      "On epoch 18 of day 93 training, loss = 0.00003937\n",
      "On epoch 19 of day 93 training, loss = 0.00003916\n",
      "On epoch 20 of day 93 training, loss = 0.00003898\n",
      "On epoch 21 of day 93 training, loss = 0.00003880\n",
      "On epoch 22 of day 93 training, loss = 0.00003863\n",
      "On epoch 23 of day 93 training, loss = 0.00003847\n",
      "On epoch 24 of day 93 training, loss = 0.00003831\n",
      "On epoch 25 of day 93 training, loss = 0.00003817\n",
      "Current test pnl = 0.0013006189838051796\n",
      " ----- On train/test for day 94 ----- \n",
      "On epoch 1 of day 94 training, loss = 0.00003678\n",
      "On epoch 2 of day 94 training, loss = 0.00003268\n",
      "On epoch 3 of day 94 training, loss = 0.00003246\n",
      "On epoch 4 of day 94 training, loss = 0.00003191\n",
      "On epoch 5 of day 94 training, loss = 0.00003108\n",
      "On epoch 6 of day 94 training, loss = 0.00003064\n",
      "On epoch 7 of day 94 training, loss = 0.00003031\n",
      "On epoch 8 of day 94 training, loss = 0.00002994\n",
      "On epoch 9 of day 94 training, loss = 0.00002950\n",
      "On epoch 10 of day 94 training, loss = 0.00002919\n",
      "On epoch 11 of day 94 training, loss = 0.00002893\n",
      "On epoch 12 of day 94 training, loss = 0.00002865\n",
      "On epoch 13 of day 94 training, loss = 0.00002836\n",
      "On epoch 14 of day 94 training, loss = 0.00002813\n",
      "On epoch 15 of day 94 training, loss = 0.00002791\n",
      "On epoch 16 of day 94 training, loss = 0.00002770\n",
      "On epoch 17 of day 94 training, loss = 0.00002749\n",
      "On epoch 18 of day 94 training, loss = 0.00002731\n",
      "On epoch 19 of day 94 training, loss = 0.00002714\n",
      "On epoch 20 of day 94 training, loss = 0.00002697\n",
      "On epoch 21 of day 94 training, loss = 0.00002681\n",
      "On epoch 22 of day 94 training, loss = 0.00002666\n",
      "On epoch 23 of day 94 training, loss = 0.00002652\n",
      "On epoch 24 of day 94 training, loss = 0.00002639\n",
      "On epoch 25 of day 94 training, loss = 0.00002626\n",
      "Current test pnl = 0.0035767797380685806\n",
      " ----- On train/test for day 95 ----- \n",
      "On epoch 1 of day 95 training, loss = 0.00004317\n",
      "On epoch 2 of day 95 training, loss = 0.00003749\n",
      "On epoch 3 of day 95 training, loss = 0.00003747\n",
      "On epoch 4 of day 95 training, loss = 0.00003646\n",
      "On epoch 5 of day 95 training, loss = 0.00003488\n",
      "On epoch 6 of day 95 training, loss = 0.00003353\n",
      "On epoch 7 of day 95 training, loss = 0.00003300\n",
      "On epoch 8 of day 95 training, loss = 0.00003241\n",
      "On epoch 9 of day 95 training, loss = 0.00003154\n",
      "On epoch 10 of day 95 training, loss = 0.00003074\n",
      "On epoch 11 of day 95 training, loss = 0.00003021\n",
      "On epoch 12 of day 95 training, loss = 0.00002978\n",
      "On epoch 13 of day 95 training, loss = 0.00002925\n",
      "On epoch 14 of day 95 training, loss = 0.00002874\n",
      "On epoch 15 of day 95 training, loss = 0.00002834\n",
      "On epoch 16 of day 95 training, loss = 0.00002801\n",
      "On epoch 17 of day 95 training, loss = 0.00002767\n",
      "On epoch 18 of day 95 training, loss = 0.00002733\n",
      "On epoch 19 of day 95 training, loss = 0.00002705\n",
      "On epoch 20 of day 95 training, loss = 0.00002681\n",
      "On epoch 21 of day 95 training, loss = 0.00002658\n",
      "On epoch 22 of day 95 training, loss = 0.00002635\n",
      "On epoch 23 of day 95 training, loss = 0.00002616\n",
      "On epoch 24 of day 95 training, loss = 0.00002598\n",
      "On epoch 25 of day 95 training, loss = 0.00002582\n",
      "Current test pnl = 0.0005242148181423545\n",
      " ----- On train/test for day 96 ----- \n",
      "On epoch 1 of day 96 training, loss = 0.00004160\n",
      "On epoch 2 of day 96 training, loss = 0.00003048\n",
      "On epoch 3 of day 96 training, loss = 0.00003717\n",
      "On epoch 4 of day 96 training, loss = 0.00003479\n",
      "On epoch 5 of day 96 training, loss = 0.00003249\n",
      "On epoch 6 of day 96 training, loss = 0.00003006\n",
      "On epoch 7 of day 96 training, loss = 0.00003016\n",
      "On epoch 8 of day 96 training, loss = 0.00003091\n",
      "On epoch 9 of day 96 training, loss = 0.00003053\n",
      "On epoch 10 of day 96 training, loss = 0.00002967\n",
      "On epoch 11 of day 96 training, loss = 0.00002896\n",
      "On epoch 12 of day 96 training, loss = 0.00002883\n",
      "On epoch 13 of day 96 training, loss = 0.00002888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On epoch 14 of day 96 training, loss = 0.00002868\n",
      "On epoch 15 of day 96 training, loss = 0.00002830\n",
      "On epoch 16 of day 96 training, loss = 0.00002800\n",
      "On epoch 17 of day 96 training, loss = 0.00002786\n",
      "On epoch 18 of day 96 training, loss = 0.00002777\n",
      "On epoch 19 of day 96 training, loss = 0.00002762\n",
      "On epoch 20 of day 96 training, loss = 0.00002742\n",
      "On epoch 21 of day 96 training, loss = 0.00002726\n",
      "On epoch 22 of day 96 training, loss = 0.00002714\n",
      "On epoch 23 of day 96 training, loss = 0.00002704\n",
      "On epoch 24 of day 96 training, loss = 0.00002693\n",
      "On epoch 25 of day 96 training, loss = 0.00002681\n",
      "Current test pnl = -0.003002751851454377\n",
      " ----- On train/test for day 97 ----- \n",
      "On epoch 1 of day 97 training, loss = 0.00004780\n",
      "On epoch 2 of day 97 training, loss = 0.00004361\n",
      "On epoch 3 of day 97 training, loss = 0.00004297\n",
      "On epoch 4 of day 97 training, loss = 0.00004258\n",
      "On epoch 5 of day 97 training, loss = 0.00004240\n",
      "On epoch 6 of day 97 training, loss = 0.00004206\n",
      "On epoch 7 of day 97 training, loss = 0.00004177\n",
      "On epoch 8 of day 97 training, loss = 0.00004154\n",
      "On epoch 9 of day 97 training, loss = 0.00004136\n",
      "On epoch 10 of day 97 training, loss = 0.00004121\n",
      "On epoch 11 of day 97 training, loss = 0.00004105\n",
      "On epoch 12 of day 97 training, loss = 0.00004089\n",
      "On epoch 13 of day 97 training, loss = 0.00004074\n",
      "On epoch 14 of day 97 training, loss = 0.00004061\n",
      "On epoch 15 of day 97 training, loss = 0.00004049\n",
      "On epoch 16 of day 97 training, loss = 0.00004038\n",
      "On epoch 17 of day 97 training, loss = 0.00004027\n",
      "On epoch 18 of day 97 training, loss = 0.00004016\n",
      "On epoch 19 of day 97 training, loss = 0.00004006\n",
      "On epoch 20 of day 97 training, loss = 0.00003997\n",
      "On epoch 21 of day 97 training, loss = 0.00003988\n",
      "On epoch 22 of day 97 training, loss = 0.00003979\n",
      "On epoch 23 of day 97 training, loss = 0.00003971\n",
      "On epoch 24 of day 97 training, loss = 0.00003964\n",
      "On epoch 25 of day 97 training, loss = 0.00003956\n",
      "Current test pnl = -0.00035317332367412746\n",
      " ----- On train/test for day 98 ----- \n",
      "On epoch 1 of day 98 training, loss = 0.00006512\n",
      "On epoch 2 of day 98 training, loss = 0.00004795\n",
      "On epoch 3 of day 98 training, loss = 0.00005291\n",
      "On epoch 4 of day 98 training, loss = 0.00005416\n",
      "On epoch 5 of day 98 training, loss = 0.00005163\n",
      "On epoch 6 of day 98 training, loss = 0.00005041\n",
      "On epoch 7 of day 98 training, loss = 0.00004878\n",
      "On epoch 8 of day 98 training, loss = 0.00004828\n",
      "On epoch 9 of day 98 training, loss = 0.00004885\n",
      "On epoch 10 of day 98 training, loss = 0.00004899\n",
      "On epoch 11 of day 98 training, loss = 0.00004863\n",
      "On epoch 12 of day 98 training, loss = 0.00004813\n",
      "On epoch 13 of day 98 training, loss = 0.00004772\n",
      "On epoch 14 of day 98 training, loss = 0.00004756\n",
      "On epoch 15 of day 98 training, loss = 0.00004754\n",
      "On epoch 16 of day 98 training, loss = 0.00004744\n",
      "On epoch 17 of day 98 training, loss = 0.00004724\n",
      "On epoch 18 of day 98 training, loss = 0.00004701\n",
      "On epoch 19 of day 98 training, loss = 0.00004683\n",
      "On epoch 20 of day 98 training, loss = 0.00004670\n",
      "On epoch 21 of day 98 training, loss = 0.00004659\n",
      "On epoch 22 of day 98 training, loss = 0.00004646\n",
      "On epoch 23 of day 98 training, loss = 0.00004632\n",
      "On epoch 24 of day 98 training, loss = 0.00004617\n",
      "On epoch 25 of day 98 training, loss = 0.00004605\n",
      "Current test pnl = -0.004847888834774494\n",
      " ----- On train/test for day 99 ----- \n",
      "On epoch 1 of day 99 training, loss = 0.00008702\n",
      "On epoch 2 of day 99 training, loss = 0.00006534\n",
      "On epoch 3 of day 99 training, loss = 0.00007104\n",
      "On epoch 4 of day 99 training, loss = 0.00007255\n",
      "On epoch 5 of day 99 training, loss = 0.00007043\n",
      "On epoch 6 of day 99 training, loss = 0.00006879\n",
      "On epoch 7 of day 99 training, loss = 0.00006678\n",
      "On epoch 8 of day 99 training, loss = 0.00006602\n",
      "On epoch 9 of day 99 training, loss = 0.00006645\n",
      "On epoch 10 of day 99 training, loss = 0.00006655\n",
      "On epoch 11 of day 99 training, loss = 0.00006610\n",
      "On epoch 12 of day 99 training, loss = 0.00006541\n",
      "On epoch 13 of day 99 training, loss = 0.00006477\n",
      "On epoch 14 of day 99 training, loss = 0.00006440\n",
      "On epoch 15 of day 99 training, loss = 0.00006422\n",
      "On epoch 16 of day 99 training, loss = 0.00006400\n",
      "On epoch 17 of day 99 training, loss = 0.00006367\n",
      "On epoch 18 of day 99 training, loss = 0.00006329\n",
      "On epoch 19 of day 99 training, loss = 0.00006293\n",
      "On epoch 20 of day 99 training, loss = 0.00006264\n",
      "On epoch 21 of day 99 training, loss = 0.00006238\n",
      "On epoch 22 of day 99 training, loss = 0.00006213\n",
      "On epoch 23 of day 99 training, loss = 0.00006186\n",
      "On epoch 24 of day 99 training, loss = 0.00006158\n",
      "On epoch 25 of day 99 training, loss = 0.00006131\n",
      "Current test pnl = -0.0016586044803261757\n"
     ]
    }
   ],
   "source": [
    "GDN_epochs = 25\n",
    "DDPM_epochs = 5\n",
    "gamma = 0.75\n",
    "delta = 0.75\n",
    "GDN_lr = 0.001\n",
    "DDPM_lr = 0.001\n",
    "DDPM_lb = 10\n",
    "training_lb = 5\n",
    "GCN = True  # 'GCN = True' = GCN only (baseline comparison), 'GCN = False' = full GDN\n",
    "\n",
    "print(f'Will be training/testing on {data.snapshot_count - training_lb} days')\n",
    "if GCN==False:\n",
    "    model_type = 'GDN'\n",
    "    print('In GDN mode:')\n",
    "    d = data.snapshot_count - training_lb - DDPM_lb\n",
    "    if d>0:  \n",
    "        print(f'The last {d} days will use DDPM/DDRM')\n",
    "    else:\n",
    "        print('No days will use DDPM/DDRM')\n",
    "else:\n",
    "    model_type = 'GCN'\n",
    "    print('In GCN mode')\n",
    "print('')\n",
    "\n",
    "outputs_GDN = rolling_train_test(gdn, data, GDN_epochs, DDPM_epochs, gamma, delta, GDN_lr, DDPM_lr, DDPM_lb, \n",
    "                             training_lb, GCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b1bc8a",
   "metadata": {},
   "source": [
    "# Saving results for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2145a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_string = f'{model_type}_{data.snapshot_count}days_{feature_size}feature_size_{GDN_epochs}GDNe_{DDPM_epochs}DDPMe_{gamma}gamma_{delta}delta_{GDN_lr}GDN_lr_{DDPM_lr}DDPM_lr_{DDPM_lb}DDPM_lb_{training_lb}_tr_lb___{data_name}'\n",
    "\n",
    "with open(f'/{model_type}_outputs/{save_string}', 'wb') as f:\n",
    "    pickle.dump(outputs_GDN, f)\n",
    "\n",
    "torch.save(gdn.state_dict(), f'/{model_type}_weights/WEIGHTS_{save_string}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
